{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we will create a very simple convolutional network to train\n",
    "# against MNIST on multiple cpus in order to explore parallel training,\n",
    "# sharding, etc... \n",
    "import os\n",
    "flags = os.environ.get('XLA_FLAGS', '')\n",
    "\n",
    "# Let's keep it simple and simulate 2 CPU devices\n",
    "os.environ['XLA_FLAGS'] = flags + \" --xla_force_host_platform_device_count=2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 fake JAX devices now: [CpuDevice(id=0), CpuDevice(id=1)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "print(f'We have 2 fake JAX devices now: {jax.devices()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important imports for this notebook\n",
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.metrics import tensorboard\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "batch_size = 64\n",
    "shuffle_buffer_size = 1000\n",
    "prefetch = 10\n",
    "image_size = 28\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/don/tensorflow_datasets/mnist/3.0.1\n",
      "INFO:absl:For 'mnist/3.0.1': fields info.[citation, splits, supervised_keys, module_name] differ on disk and in the code. Keeping the one from code.\n",
      "INFO:absl:Reusing dataset mnist (/home/don/tensorflow_datasets/mnist/3.0.1)\n",
      "INFO:absl:Creating a tf.data.Dataset reading 1 files located in folders: /home/don/tensorflow_datasets/mnist/3.0.1.\n",
      "INFO:absl:Constructing tf.data.Dataset mnist for split train, from /home/don/tensorflow_datasets/mnist/3.0.1\n"
     ]
    }
   ],
   "source": [
    "# Let's get the data first - start with mnist\n",
    "\n",
    "# Decoding functions for normalization and augmentation\n",
    "def normalize_image(example):\n",
    "    image = example['image']\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = image/ 255.\n",
    "\n",
    "    return {'image': image, 'label': example['label']}\n",
    "\n",
    "# These commands break up the load command into it's parts for more controls\n",
    "ds_builder = tfds.builder('mnist')\n",
    "ds_builder.download_and_prepare()\n",
    "\n",
    "num_train_steps = ds_builder.info.splits['train'].num_examples\n",
    "\n",
    "train_ds = ds_builder.as_dataset('train')\n",
    "\n",
    "train_ds = train_ds.cache().repeat().shuffle(shuffle_buffer_size).map(normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(prefetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-09 18:39:26.622367: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-06-09 18:39:26.622840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "from flax import jax_utils\n",
    "\n",
    "# Because we are simulating multiple devices, we will need to shard \n",
    "# the data. In this case we create a new dimension before batch \n",
    "# equal to the number of devices. pmap takes care of the details\n",
    "# after that for the most part.\n",
    "def prepare_tf_data(xs):\n",
    "  \"\"\"Convert a input batch from tf Tensors to numpy arrays.\"\"\"\n",
    "  local_device_count = jax.local_device_count()\n",
    "\n",
    "  def _prepare(x):\n",
    "    # Use _numpy() for zero-copy conversion between TF and NumPy.\n",
    "    x = x._numpy()  # pylint: disable=protected-access\n",
    "\n",
    "    # reshape (host_batch_size, height, width, 3) to\n",
    "    # (local_devices, device_batch_size, height, width, 3)\n",
    "    return x.reshape((local_device_count, -1) + x.shape[1:])\n",
    "  \n",
    "  # This will apply it across the dictionary\n",
    "  return jax.tree_util.tree_map(_prepare, xs)\n",
    "\n",
    "# Retuns a separate iterator that applys the sharding function\n",
    "it = map(prepare_tf_data, train_ds)\n",
    "# prefetches into memory - apparently speeds things up quite a bit on GPUs.\n",
    "# Unclear why, but just do it. Something about parallizing data fetching\n",
    "# and computing.\n",
    "it = jax_utils.prefetch_to_device(it, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a model, so let's make the simplest one possible\n",
    "from flax import linen as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  # will use this eventually - half precision will be faster on A100 and greater\n",
    "  dtype = jnp.float32\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "# Model and state initialization functions\n",
    "def initialized(key, image_size, num_channels, model):\n",
    "  input_shape = (1, image_size, image_size, num_channels)\n",
    "\n",
    "  @jax.jit\n",
    "  def init(*args):\n",
    "    return model.init(*args)\n",
    "\n",
    "  variables = init({'params': key}, jnp.ones(input_shape, model.dtype))\n",
    "  # Could add batch stats here\n",
    "  return variables['params']\n",
    "\n",
    "def create_train_state(\n",
    "    rng, model, image_size, num_channels, learning_rate, momentum\n",
    "):\n",
    "  \"\"\"Create initial training state.\"\"\"\n",
    "  # Get the params (and batch stats if applying batch normalization)\n",
    "  params = initialized(rng, image_size, num_channels, model)\n",
    "  tx = optax.sgd(\n",
    "      learning_rate=learning_rate,\n",
    "      momentum=momentum,\n",
    "      nesterov=True,\n",
    "  )\n",
    "  # We will need to define a custom TrainState if we want to do use \n",
    "  # more advanced networks\n",
    "  state = train_state.TrainState.create(\n",
    "      apply_fn=model.apply,\n",
    "      params=params,\n",
    "      tx=tx,\n",
    "  )\n",
    "  return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main training step\n",
    "from jax import lax\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "  one_hot_labels = jax.nn.one_hot(labels, num_classes=NUM_CLASSES)\n",
    "  xentropy = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)\n",
    "  return jnp.mean(xentropy)\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "  loss = cross_entropy_loss(logits, labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "\n",
    "  # Combine the results across the devices. So, all metrics end up being\n",
    "  # means of means\n",
    "  metrics = lax.pmean(metrics, axis_name='batch')\n",
    "  return metrics\n",
    "\n",
    "def train_step(state, batch, learning_rate):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    \"\"\"loss function used for training.\"\"\"\n",
    "    logits = state.apply_fn(\n",
    "        {'params': params},\n",
    "        batch['image']\n",
    "    )\n",
    "    loss = cross_entropy_loss(logits, batch['label'])\n",
    "    weight_penalty_params = jax.tree_util.tree_leaves(params)\n",
    "    weight_decay = 0.0001\n",
    "    weight_l2 = sum(\n",
    "        jnp.sum(x**2) for x in weight_penalty_params if x.ndim > 1\n",
    "    )\n",
    "    weight_penalty = weight_decay * 0.5 * weight_l2\n",
    "    loss = loss + weight_penalty\n",
    "    return loss, logits\n",
    "\n",
    "  lr = learning_rate\n",
    "\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  aux, grads = grad_fn(state.params)\n",
    "  # Re-use same axis_name as in the call to `pmap(...train_step...)` below.\n",
    "  grads = lax.pmean(grads, axis_name='batch')\n",
    "\n",
    "  # Not currently using, but you can get the the variables out of the \n",
    "  # new_model_state here\n",
    "  logits = aux[1]\n",
    "  metrics = compute_metrics(logits, batch['label'])\n",
    "  metrics['learning_rate'] = lr\n",
    "\n",
    "  # You will also need to update any other custom state variables like batch\n",
    "  # stats here.\n",
    "  new_state = state.apply_gradients(\n",
    "      grads=grads\n",
    "  )\n",
    "\n",
    "  return new_state, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a separate function for evaluation. No weight decay or loss needs\n",
    "# to be calculated. We might want to monitor the loss though for overtraining.\n",
    "def eval_step(state, batch):\n",
    "  variables = {'params': state.params}\n",
    "  logits = state.apply_fn(variables, batch['image'], train=False)\n",
    "  return compute_metrics(logits, batch['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:{'train_accuracy': 0.7897656, 'train_learning_rate': 0.009999999, 'train_loss': 0.71739966}\n",
      "INFO:absl:[200] train_accuracy=0.7897655963897705, train_learning_rate=0.009999998845160007, train_loss=0.7173996567726135\n",
      "INFO:absl:{'train_accuracy': 0.9203906, 'train_learning_rate': 0.009999999, 'train_loss': 0.2663974}\n",
      "INFO:absl:[400] train_accuracy=0.9203906059265137, train_learning_rate=0.009999998845160007, train_loss=0.2663973867893219\n",
      "INFO:absl:{'train_accuracy': 0.9429687, 'train_learning_rate': 0.009999999, 'train_loss': 0.19493891}\n",
      "INFO:absl:[600] train_accuracy=0.9429687261581421, train_learning_rate=0.009999998845160007, train_loss=0.19493891298770905\n",
      "INFO:absl:{'train_accuracy': 0.95109373, 'train_learning_rate': 0.009999999, 'train_loss': 0.16796201}\n",
      "INFO:absl:[800] train_accuracy=0.9510937333106995, train_learning_rate=0.009999998845160007, train_loss=0.16796201467514038\n",
      "INFO:absl:test epoch: 0, loss: 0.0000, accuracy: 0.00\n",
      "INFO:absl:{'train_accuracy': 0.9627344, 'train_learning_rate': 0.009999999, 'train_loss': 0.12918855}\n",
      "INFO:absl:[1000] train_accuracy=0.9627344012260437, train_learning_rate=0.009999998845160007, train_loss=0.12918855249881744\n",
      "INFO:absl:{'train_accuracy': 0.9689062, 'train_learning_rate': 0.009999999, 'train_loss': 0.10357891}\n",
      "INFO:absl:[1200] train_accuracy=0.9689062237739563, train_learning_rate=0.009999998845160007, train_loss=0.10357891023159027\n",
      "INFO:absl:{'train_accuracy': 0.9692969, 'train_learning_rate': 0.009999999, 'train_loss': 0.10063533}\n",
      "INFO:absl:[1400] train_accuracy=0.9692968726158142, train_learning_rate=0.009999998845160007, train_loss=0.10063532739877701\n",
      "INFO:absl:{'train_accuracy': 0.9702344, 'train_learning_rate': 0.009999999, 'train_loss': 0.09606897}\n",
      "INFO:absl:[1600] train_accuracy=0.9702343940734863, train_learning_rate=0.009999998845160007, train_loss=0.09606897085905075\n",
      "INFO:absl:{'train_accuracy': 0.97515625, 'train_learning_rate': 0.009999999, 'train_loss': 0.08349081}\n",
      "INFO:absl:[1800] train_accuracy=0.9751562476158142, train_learning_rate=0.009999998845160007, train_loss=0.08349081128835678\n",
      "INFO:absl:test epoch: 1, loss: 0.0000, accuracy: 0.00\n",
      "INFO:absl:{'train_accuracy': 0.9775781, 'train_learning_rate': 0.009999999, 'train_loss': 0.07709157}\n",
      "INFO:absl:[2000] train_accuracy=0.9775781035423279, train_learning_rate=0.009999998845160007, train_loss=0.07709156721830368\n",
      "INFO:absl:{'train_accuracy': 0.9783594, 'train_learning_rate': 0.009999999, 'train_loss': 0.06907071}\n",
      "INFO:absl:[2200] train_accuracy=0.9783594012260437, train_learning_rate=0.009999998845160007, train_loss=0.0690707117319107\n",
      "INFO:absl:{'train_accuracy': 0.97734374, 'train_learning_rate': 0.009999999, 'train_loss': 0.06861652}\n",
      "INFO:absl:[2400] train_accuracy=0.977343738079071, train_learning_rate=0.009999998845160007, train_loss=0.06861651688814163\n",
      "INFO:absl:{'train_accuracy': 0.97945315, 'train_learning_rate': 0.009999999, 'train_loss': 0.06810683}\n",
      "INFO:absl:[2600] train_accuracy=0.9794531464576721, train_learning_rate=0.009999998845160007, train_loss=0.06810683012008667\n",
      "INFO:absl:{'train_accuracy': 0.9805469, 'train_learning_rate': 0.009999999, 'train_loss': 0.061658956}\n",
      "INFO:absl:[2800] train_accuracy=0.9805468916893005, train_learning_rate=0.009999998845160007, train_loss=0.06165895611047745\n",
      "INFO:absl:test epoch: 2, loss: 0.0000, accuracy: 0.00\n",
      "INFO:absl:{'train_accuracy': 0.98296875, 'train_learning_rate': 0.009999999, 'train_loss': 0.058748506}\n",
      "INFO:absl:[3000] train_accuracy=0.9829687476158142, train_learning_rate=0.009999998845160007, train_loss=0.058748506009578705\n",
      "INFO:absl:{'train_accuracy': 0.9830469, 'train_learning_rate': 0.009999999, 'train_loss': 0.05346735}\n",
      "INFO:absl:[3200] train_accuracy=0.9830468893051147, train_learning_rate=0.009999998845160007, train_loss=0.05346734821796417\n",
      "INFO:absl:{'train_accuracy': 0.9828125, 'train_learning_rate': 0.009999999, 'train_loss': 0.05335103}\n",
      "INFO:absl:[3400] train_accuracy=0.9828125238418579, train_learning_rate=0.009999998845160007, train_loss=0.053351029753685\n",
      "INFO:absl:{'train_accuracy': 0.9838281, 'train_learning_rate': 0.009999999, 'train_loss': 0.05289504}\n",
      "INFO:absl:[3600] train_accuracy=0.9838281273841858, train_learning_rate=0.009999998845160007, train_loss=0.052895039319992065\n",
      "INFO:absl:test epoch: 3, loss: 0.0000, accuracy: 0.00\n",
      "INFO:absl:{'train_accuracy': 0.9842188, 'train_learning_rate': 0.009999999, 'train_loss': 0.052096967}\n",
      "INFO:absl:[3800] train_accuracy=0.9842187762260437, train_learning_rate=0.009999998845160007, train_loss=0.05209696665406227\n",
      "INFO:absl:{'train_accuracy': 0.9863281, 'train_learning_rate': 0.009999999, 'train_loss': 0.04366846}\n",
      "INFO:absl:[4000] train_accuracy=0.986328125, train_learning_rate=0.009999998845160007, train_loss=0.043668460100889206\n",
      "INFO:absl:{'train_accuracy': 0.98695314, 'train_learning_rate': 0.009999999, 'train_loss': 0.04466884}\n",
      "INFO:absl:[4200] train_accuracy=0.9869531393051147, train_learning_rate=0.009999998845160007, train_loss=0.04466883838176727\n",
      "INFO:absl:{'train_accuracy': 0.9857031, 'train_learning_rate': 0.009999999, 'train_loss': 0.047329683}\n",
      "INFO:absl:[4400] train_accuracy=0.9857031106948853, train_learning_rate=0.009999998845160007, train_loss=0.04732968285679817\n",
      "INFO:absl:{'train_accuracy': 0.98664063, 'train_learning_rate': 0.009999999, 'train_loss': 0.0408642}\n",
      "INFO:absl:[4600] train_accuracy=0.9866406321525574, train_learning_rate=0.009999998845160007, train_loss=0.04086419939994812\n",
      "INFO:absl:test epoch: 4, loss: 0.0000, accuracy: 0.00\n",
      "INFO:absl:{'train_accuracy': 0.98710936, 'train_learning_rate': 0.009999999, 'train_loss': 0.041595317}\n",
      "INFO:absl:[4800] train_accuracy=0.987109363079071, train_learning_rate=0.009999998845160007, train_loss=0.04159531742334366\n",
      "INFO:absl:{'train_accuracy': 0.9885156, 'train_learning_rate': 0.009999999, 'train_loss': 0.03789254}\n",
      "INFO:absl:[5000] train_accuracy=0.9885156154632568, train_learning_rate=0.009999998845160007, train_loss=0.03789253905415535\n",
      "INFO:absl:{'train_accuracy': 0.9882031, 'train_learning_rate': 0.009999999, 'train_loss': 0.038147382}\n",
      "INFO:absl:[5200] train_accuracy=0.9882031083106995, train_learning_rate=0.009999998845160007, train_loss=0.03814738243818283\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(steps_per_epoch\u001b[38;5;241m*\u001b[39mnum_epochs), it):\n\u001b[0;32m---> 38\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mp_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     train_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Calculate train metrics every so often (metrics from \u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# previous steps is discarded then)\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, trace)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Here we will build up the train and evaluate loop\n",
    "import functools\n",
    "from flax.training import common_utils\n",
    "from clu import metric_writers\n",
    "\n",
    "num_epochs = 10\n",
    "workdir = './pmap_tests'\n",
    "\n",
    "writer = metric_writers.create_default_writer(\n",
    "  logdir=workdir, just_logging=jax.process_index() != 0\n",
    ")\n",
    "\n",
    "rng = jax.random.key(0)\n",
    "model = CNN()\n",
    "\n",
    "steps_per_epoch = (\n",
    "      num_train_steps // batch_size\n",
    "  )\n",
    "\n",
    "state = create_train_state(rng, model, image_size, num_channels, learning_rate, momentum)\n",
    "\n",
    "# State is replicated across devices - we will run the data on both devices, but\n",
    "# each device will have to have state (assume both are updated after each \n",
    "# training loop)\n",
    "state = jax_utils.replicate(state)\n",
    "\n",
    "# We pmap the training step for automatic sharding of the data array into \n",
    "# number of devices. Everything produced by the model then needs to be \n",
    "# averaged\n",
    "p_train_step = jax.pmap(\n",
    "      functools.partial(train_step, learning_rate=learning_rate),\n",
    "      axis_name='batch',\n",
    "  )\n",
    "\n",
    "train_metrics = []\n",
    "\n",
    "for step, batch in zip(range(steps_per_epoch*num_epochs), it):\n",
    "    state, metrics = p_train_step(state, batch)\n",
    "\n",
    "    train_metrics.append(metrics)\n",
    "    # Calculate train metrics every so often (metrics from \n",
    "    # previous steps is discarded then)\n",
    "    if (step + 1) % 200 == 0:\n",
    "      train_metrics = common_utils.get_metrics(train_metrics)\n",
    "        \n",
    "      summary = {\n",
    "        f'train_{k}': v\n",
    "        for k, v in jax.tree_util.tree_map(\n",
    "          lambda x: x.mean(), train_metrics\n",
    "          ).items()\n",
    "        }\n",
    "      logging.info(summary)\n",
    "      writer.write_scalars(step + 1, summary)\n",
    "      train_metrics = []\n",
    "    \n",
    "    if (step + 1) % steps_per_epoch == 0:\n",
    "        epoch = step // steps_per_epoch\n",
    "\n",
    "        logging.info(\n",
    "          'test epoch: %d, loss: %.4f, accuracy: %.2f',\n",
    "          epoch,\n",
    "          0,\n",
    "          0 * 100,\n",
    "      )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': Array([0.03125, 0.03125], dtype=float32),\n",
       " 'learning_rate': Array([0.01, 0.01], dtype=float32, weak_type=True),\n",
       " 'loss': Array([2.3066719, 2.3066719], dtype=float32)}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
