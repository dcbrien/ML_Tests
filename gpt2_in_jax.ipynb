{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "# Here is the dictionary tree we have to replicate. pytrees are just nested dictionaries.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "# TODO:\n",
    "# - Add options for different dtypes of the model\n",
    "# - Add options for difference precisions (and understand that better - I think it has to do with the mantissa of the floats\n",
    "# - Add istraining variable to allow dropout and other operations that are different during training and inference\n",
    "# - Maybe: Flash attention?\n",
    "# - Maybe: Sharding for multiple gpu training?\n",
    "# - Use the Jax configuration utilities instead of the GPTConfig class\n",
    "# - Develop my own training routing with all of the bells and whistles required: timing, checkpointinng, etc...\n",
    "# - Move everything to python files for more professional-like code from the command line\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        #self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        # Had to use Einsum to match the matrix multiplication of GPT2 and pytorch. They accept \n",
    "        # both shapes and then multiply by the transpose of the matrix (basically means the \n",
    "        # shape of the matrix is transpose, but the operation is the same). I confirmed that this\n",
    "        # produces the same result as the complicated huggingface conv1d version (conv1d is also\n",
    "        # just a linear matrix operation as well). They do add a lot of variables for \n",
    "        # mixed-precision training, that I do not.\n",
    "        # TODO: Move this into a new module as I am repeating it everywhere\n",
    "        self.c_fc = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "        #self.c_proj = nn.Dense(self.config.n_embd)\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, 4 * self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTAttention(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    # we will need to roll our own attention module because the built in one has a bunch of different\n",
    "    # naming and structure compared to the original GPT, which just references the projection layers\n",
    "    def setup(self):\n",
    "        # The first thing we do is project up to 3x the model size, because we are going to split\n",
    "        # the data into q, v, k\n",
    "        self.c_attn = nn.Einsum((3 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "        # At the end we have to project everything back to the regular model size of n_embd\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        B, T, C = jnp.shape(x)\n",
    "        \n",
    "        # Project to qkv\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
    "\n",
    "        query_length, key_length = q.shape[1], k.shape[1]\n",
    "\n",
    "        # Now reshape with a new head \"batch\" dimension\n",
    "        k = jnp.reshape(k, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        q = jnp.reshape(q, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        v = jnp.reshape(v, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "\n",
    "        # Make the attention mask\n",
    "        # TODO: For round 1 I'm just trusting the linen functions. They seem to be doing the correct thing here, but I may have to \n",
    "        # return to this for a closer look at the math if I'm not getting the GPT2 results\n",
    "        # Just copied this from the huggingface code to be consistent. First part just broadcasts the causal masks to the batch\n",
    "        # dimensions (replicating it as a lower triangular matrix of truths). The attention_bias is a stripped down version of the \n",
    "        # huggingface code, but the bias has to be floats. They bias the attention softmax, so basically set to -inf where you\n",
    "        # want it ignored. This will need to be OR'd with an attention mask in certain situations, like encoder/decoder networks.\n",
    "        # TODO: Also, this mask does not need to be applied during inference, so we could have an 'istraining' variable passed \n",
    "        # down the network for these cases and then ignore the mask calculations for increased speed. In the Flax examples, they \n",
    "        # also appear to cache the results of previous calculations, which I guess makes sense because we are just adding one \n",
    "        # token at a time to the input sequence and then calculating again. There is no point in recalculating the previous \n",
    "        # token outputs every time. I should probbaly implement some version of that too.\n",
    "        c_mask = nn.make_causal_mask(jnp.ones((1, self.config.block_size), dtype=\"bool\"), dtype=\"bool\")\n",
    "        c_mask = c_mask[:, :, :query_length, :key_length]\n",
    "        c_mask = jnp.broadcast_to(c_mask, (B,) + c_mask.shape[1:])\n",
    "\n",
    "        attention_bias = jax.lax.select(\n",
    "                c_mask > 0,\n",
    "                jnp.full(c_mask.shape, 0.0).astype(jnp.float32),\n",
    "                jnp.full(c_mask.shape, jnp.finfo(jnp.float32).min).astype(jnp.float32),\n",
    "            )\n",
    "\n",
    "        # use the built in flax libraries to calculate the attention matrix - attention weights are not returned, but could be \n",
    "        # I think bias gives more control compared to mask - i.e. bias can be a float. They might result in the same output with a \n",
    "        # boolean mask, but I will have to test that.\n",
    "        # TODO: I don't think Flax has a flash attention module. Is there any way to add that for Flax that will actually be \n",
    "        # optimized for hardware? I don't know.\n",
    "        y = nn.dot_product_attention(q, k, v, bias=attention_bias)\n",
    "\n",
    "        # Merge the heads back together\n",
    "        y = y.reshape((B, T, C))\n",
    "\n",
    "        # Project output with a FC layer\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm(epsilon=1e-05)\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        self.attn = GPTAttention(self.config)\n",
    "        self.ln_2 = nn.LayerNorm(epsilon=1e-05)\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        x = inputs + self.attn(x)\n",
    "        inputs2 = x\n",
    "        x = self.ln_2(x)\n",
    "        x = inputs2 + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm(epsilon=1e-05)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        input_shape = jnp.shape(x)       \n",
    "\n",
    "        x = self.wte(x)\n",
    "\n",
    "        # For the positional encodings we need an index that is simple the position\n",
    "        # of each token. This will be the same shape as the input, but will simply\n",
    "        # be repeating and increasing numbers from 1.\n",
    "        # jnp.atleast_2d is needed so we can initialize with a batch size of 1\n",
    "        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(inputs).shape[-1]), input_shape)\n",
    "        x_wpe = self.wpe(position_ids)\n",
    "        \n",
    "        x += x_wpe\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "        # So the Flax model does not return parameters for lm_head, because lm_head is simply the \n",
    "        # inverse operation of the initial word embedding, so we can just reuse those weights. They\n",
    "        # use the term 'tied' for this. The weights are tied together.\n",
    "        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # This is from the huggingface code - might as well reuse it here.\n",
    "        shared_kernel = self.transformer.variables['params']['wte']['embedding'].T\n",
    "        lm_logits = self.lm_head.apply({'params': {'kernel': shared_kernel}}, x)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, GPTConfig.vocab_size, (1, GPTConfig.block_size), dtype='i4')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import tiktoken\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "#from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "# Generate a starting sequence and we will generate the rest of the sequence with GPT2 for\n",
    "# however many batches we need\n",
    "num_return_sequences = 20\n",
    "max_length = 50\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"The advance of machine learning\")\n",
    "\n",
    "tokens = jnp.array(tokens, dtype=\"i4\")\n",
    "\n",
    "initial_input = jnp.atleast_2d(tokens).repeat(num_return_sequences, 0)\n",
    "\n",
    "init_length=jnp.shape(initial_input)[-1]\n",
    "\n",
    "# Has to be a constant length for the fast-autogression, so we need to pad (we will ignore the pads until \n",
    "# those indicies are actually needed by the model as it grows)\n",
    "initial_input=jnp.pad(initial_input, ((0, 0), (0, max_length-init_length)), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax-style fast autoregression using the jax looping tools. This takes a bit of time to get correct\n",
    "# with each model as you have to design it using the XLA tools and every single parameter needs to \n",
    "# be deterministic. This leads to lots of gotchas. For example, dynamic_slice and dynamic_slice_update\n",
    "# need to have a constant for the size, but can be placed anywhere in the matrix (i.e., you can't use\n",
    "# a variable with this). And, take_along_axis needs to be used for the top_k type indexing as it uses \n",
    "# the XLA gather method underneath, which is super complicated to use by hand. Every model produces \n",
    "# new difficulties and I'm sure this can be optimized quite a bit if I understood jax a bit better.\n",
    "rng = jax.random.key(2)\n",
    "\n",
    "temperature = 0.7\n",
    "\n",
    "def get_sentence(n, carry):\n",
    "    x, rng = carry\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    # pull out only the sequence up until the currently filled in\n",
    "    # values. It would be better if we could pass in a variable \n",
    "    # length sequence, but I don't think that's allowed by jax\n",
    "    logits = GPT2.apply({'params': model_hf.params}, x)\n",
    "\n",
    "    # GPT2 predicted the new token int he last column of the output, \n",
    "    # so we only need this.\n",
    "    logits = logits[:, n, :]\n",
    "\n",
    "    # Sample from the top 50 tokens\n",
    "    topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "\n",
    "    ix = jnp.expand_dims(jax.random.categorical(rng, topk_probs/temperature, axis=-1), axis=-1)\n",
    "\n",
    "    #print(ix)\n",
    "\n",
    "    xcol=jnp.atleast_2d(jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1))\n",
    "\n",
    "    x = jax.lax.dynamic_update_slice(x, xcol, (0, n+1))\n",
    "\n",
    "    return x, rng\n",
    "\n",
    "x, rng = jax.lax.fori_loop(init_length-1, max_length-1, get_sentence, (initial_input, rng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The advance of machine learning algorithms and the development of machine learning platforms allow us to rapidly develop and test new techniques for solving complex problems in real-time.\n",
      "\n",
      "Our customers can be expected to contribute to the development of a new algorithm.\n",
      "\n",
      "\n",
      "The advance of machine learning and machine learning with algorithms is a long way away. Machine learning is one of the most important tools for improving data science and machine learning development. However, one of the many challenges of machine learning is learning for information. Data\n",
      "The advance of machine learning has brought with it a new era of artificial intelligence and machine learning is now one of the most exciting developments in recent memory.\n",
      "\n",
      "The machine learning toolkit that we are using is called Deep Learning, and it is a\n",
      "The advance of machine learning has made it possible to design artificial intelligence to handle complex situations. But that is only one part of the story. The next part is the transformation into real-time AI that we are going to see in the next few years\n",
      "The advance of machine learning to machine learning is not a direct result of the computer science paradigm, but rather comes from a process of experimentation, the development of techniques that allow for parallel, non-cooperative processes.\n",
      "\n",
      "Machine learning makes it possible\n",
      "The advance of machine learning in AI is crucial to the development of future AI technologies.\n",
      "\n",
      "This means that the AI community should focus on how to better understand how machine learning should be used to improve our own lives.\n",
      "\n",
      "This is key because\n",
      "The advance of machine learning on the web has had a very positive effect on the performance of the Internet of Things (IoT) ecosystem.\n",
      "\n",
      "For example, Microsoft is now providing developers with a new framework to extend and improve the IoT.\n",
      "The advance of machine learning has been a big deal in the past few years. In the past seven years, we've seen some of the best machine learning algorithms ever developed. In the past few weeks, we've seen two of the most popular machine\n",
      "The advance of machine learning to machine learning is a good thing, but it does not take into account the complexity of a particular problem. Machine learning is an important tool for machine learning because it is the first step towards a better understanding of the properties of\n",
      "The advance of machine learning has been a big deal in the last couple of years. Computers have come to enjoy the power of machine learning, and we can see a lot of this in the AI fields – particularly the AI fields such as Artificial Intelligence\n",
      "The advance of machine learning will lead to better, more accurate, and more scalable software development.\"\n",
      "\n",
      "A second paper by the researchers is a series of studies by researchers at MIT and Stanford.\n",
      "\n",
      "Explore further: Machine learning will lead to better\n",
      "The advance of machine learning to a wide range of tasks, it is expected to be the next big thing in business software. While there are many examples of these advancements, there are also many more that are still in development and are still unknown to the\n",
      "The advance of machine learning to the consumer is a major step towards the industry's goal of automating the process of manufacturing.\n",
      "\n",
      "The company has been developing an innovative approach to learn how to learn to program, with the goal of creating a platform\n",
      "The advance of machine learning is not only making it much more scalable, it also offers us a much better way to make it happen – a new kind of machine learning API that enables us to make predictions about the future and predict the future in a way\n",
      "The advance of machine learning has been a huge success in the last few years, but this trend is slowing down quickly. Machine learning is now used in many other industries, such as medical or law, but it's still not as widespread as it once\n",
      "The advance of machine learning has led to a wave of research into the future of computer vision and artificial intelligence. Artificial intelligence has been used to predict weather, travel, and business transactions, but new research shows that the technology has been used to build better\n",
      "The advance of machine learning systems such as machine learning has been the subject of fierce debate and speculation. In the last few years, however, the technology has expanded exponentially, and with it, the value of computer vision. As the study of machine learning\n",
      "The advance of machine learning from the early days of artificial intelligence is already underway. The advent of machine learning as an important tool for the human race is in full force. The world's population has grown by about one-third since the 1950s—\n",
      "The advance of machine learning is also accelerating. This could be due to a number of factors:\n",
      "\n",
      "For all the advances in AI, we still have to build new ones. For example, we still need to grow and test new things. We\n",
      "The advance of machine learning, of course, is a challenge. It requires a lot of effort, and even more effort than with humans. We've had so many advances in machine learning that we've been able to have a more robust understanding of the\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for i in range(0, x.shape[0]):\n",
    "    print(enc.decode(x[i,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n",
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n"
     ]
    }
   ],
   "source": [
    "# Just comparing the shapes of our model vs the GPT2 trained weights.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer'])))\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
