{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested flags to set from the jax docs\n",
    "import os\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    #'--xla_gpu_enable_triton_softmax_fusion=true '\n",
    "    #'--xla_gpu_triton_gemm_any=True '\n",
    "    #'--xla_gpu_enable_async_collectives=true '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    "    '--xla_gpu_enable_highest_priority_async_stream=true '\n",
    ")\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 19:56:02.943577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-20 19:56:02.943675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-20 19:56:02.953796: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-20 19:56:12.113952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained weights from huggingface - Do not run this if you are \n",
    "# planning to train your own model. It will just take up memory\n",
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "# Here is the dictionary tree we have to replicate. pytrees are just nested dictionaries.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from flax import linen as nn\n",
    "from typing import Any\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "# TODO:\n",
    "# - Add istraining variable to allow dropout and other operations that are different during training and inference\n",
    "# - Use the Jax configuration utilities instead of the GPTConfig class\n",
    "# - Develop my own training routing with all of the bells and whistles required: timing, checkpointinng, etc...\n",
    "# - Move everything to python files for more professional-like code from the command line\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dtype: Any = jnp.bfloat16\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        #self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        # Had to use Einsum to match the matrix multiplication of GPT2 and pytorch. They accept \n",
    "        # both shapes and then multiply by the transpose of the matrix (basically means the \n",
    "        # shape of the matrix is transpose, but the operation is the same). I confirmed that this\n",
    "        # produces the same result as the complicated huggingface conv1d version (conv1d is also\n",
    "        # just a linear matrix operation as well). They do add a lot of variables for \n",
    "        # mixed-precision training, that I do not.\n",
    "        # TODO: Move this into a new module as I am repeating it everywhere\n",
    "        #self.c_fc = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        self.c_fc = nn.Dense(self.config.n_embd * 4, kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        #self.c_proj = nn.Dense(self.config.n_embd)\n",
    "        #self.c_proj = nn.Einsum((self.config.n_embd, 4 * self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        self.c_proj = nn.Dense(self.config.n_embd, kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTAttention(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    # we will need to roll our own attention module because the built in one has a bunch of different\n",
    "    # naming and structure compared to the original GPT, which just references the projection layers\n",
    "    def setup(self):\n",
    "        # The first thing we do is project up to 3x the model size, because we are going to split\n",
    "        # the data into q, v, k\n",
    "        #self.c_attn = nn.Einsum((3 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        self.c_attn = nn.Dense(3 * self.config.n_embd, kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        \n",
    "        # At the end we have to project everything back to the regular model size of n_embd\n",
    "        #self.c_proj = nn.Einsum((self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=(self.config.n_layer*2) ** -0.5), dtype=self.config.dtype)\n",
    "        self.c_proj = nn.Dense(self.config.n_embd, kernel_init=jax.nn.initializers.normal(stddev=(self.config.n_layer*2) ** -0.5), dtype=self.config.dtype)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        B, T, C = jnp.shape(x)\n",
    "        \n",
    "        # Project to qkv\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
    "\n",
    "        query_length, key_length = q.shape[1], k.shape[1]\n",
    "\n",
    "        # Now reshape with a new head \"batch\" dimension\n",
    "        k = jnp.reshape(k, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        q = jnp.reshape(q, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        v = jnp.reshape(v, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "\n",
    "        # Make the attention mask\n",
    "        # TODO: For round 1 I'm just trusting the linen functions. They seem to be doing the correct thing here, but I may have to \n",
    "        # return to this for a closer look at the math if I'm not getting the GPT2 results\n",
    "        # Just copied this from the huggingface code to be consistent. First part just broadcasts the causal masks to the batch\n",
    "        # dimensions (replicating it as a lower triangular matrix of truths). The attention_bias is a stripped down version of the \n",
    "        # huggingface code, but the bias has to be floats. They bias the attention softmax, so basically set to -inf where you\n",
    "        # want it ignored. This will need to be OR'd with an attention mask in certain situations, like encoder/decoder networks.\n",
    "        # TODO: Also, this mask does not need to be applied during inference, so we could have an 'istraining' variable passed \n",
    "        # down the network for these cases and then ignore the mask calculations for increased speed. In the Flax examples, they \n",
    "        # also appear to cache the results of previous calculations, which I guess makes sense because we are just adding one \n",
    "        # token at a time to the input sequence and then calculating again. There is no point in recalculating the previous \n",
    "        # token outputs every time. I should probbaly implement some version of that too.\n",
    "        c_mask = nn.make_causal_mask(jnp.ones((1, self.config.block_size), dtype=\"bool\"), dtype=\"bool\")\n",
    "        c_mask = c_mask[:, :, :query_length, :key_length]\n",
    "        c_mask = jnp.broadcast_to(c_mask, (B,) + c_mask.shape[1:])\n",
    "\n",
    "        attention_bias = jax.lax.select(\n",
    "                c_mask > 0,\n",
    "                jnp.full(c_mask.shape, 0.0).astype(self.config.dtype),\n",
    "                jnp.full(c_mask.shape, jnp.finfo(self.config.dtype).min).astype(self.config.dtype),\n",
    "            )\n",
    "\n",
    "        # use the built in flax libraries to calculate the attention matrix - attention weights are not returned, but could be \n",
    "        # I think bias gives more control compared to mask - i.e. bias can be a float. They might result in the same output with a \n",
    "        # boolean mask, but I will have to test that.\n",
    "        # TODO: I don't think Flax has a flash attention module. Is there any way to add that for Flax that will actually be \n",
    "        # optimized for hardware? I don't know.\n",
    "        y = nn.dot_product_attention(q, k, v, bias=attention_bias, dtype=self.config.dtype)\n",
    "\n",
    "        # Merge the heads back together\n",
    "        y = y.reshape((B, T, C))\n",
    "\n",
    "        # Project output with a FC layer\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        self.attn = GPTAttention(self.config)\n",
    "        self.ln_2 = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        x = inputs + self.attn(x)\n",
    "        inputs2 = x\n",
    "        x = self.ln_2(x)\n",
    "        x = inputs2 + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd, embedding_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd, embedding_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        input_shape = jnp.shape(x)       \n",
    "\n",
    "        x = self.wte(x)\n",
    "\n",
    "        # For the positional encodings we need an index that is simple the position\n",
    "        # of each token. This will be the same shape as the input, but will simply\n",
    "        # be repeating and increasing numbers from 1.\n",
    "        # jnp.atleast_2d is needed so we can initialize with a batch size of 1\n",
    "        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(inputs).shape[-1]), input_shape)\n",
    "        x_wpe = self.wpe(position_ids)\n",
    "        \n",
    "        x += x_wpe\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "        # So the Flax model does not return parameters for lm_head, because lm_head is simply the \n",
    "        # inverse operation of the initial word embedding, so we can just reuse those weights. They\n",
    "        # use the term 'tied' for this. The weights are tied together.\n",
    "        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False, dtype=self.config.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # This is from the huggingface code - might as well reuse it here.\n",
    "        shared_kernel = self.transformer.variables['params']['wte']['embedding'].T\n",
    "        lm_logits = self.lm_head.apply({'params': {'kernel': shared_kernel}}, x)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, GPTConfig.vocab_size, (1, GPTConfig.block_size), dtype='i4')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import tiktoken\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "#from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "# Generate a starting sequence and we will generate the rest of the sequence with GPT2 for\n",
    "# however many batches we need\n",
    "num_return_sequences = 1\n",
    "max_length = 100\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"\")\n",
    "\n",
    "tokens = jnp.array(tokens, dtype=\"i4\")\n",
    "\n",
    "initial_input = jnp.atleast_2d(tokens).repeat(num_return_sequences, 0)\n",
    "\n",
    "init_length=jnp.shape(initial_input)[-1]\n",
    "\n",
    "# Has to be a constant length for the fast-autogression, so we need to pad (we will ignore the pads until \n",
    "# those indicies are actually needed by the model as it grows)\n",
    "initial_input=jnp.pad(initial_input, ((0, 0), (0, max_length-init_length)), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax-style fast autoregression using the jax looping tools. This takes a bit of time to get correct\n",
    "# with each model as you have to design it using the XLA tools and every single parameter needs to \n",
    "# be deterministic. This leads to lots of gotchas. For example, dynamic_slice and dynamic_slice_update\n",
    "# need to have a constant for the size, but can be placed anywhere in the matrix (i.e., you can't use\n",
    "# a variable with this). And, take_along_axis needs to be used for the top_k type indexing as it uses \n",
    "# the XLA gather method underneath, which is super complicated to use by hand. Every model produces \n",
    "# new difficulties and I'm sure this can be optimized quite a bit if I understood jax a bit better.\n",
    "rng = jax.random.key(27)\n",
    "\n",
    "temperature = 1.0\n",
    "\n",
    "def get_sentence(n, carry):\n",
    "    x, rng = carry\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    # pull out only the sequence up until the currently filled in\n",
    "    # values. It would be better if we could pass in a variable \n",
    "    # length sequence, but I don't think that's allowed by jax\n",
    "    logits = GPT2.apply({'params': jax_utils.unreplicate(state.params)}, x)\n",
    "\n",
    "    # GPT2 predicted the new token int he last column of the output, \n",
    "    # so we only need this.\n",
    "    logits = logits[:, n, :]\n",
    "\n",
    "    # Sample from the top 50 tokens\n",
    "    topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "\n",
    "    ix = jnp.expand_dims(jax.random.categorical(new_rng, topk_probs/temperature, axis=-1), axis=-1)\n",
    "\n",
    "    #print(ix)\n",
    "\n",
    "    xcol=jnp.atleast_2d(jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1))\n",
    "\n",
    "    x = jax.lax.dynamic_update_slice(x, xcol, (0, n+1))\n",
    "\n",
    "    return x, rng\n",
    "\n",
    "x, rng = jax.lax.fori_loop(init_length-1, max_length-1, get_sentence, (initial_input, rng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rie.\n",
      "\n",
      "RINGBRO doth in all.\n",
      "\n",
      "RICHARD II:\n",
      "\n",
      "Nay, she, I fear:\n",
      "' MARUEENRY BOLINGHAM:\n",
      "Than it, if you, thou, no more I pray you come at death?\n",
      "HORTIO:\n",
      "O, my husband,\n",
      "NANUS:\n",
      "The father's love.\n",
      "\n",
      "My mother.\n",
      "\n",
      "GLOUCESUS:\n",
      "\n",
      "To\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for i in range(0, x.shape[0]):\n",
    "    print(enc.decode(x[i,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n",
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n"
     ]
    }
   ],
   "source": [
    "# Just comparing the shapes of our model vs the GPT2 trained weights.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer'])))\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "GPT2 = GPT(GPTConfig(vocab_size=50304))\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(2), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x_init = np.random.randint(0, GPTConfig.vocab_size, (1, GPTConfig.block_size), dtype='i4')\n",
    "\n",
    "y_init = GPT2.init(key2, x_init)\n",
    "\n",
    "#logits = GPT2.apply({'params': y_init['params']}, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In jax we create a separate apply function to calculate the loss\n",
    "\n",
    "# From the flax examples. We should track some metrics as we train\n",
    "def compute_metrics(logits, labels, loss):\n",
    "    accuracy = jax.lax.pmean(jnp.mean(jnp.argmax(logits, -1) == labels), axis_name='batch')\n",
    "    metrics = {\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy,\n",
    "    }\n",
    "    metrics = jax.lax.pmean(metrics, axis_name='batch')\n",
    "    return metrics\n",
    "\n",
    "#@jax.jit\n",
    "def apply_model(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(\n",
    "            {'params': params}, \n",
    "            batch['x'])\n",
    "        #one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch['y']))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits)), grads = grad_fn(state.params)\n",
    "    #accuracy = jax.lax.pmean(jnp.mean(jnp.argmax(logits, -1) == batch['y']), axis_name='batch')\n",
    "\n",
    "    #loss = jax.lax.pmean(loss, axis_name='batch')\n",
    "    grads = jax.lax.pmean(grads, axis_name='batch')\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    metrics = compute_metrics(logits, batch['y'], loss)\n",
    "    return metrics, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do this for the entire tiny Shakespeare dataset\n",
    "# We can follow Karpathy's style, but we will use the tensorflow datasets\n",
    "# tools as our data loader as it is the natural thing to use with jax/flax\n",
    "# We can also then use prefetch to device, which should speed things up as well\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.num_devices = jax.device_count()\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        assert self.B % self.num_devices == 0, \"Number of devices must divide evenly into number of batches\"\n",
    "        \n",
    "        with open(\"input.txt\", 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        # Could load it into gpu memory here if you want for speed. It would \n",
    "        # depend on how much you have to spare\n",
    "        self.tokens = tokens\n",
    "        print(f'Loaded {len(self.tokens)} tokens')\n",
    "        print(f'One epoch = {len(self.tokens) // (B*T) } batches')\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        D, B, T = self.num_devices, self.B, self.T\n",
    "        buf = np.array(self.tokens[self.current_position:self.current_position+B*T+1])\n",
    "        x = buf[:-1].reshape((D, B // D, T))\n",
    "        y = buf[1:].reshape((D, B // D ,T))\n",
    "        self.current_position+=B*T\n",
    "\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        return {'x': x, 'y': y}\n",
    "\n",
    "    # The Tensorflow Data way\n",
    "    def getIterator(self):\n",
    "        D, B, T = self.num_devices, self.B, self.T\n",
    "\n",
    "        ds = tf.data.Dataset.from_tensor_slices(self.tokens)\n",
    "        \n",
    "        #x = np.array(self.tokens)\n",
    "        \n",
    "        #ds = tf.data.Dataset.from_tensor_slices(x[:-(x.shape[0] % (T+1))].reshape((-1, T+1)))\n",
    "\n",
    "        ds = ds.batch(B*T+1, drop_remainder=True).map(lambda x: {'x': tf.reshape(x[:-1], (D, B // D, T)), 'y': tf.reshape(x[1:], (D, B // D, T))}, num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE).cache().repeat()\n",
    "        \n",
    "        #ds = ds.shuffle(1000).batch(B*T+1, drop_remainder=True)(e(x[1:], (D, B //2, T))}).prefetch(tf.data.AUTOTUNE).cache().repeat()\n",
    "        \n",
    "        #ds = ds.shuffle(1000).batch(B, drop_remainder=True).prefetch(tf.data.AUTOTUNE).cache().repeat()        \n",
    "        return ds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tf_data(xs):\n",
    "  \"\"\"Convert a input batch from tf Tensors to numpy arrays.\"\"\"\n",
    "  local_device_count = jax.local_device_count()\n",
    "\n",
    "  def _prepare(x):\n",
    "    # Use _numpy() for zero-copy conversion between TF and NumPy.\n",
    "    x = x._numpy()  # pylint: disable=protected-access\n",
    "\n",
    "    # reshape (host_batch_size, height, width, 3) to\n",
    "    # (local_devices, device_batch_size, height, width, 3)\n",
    "    return {'x': x[:, :-1].reshape((2, 8, -1)), 'y': x[:, 1:].reshape((2, 8, -1))}\n",
    "\n",
    "  return jax.tree_util.tree_map(_prepare, xs)\n",
    "\n",
    "#it = map(prepare_tf_data, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "One epoch = 165 batches\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from flax import jax_utils\n",
    "from typing import Any\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "train_loader = DataLoaderLite(8, 256)\n",
    "\n",
    "#learning_rate = 0.0003\n",
    "max_lr = 6e-4\n",
    "\n",
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=max_lr*0.1,\n",
    "    peak_value=max_lr,\n",
    "    warmup_steps=10,\n",
    "    decay_steps=1_000,\n",
    "    end_value=max_lr*0.1\n",
    ")\n",
    "\n",
    "# Start with AdamW\n",
    "#tx = optax.adamw(learning_rate=learning_rate)\n",
    "tx = optax.chain(\n",
    "    optax.clip(1.0),\n",
    "    optax.adamw(learning_rate=schedule, b2=0.95, weight_decay=0.1),\n",
    "    )\n",
    "\n",
    "# Gradient accumulation here\n",
    "# Note that we can supply a function here instead if we want to adjust the batch sizes\n",
    "# as we train.\n",
    "tx = optax.MultiSteps(tx, every_k_schedule=2)\n",
    "\n",
    "# Flax handles the training state for us. Generally, you should subclass the TrainState and \n",
    "# add anything you need to it. Here, we don't need any additions, but we may in the future\n",
    "# so I will leave it in for now. Things you might add are the batch_stats or dropout keys\n",
    "class TrainState(train_state.TrainState):\n",
    "  key: jax.Array\n",
    "  batch_stats: Any\n",
    "\n",
    "# Replace this with our subclassed TrainState when we need it.\n",
    "state = train_state.TrainState.create(apply_fn=GPT2.apply, params=y_init['params'], tx=tx)\n",
    "\n",
    "# for distributed training we need to replicate the state\n",
    "state = jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 21:34:17.703404: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO:absl:The first step will take a bit of time to compile\n",
      "INFO:absl:[2] train_accuracy=0.0, train_loss=11, train_lr=0.0001140000531449914\n",
      "INFO:absl:{'train_accuracy': 0.0, 'train_loss': 11, 'train_lr': 0.00011400005}\n",
      "INFO:absl:{'train_accuracy': 0.0, 'train_loss': 10.9375, 'train_lr': 0.00016800003}\n",
      "INFO:absl:[3] train_accuracy=0.0, train_loss=10.9375, train_lr=0.00016800002777017653\n",
      "INFO:absl:{'train_accuracy': 0.0, 'train_loss': 10.75, 'train_lr': 0.00022200006}\n",
      "INFO:absl:[4] train_accuracy=0.0, train_loss=10.75, train_lr=0.00022200006060302258\n",
      "INFO:absl:[5] train_accuracy=0.0, train_loss=10.75, train_lr=0.0002760000352282077\n",
      "INFO:absl:{'train_accuracy': 0.0, 'train_loss': 10.75, 'train_lr': 0.00027600004}\n",
      "INFO:absl:{'train_accuracy': 0.11279297, 'train_loss': 10.4375, 'train_lr': 0.00033000004}\n",
      "INFO:absl:[6] train_accuracy=0.11279296875, train_loss=10.4375, train_lr=0.0003300000389572233\n",
      "INFO:absl:{'train_accuracy': 0.11035156, 'train_loss': 10.4375, 'train_lr': 0.00038400004}\n",
      "INFO:absl:[7] train_accuracy=0.1103515625, train_loss=10.4375, train_lr=0.0003840000426862389\n",
      "INFO:absl:{'train_accuracy': 0.1274414, 'train_loss': 10.1875, 'train_lr': 0.00043800002}\n",
      "INFO:absl:[8] train_accuracy=0.12744140625, train_loss=10.1875, train_lr=0.000438000017311424\n",
      "INFO:absl:{'train_accuracy': 0.12402344, 'train_loss': 10.1875, 'train_lr': 0.000492}\n",
      "INFO:absl:[9] train_accuracy=0.1240234375, train_loss=10.1875, train_lr=0.0004920000210404396\n",
      "INFO:absl:{'train_accuracy': 0.12646484, 'train_loss': 9.9375, 'train_lr': 0.000546}\n",
      "INFO:absl:[10] train_accuracy=0.12646484375, train_loss=9.9375, train_lr=0.0005460000247694552\n",
      "INFO:absl:{'train_accuracy': 0.13378906, 'train_loss': 9.9375, 'train_lr': 0.0006}\n",
      "INFO:absl:[11] train_accuracy=0.1337890625, train_loss=9.9375, train_lr=0.0006000000284984708\n",
      "INFO:absl:{'train_accuracy': 0.1303711, 'train_loss': 9.75, 'train_lr': 0.0005999987}\n",
      "INFO:absl:[12] train_accuracy=0.13037109375, train_loss=9.75, train_lr=0.0005999986897222698\n",
      "INFO:absl:{'train_accuracy': 0.15332031, 'train_loss': 9.625, 'train_lr': 0.0005999946}\n",
      "INFO:absl:[13] train_accuracy=0.1533203125, train_loss=9.625, train_lr=0.0005999946151860058\n",
      "INFO:absl:[14] train_accuracy=0.1279296875, train_loss=9.5625, train_lr=0.000599987804889679\n",
      "INFO:absl:{'train_accuracy': 0.12792969, 'train_loss': 9.5625, 'train_lr': 0.0005999878}\n",
      "INFO:absl:{'train_accuracy': 0.1352539, 'train_loss': 9.5, 'train_lr': 0.00059997826}\n",
      "INFO:absl:[15] train_accuracy=0.13525390625, train_loss=9.5, train_lr=0.0005999782588332891\n",
      "INFO:absl:[16] train_accuracy=0.1240234375, train_loss=9.4375, train_lr=0.0005999660352244973\n",
      "INFO:absl:{'train_accuracy': 0.12402344, 'train_loss': 9.4375, 'train_lr': 0.00059996604}\n",
      "INFO:absl:{'train_accuracy': 0.1381836, 'train_loss': 9.3125, 'train_lr': 0.00059995113}\n",
      "INFO:absl:[17] train_accuracy=0.13818359375, train_loss=9.3125, train_lr=0.0005999511340633035\n",
      "INFO:absl:{'train_accuracy': 0.11621094, 'train_loss': 9.3125, 'train_lr': 0.0005999334}\n",
      "INFO:absl:[18] train_accuracy=0.1162109375, train_loss=9.3125, train_lr=0.0005999333807267249\n",
      "INFO:absl:{'train_accuracy': 0.13574219, 'train_loss': 9.1875, 'train_lr': 0.000599913}\n",
      "INFO:absl:[19] train_accuracy=0.1357421875, train_loss=9.1875, train_lr=0.0005999130080454051\n",
      "INFO:absl:{'train_accuracy': 0.123535156, 'train_loss': 9, 'train_lr': 0.0005998899}\n",
      "INFO:absl:[20] train_accuracy=0.12353515625, train_loss=9, train_lr=0.0005998898996040225\n",
      "INFO:absl:{'train_accuracy': 0.11816406, 'train_loss': 9, 'train_lr': 0.00059986406}\n",
      "INFO:absl:[21] train_accuracy=0.1181640625, train_loss=9, train_lr=0.0005998640554025769\n",
      "INFO:absl:{'train_accuracy': 0.11328125, 'train_loss': 8.875, 'train_lr': 0.00059983553}\n",
      "INFO:absl:[22] train_accuracy=0.11328125, train_loss=8.875, train_lr=0.0005998355336487293\n",
      "INFO:absl:{'train_accuracy': 0.10888672, 'train_loss': 8.875, 'train_lr': 0.0005998043}\n",
      "INFO:absl:[23] train_accuracy=0.10888671875, train_loss=8.875, train_lr=0.0005998042761348188\n",
      "INFO:absl:{'train_accuracy': 0.12646484, 'train_loss': 8.5625, 'train_lr': 0.00059977034}\n",
      "INFO:absl:[24] train_accuracy=0.12646484375, train_loss=8.5625, train_lr=0.0005997703410685062\n",
      "INFO:absl:{'train_accuracy': 0.110839844, 'train_loss': 8.8125, 'train_lr': 0.0005997336}\n",
      "INFO:absl:[25] train_accuracy=0.11083984375, train_loss=8.8125, train_lr=0.0005997336120344698\n",
      "INFO:absl:{'train_accuracy': 0.11767578, 'train_loss': 8.5625, 'train_lr': 0.0005996942}\n",
      "INFO:absl:[26] train_accuracy=0.11767578125, train_loss=8.5625, train_lr=0.0005996942054480314\n",
      "INFO:absl:{'train_accuracy': 0.123046875, 'train_loss': 8.5, 'train_lr': 0.0005996521}\n",
      "INFO:absl:[27] train_accuracy=0.123046875, train_loss=8.5, train_lr=0.000599652121309191\n",
      "INFO:absl:{'train_accuracy': 0.11376953, 'train_loss': 8.3125, 'train_lr': 0.0005996072}\n",
      "INFO:absl:[28] train_accuracy=0.11376953125, train_loss=8.3125, train_lr=0.0005996071849949658\n",
      "INFO:absl:{'train_accuracy': 0.11035156, 'train_loss': 8.375, 'train_lr': 0.0005995597}\n",
      "INFO:absl:[29] train_accuracy=0.1103515625, train_loss=8.375, train_lr=0.0005995596875436604\n",
      "INFO:absl:{'train_accuracy': 0.110839844, 'train_loss': 8.0625, 'train_lr': 0.0005995094}\n",
      "INFO:absl:[30] train_accuracy=0.11083984375, train_loss=8.0625, train_lr=0.0005995093961246312\n",
      "INFO:absl:{'train_accuracy': 0.13427734, 'train_loss': 7.90625, 'train_lr': 0.0005994564}\n",
      "INFO:absl:[31] train_accuracy=0.13427734375, train_loss=7.90625, train_lr=0.0005994564271531999\n",
      "INFO:absl:{'train_accuracy': 0.11669922, 'train_loss': 7.8125, 'train_lr': 0.0005994007}\n",
      "INFO:absl:[32] train_accuracy=0.11669921875, train_loss=7.8125, train_lr=0.0005994007224217057\n",
      "INFO:absl:{'train_accuracy': 0.115234375, 'train_loss': 7.875, 'train_lr': 0.00059934234}\n",
      "INFO:absl:[33] train_accuracy=0.115234375, train_loss=7.875, train_lr=0.0005993423401378095\n",
      "INFO:absl:[34] train_accuracy=0.1279296875, train_loss=7.5625, train_lr=0.0005992811638861895\n",
      "INFO:absl:{'train_accuracy': 0.12792969, 'train_loss': 7.5625, 'train_lr': 0.00059928116}\n",
      "INFO:absl:{'train_accuracy': 0.1328125, 'train_loss': 7.53125, 'train_lr': 0.00059921737}\n",
      "INFO:absl:[35] train_accuracy=0.1328125, train_loss=7.53125, train_lr=0.0005992173682898283\n",
      "INFO:absl:{'train_accuracy': 0.12207031, 'train_loss': 7.34375, 'train_lr': 0.0005991508}\n",
      "INFO:absl:[36] train_accuracy=0.1220703125, train_loss=7.34375, train_lr=0.0005991507787257433\n",
      "INFO:absl:[37] train_accuracy=0.1240234375, train_loss=7.3125, train_lr=0.0005990815698169172\n",
      "INFO:absl:{'train_accuracy': 0.12402344, 'train_loss': 7.3125, 'train_lr': 0.00059908157}\n",
      "INFO:absl:{'train_accuracy': 0.11279297, 'train_loss': 7.34375, 'train_lr': 0.00059900957}\n",
      "INFO:absl:[38] train_accuracy=0.11279296875, train_loss=7.34375, train_lr=0.0005990095669403672\n",
      "INFO:absl:[39] train_accuracy=0.10791015625, train_loss=7.3125, train_lr=0.0005989348865114152\n",
      "INFO:absl:{'train_accuracy': 0.107910156, 'train_loss': 7.3125, 'train_lr': 0.0005989349}\n",
      "INFO:absl:{'train_accuracy': 0.111328125, 'train_loss': 7.125, 'train_lr': 0.0005988575}\n",
      "INFO:absl:[40] train_accuracy=0.111328125, train_loss=7.125, train_lr=0.0005988575285300612\n",
      "INFO:absl:{'train_accuracy': 0.13671875, 'train_loss': 7.25, 'train_lr': 0.00059877743}\n",
      "INFO:absl:[41] train_accuracy=0.13671875, train_loss=7.25, train_lr=0.0005987774347886443\n",
      "INFO:absl:{'train_accuracy': 0.10253906, 'train_loss': 7.28125, 'train_lr': 0.00059869466}\n",
      "INFO:absl:[42] train_accuracy=0.1025390625, train_loss=7.28125, train_lr=0.0005986946634948254\n",
      "INFO:absl:{'train_accuracy': 0.11376953, 'train_loss': 7.25, 'train_lr': 0.00059860916}\n",
      "INFO:absl:[43] train_accuracy=0.11376953125, train_loss=7.25, train_lr=0.0005986091564409435\n",
      "INFO:absl:{'train_accuracy': 0.12109375, 'train_loss': 7.09375, 'train_lr': 0.0005985209}\n",
      "INFO:absl:[44] train_accuracy=0.12109375, train_loss=7.09375, train_lr=0.0005985209136269987\n",
      "INFO:absl:{'train_accuracy': 0.125, 'train_loss': 7, 'train_lr': 0.00059843005}\n",
      "INFO:absl:[45] train_accuracy=0.125, train_loss=7, train_lr=0.0005984300514683127\n",
      "INFO:absl:{'train_accuracy': 0.115722656, 'train_loss': 7.03125, 'train_lr': 0.00059833645}\n",
      "INFO:absl:[46] train_accuracy=0.11572265625, train_loss=7.03125, train_lr=0.0005983364535495639\n",
      "INFO:absl:{'train_accuracy': 0.11816406, 'train_loss': 6.9375, 'train_lr': 0.0005982401}\n",
      "INFO:absl:[47] train_accuracy=0.1181640625, train_loss=6.9375, train_lr=0.0005982401198707521\n",
      "INFO:absl:{'train_accuracy': 0.11230469, 'train_loss': 6.96875, 'train_lr': 0.00059814105}\n",
      "INFO:absl:[48] train_accuracy=0.1123046875, train_loss=6.96875, train_lr=0.0005981410504318774\n",
      "INFO:absl:{'train_accuracy': 0.099609375, 'train_loss': 7.0625, 'train_lr': 0.00059803936}\n",
      "INFO:absl:[49] train_accuracy=0.099609375, train_loss=7.0625, train_lr=0.0005980393616482615\n",
      "INFO:absl:{'train_accuracy': 0.1015625, 'train_loss': 6.9375, 'train_lr': 0.00059793494}\n",
      "INFO:absl:[50] train_accuracy=0.1015625, train_loss=6.9375, train_lr=0.0005979349371045828\n",
      "INFO:absl:{'train_accuracy': 0.104003906, 'train_loss': 7.03125, 'train_lr': 0.00059782784}\n",
      "INFO:absl:[51] train_accuracy=0.10400390625, train_loss=7.03125, train_lr=0.000597827835008502\n",
      "INFO:absl:{'train_accuracy': 0.11035156, 'train_loss': 6.96875, 'train_lr': 0.000597718}\n",
      "INFO:absl:[52] train_accuracy=0.1103515625, train_loss=6.96875, train_lr=0.0005977179971523583\n",
      "INFO:absl:{'train_accuracy': 0.103027344, 'train_loss': 6.9375, 'train_lr': 0.00059760554}\n",
      "INFO:absl:[53] train_accuracy=0.10302734375, train_loss=6.9375, train_lr=0.0005976055399514735\n",
      "INFO:absl:[54] train_accuracy=0.10888671875, train_loss=6.875, train_lr=0.0005974902887828648\n",
      "INFO:absl:{'train_accuracy': 0.10888672, 'train_loss': 6.875, 'train_lr': 0.0005974903}\n",
      "INFO:absl:{'train_accuracy': 0.11816406, 'train_loss': 6.65625, 'train_lr': 0.0005973724}\n",
      "INFO:absl:[55] train_accuracy=0.1181640625, train_loss=6.65625, train_lr=0.000597372418269515\n",
      "INFO:absl:{'train_accuracy': 0.10546875, 'train_loss': 6.71875, 'train_lr': 0.0005972518}\n",
      "INFO:absl:[56] train_accuracy=0.10546875, train_loss=6.71875, train_lr=0.0005972518119961023\n",
      "INFO:absl:{'train_accuracy': 0.09814453, 'train_loss': 6.9375, 'train_lr': 0.0005971285}\n",
      "INFO:absl:[57] train_accuracy=0.09814453125, train_loss=6.9375, train_lr=0.0005971285281702876\n",
      "INFO:absl:{'train_accuracy': 0.10449219, 'train_loss': 6.875, 'train_lr': 0.00059700257}\n",
      "INFO:absl:[58] train_accuracy=0.1044921875, train_loss=6.875, train_lr=0.0005970025667920709\n",
      "INFO:absl:{'train_accuracy': 0.10498047, 'train_loss': 6.75, 'train_lr': 0.0005968739}\n",
      "INFO:absl:[59] train_accuracy=0.10498046875, train_loss=6.75, train_lr=0.0005968739278614521\n",
      "INFO:absl:{'train_accuracy': 0.106933594, 'train_loss': 6.65625, 'train_lr': 0.00059674255}\n",
      "INFO:absl:[60] train_accuracy=0.10693359375, train_loss=6.65625, train_lr=0.0005967425531707704\n",
      "INFO:absl:{'train_accuracy': 0.104003906, 'train_loss': 6.6875, 'train_lr': 0.00059660856}\n",
      "INFO:absl:[61] train_accuracy=0.10400390625, train_loss=6.6875, train_lr=0.0005966085591353476\n",
      "INFO:absl:{'train_accuracy': 0.109375, 'train_loss': 6.65625, 'train_lr': 0.00059647183}\n",
      "INFO:absl:[62] train_accuracy=0.109375, train_loss=6.65625, train_lr=0.0005964718293398619\n",
      "INFO:absl:{'train_accuracy': 0.107910156, 'train_loss': 6.625, 'train_lr': 0.0005963324}\n",
      "INFO:absl:[63] train_accuracy=0.10791015625, train_loss=6.625, train_lr=0.0005963324219919741\n",
      "INFO:absl:{'train_accuracy': 0.11816406, 'train_loss': 6.53125, 'train_lr': 0.0005961903}\n",
      "INFO:absl:[64] train_accuracy=0.1181640625, train_loss=6.53125, train_lr=0.0005961902788840234\n",
      "INFO:absl:{'train_accuracy': 0.104003906, 'train_loss': 6.625, 'train_lr': 0.0005960456}\n",
      "INFO:absl:[65] train_accuracy=0.10400390625, train_loss=6.625, train_lr=0.0005960455746389925\n",
      "INFO:absl:{'train_accuracy': 0.12890625, 'train_loss': 6.6875, 'train_lr': 0.00059589813}\n",
      "INFO:absl:[66] train_accuracy=0.12890625, train_loss=6.6875, train_lr=0.0005958981346338987\n",
      "INFO:absl:{'train_accuracy': 0.12109375, 'train_loss': 6.6875, 'train_lr': 0.000595748}\n",
      "INFO:absl:[67] train_accuracy=0.12109375, train_loss=6.6875, train_lr=0.0005957480170764029\n",
      "INFO:absl:{'train_accuracy': 0.122558594, 'train_loss': 6.8125, 'train_lr': 0.0005955952}\n",
      "INFO:absl:[68] train_accuracy=0.12255859375, train_loss=6.8125, train_lr=0.000595595221966505\n",
      "INFO:absl:{'train_accuracy': 0.11230469, 'train_loss': 6.78125, 'train_lr': 0.0005954397}\n",
      "INFO:absl:[69] train_accuracy=0.1123046875, train_loss=6.78125, train_lr=0.0005954396910965443\n",
      "INFO:absl:{'train_accuracy': 0.10839844, 'train_loss': 6.90625, 'train_lr': 0.00059528166}\n",
      "INFO:absl:[70] train_accuracy=0.1083984375, train_loss=6.90625, train_lr=0.0005952816572971642\n",
      "INFO:absl:[71] train_accuracy=0.11767578125, train_loss=6.65625, train_lr=0.0005951207713223994\n",
      "INFO:absl:{'train_accuracy': 0.11767578, 'train_loss': 6.65625, 'train_lr': 0.0005951208}\n",
      "INFO:absl:{'train_accuracy': 0.123535156, 'train_loss': 6.5625, 'train_lr': 0.0005949573}\n",
      "INFO:absl:[72] train_accuracy=0.12353515625, train_loss=6.5625, train_lr=0.0005949573242105544\n",
      "INFO:absl:{'train_accuracy': 0.111328125, 'train_loss': 7, 'train_lr': 0.00059479114}\n",
      "INFO:absl:[73] train_accuracy=0.111328125, train_loss=7, train_lr=0.0005947911413386464\n",
      "INFO:absl:{'train_accuracy': 0.13183594, 'train_loss': 6.625, 'train_lr': 0.0005946223}\n",
      "INFO:absl:[74] train_accuracy=0.1318359375, train_loss=6.625, train_lr=0.0005946222809143364\n",
      "INFO:absl:{'train_accuracy': 0.111816406, 'train_loss': 6.5625, 'train_lr': 0.0005944508}\n",
      "INFO:absl:[75] train_accuracy=0.11181640625, train_loss=6.5625, train_lr=0.0005944508011452854\n",
      "INFO:absl:[76] train_accuracy=0.11962890625, train_loss=6.75, train_lr=0.0005942767020314932\n",
      "INFO:absl:{'train_accuracy': 0.119628906, 'train_loss': 6.75, 'train_lr': 0.0005942767}\n",
      "INFO:absl:{'train_accuracy': 0.099609375, 'train_loss': 6.78125, 'train_lr': 0.00059409987}\n",
      "INFO:absl:[77] train_accuracy=0.099609375, train_loss=6.78125, train_lr=0.0005940998671576381\n",
      "INFO:absl:{'train_accuracy': 0.10839844, 'train_loss': 6.6875, 'train_lr': 0.0005939204}\n",
      "INFO:absl:[78] train_accuracy=0.1083984375, train_loss=6.6875, train_lr=0.0005939204129390419\n",
      "INFO:absl:{'train_accuracy': 0.10449219, 'train_loss': 6.46875, 'train_lr': 0.0005937383}\n",
      "INFO:absl:[79] train_accuracy=0.1044921875, train_loss=6.46875, train_lr=0.0005937382811680436\n",
      "INFO:absl:{'train_accuracy': 0.11035156, 'train_loss': 6.4375, 'train_lr': 0.00059355353}\n",
      "INFO:absl:[80] train_accuracy=0.1103515625, train_loss=6.4375, train_lr=0.0005935535300523043\n",
      "INFO:absl:{'train_accuracy': 0.11376953, 'train_loss': 6.46875, 'train_lr': 0.00059336604}\n",
      "INFO:absl:[81] train_accuracy=0.11376953125, train_loss=6.46875, train_lr=0.000593366043176502\n",
      "INFO:absl:{'train_accuracy': 0.110839844, 'train_loss': 6.5625, 'train_lr': 0.000593176}\n",
      "INFO:absl:[82] train_accuracy=0.11083984375, train_loss=6.5625, train_lr=0.0005931759951636195\n",
      "INFO:absl:{'train_accuracy': 0.118652344, 'train_loss': 6.625, 'train_lr': 0.0005929832}\n",
      "INFO:absl:[83] train_accuracy=0.11865234375, train_loss=6.625, train_lr=0.0005929832113906741\n",
      "INFO:absl:{'train_accuracy': 0.12109375, 'train_loss': 6.71875, 'train_lr': 0.00059278787}\n",
      "INFO:absl:[84] train_accuracy=0.12109375, train_loss=6.71875, train_lr=0.0005927878664806485\n",
      "INFO:absl:{'train_accuracy': 0.12597656, 'train_loss': 6.59375, 'train_lr': 0.00059258984}\n",
      "INFO:absl:[85] train_accuracy=0.1259765625, train_loss=6.59375, train_lr=0.0005925898440182209\n",
      "INFO:absl:{'train_accuracy': 0.13330078, 'train_loss': 6.53125, 'train_lr': 0.00059238914}\n",
      "INFO:absl:[86] train_accuracy=0.13330078125, train_loss=6.53125, train_lr=0.0005923891440033913\n",
      "INFO:absl:{'train_accuracy': 0.14697266, 'train_loss': 6.46875, 'train_lr': 0.0005921859}\n",
      "INFO:absl:[87] train_accuracy=0.14697265625, train_loss=6.46875, train_lr=0.0005921858828514814\n",
      "INFO:absl:{'train_accuracy': 0.13183594, 'train_loss': 6.375, 'train_lr': 0.0005919799}\n",
      "INFO:absl:[88] train_accuracy=0.1318359375, train_loss=6.375, train_lr=0.0005919798859395087\n",
      "INFO:absl:{'train_accuracy': 0.12792969, 'train_loss': 6.5625, 'train_lr': 0.00059177127}\n",
      "INFO:absl:[89] train_accuracy=0.1279296875, train_loss=6.5625, train_lr=0.0005917712696827948\n",
      "INFO:absl:{'train_accuracy': 0.107910156, 'train_loss': 6.5625, 'train_lr': 0.00059156003}\n",
      "INFO:absl:[90] train_accuracy=0.10791015625, train_loss=6.5625, train_lr=0.0005915600340813398\n",
      "INFO:absl:{'train_accuracy': 0.10888672, 'train_loss': 6.71875, 'train_lr': 0.0005913462}\n",
      "INFO:absl:[91] train_accuracy=0.10888671875, train_loss=6.71875, train_lr=0.0005913461791351438\n",
      "INFO:absl:[92] train_accuracy=0.11474609375, train_loss=6.5, train_lr=0.0005911297048442066\n",
      "INFO:absl:{'train_accuracy': 0.114746094, 'train_loss': 6.5, 'train_lr': 0.0005911297}\n",
      "INFO:absl:{'train_accuracy': 0.10253906, 'train_loss': 6.59375, 'train_lr': 0.00059091055}\n",
      "INFO:absl:[93] train_accuracy=0.1025390625, train_loss=6.59375, train_lr=0.0005909105530008674\n",
      "INFO:absl:{'train_accuracy': 0.111816406, 'train_loss': 6.59375, 'train_lr': 0.0005906888}\n",
      "INFO:absl:[94] train_accuracy=0.11181640625, train_loss=6.59375, train_lr=0.0005906887818127871\n",
      "INFO:absl:{'train_accuracy': 0.12060547, 'train_loss': 6.3125, 'train_lr': 0.0005904644}\n",
      "INFO:absl:[95] train_accuracy=0.12060546875, train_loss=6.3125, train_lr=0.0005904643912799656\n",
      "INFO:absl:{'train_accuracy': 0.12792969, 'train_loss': 6.21875, 'train_lr': 0.0005902374}\n",
      "INFO:absl:[96] train_accuracy=0.1279296875, train_loss=6.21875, train_lr=0.0005902373814024031\n",
      "INFO:absl:{'train_accuracy': 0.118652344, 'train_loss': 6.6875, 'train_lr': 0.0005900078}\n",
      "INFO:absl:[97] train_accuracy=0.11865234375, train_loss=6.6875, train_lr=0.0005900078103877604\n",
      "INFO:absl:{'train_accuracy': 0.1274414, 'train_loss': 6.46875, 'train_lr': 0.0005897755}\n",
      "INFO:absl:[98] train_accuracy=0.12744140625, train_loss=6.46875, train_lr=0.0005897755036130548\n",
      "INFO:absl:{'train_accuracy': 0.12109375, 'train_loss': 6.4375, 'train_lr': 0.0005895407}\n",
      "INFO:absl:[99] train_accuracy=0.12109375, train_loss=6.4375, train_lr=0.0005895406939089298\n",
      "INFO:absl:{'train_accuracy': 0.14013672, 'train_loss': 6.40625, 'train_lr': 0.0005893032}\n",
      "INFO:absl:[100] train_accuracy=0.14013671875, train_loss=6.40625, train_lr=0.0005893032066524029\n",
      "INFO:absl:{'train_accuracy': 0.13378906, 'train_loss': 6.375, 'train_lr': 0.0005890631}\n",
      "INFO:absl:[101] train_accuracy=0.1337890625, train_loss=6.375, train_lr=0.0005890631000511348\n"
     ]
    }
   ],
   "source": [
    "logdir = './metrics'\n",
    "from clu import metric_writers\n",
    "from absl import logging\n",
    "from flax.training import common_utils\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# I can get about 245ms or 66500 tok/sec out of the compiled jax code with bfloat16. Karpathy\n",
    "# is at about 93ms at this point, but I'm not sure where the slowdown is. Maybe pytorch is a\n",
    "# bit more specialized for this application, whereas jax is more general and numpy-like in \n",
    "# it's optimization.\n",
    "# Removing the Einsum layers helped a little bit. Down to 235ms.\n",
    "# It could also just be the hardware and infrastructure. Maybe the cpu/ram/pcie lanes are \n",
    "# a bit slower on this box\n",
    "# After pmap'ing the update function (or jit for non-distributed), we are at 116ms or 141000 tok/sec\n",
    "# This is getting close to Karpathy's numbers. Prefetching the data may be the next step\n",
    "\n",
    "# It is very important to jit the apply_gradients function as well or else you will take a \n",
    "# very large speed penalty\n",
    "#@jax.jit\n",
    "#def update_model(state, grads):\n",
    "#    return state.apply_gradients(grads=grads)\n",
    "\n",
    "it = train_loader.getIterator().as_numpy_iterator()\n",
    "#it = map(prepare_tf_data, it)\n",
    "it = jax_utils.prefetch_to_device(it, 2)\n",
    "\n",
    "logging.info(\"The first step will take a bit of time to compile\")\n",
    "p_train_step = jax.pmap(apply_model, axis_name='batch')\n",
    "\n",
    "writer = metric_writers.create_default_writer(logdir)\n",
    "\n",
    "train_metrics = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    #batch = train_loader.next_batch()\n",
    "    batch = next(it)\n",
    "    metrics, state = p_train_step(state, batch)\n",
    "\n",
    "   # state = update_model(state, grads)\n",
    "\n",
    "    # Making sure we are synchronized - Probably the only way to do this in jax. This may \n",
    "    # be slowing things down a bit though. Probably remove this when not timing.\n",
    "    jax.random.normal(jax.random.key(0), ()).block_until_ready()\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000\n",
    "\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "\n",
    "    metrics['lr'] = np.array([schedule(epoch)])\n",
    "    #metrics['dt'] = np.array(dt)\n",
    "    #metrics['tok_per_sec'] = np.array(tokens_per_sec)\n",
    "    train_metrics.append(metrics)\n",
    "\n",
    "    # This will unreplicate for us\n",
    "    train_metrics=common_utils.get_metrics(train_metrics)\n",
    "\n",
    "    # From the flax examples. Just average the train_stats if we only want to \n",
    "    # provide them at a certain number of steps\n",
    "    summary = {\n",
    "            f'train_{k}': v\n",
    "            for k, v in jax.tree_util.tree_map(\n",
    "                lambda x: np.mean(x), train_metrics\n",
    "            ).items()\n",
    "        }\n",
    "\n",
    "    writer.write_scalars(epoch + 1, summary)\n",
    "    logging.info(summary)\n",
    "    train_metrics = []\n",
    "\n",
    "    #print(f'step {epoch}, loss: {jax_utils.unreplicate(loss)}, accuracy: {jax_utils.unreplicate(accuracy):.2f}, lr: {schedule(epoch):.5f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': array([0.], dtype=float32),\n",
       " 'loss': array([10.6875], dtype=bfloat16),\n",
       " 'lr': array([0.000114], dtype=float32)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flax.training import common_utils\n",
    "\n",
    "common_utils.get_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': Array([0.], dtype=float32),\n",
       "  'loss': Array([10.6875], dtype=bfloat16),\n",
       "  'lr': array([0.000114], dtype=float32)}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
