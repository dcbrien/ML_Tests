{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FlaxGPT2LMHeadModel' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlaxGPT2LMHeadModel\n\u001b[1;32m      3\u001b[0m model_hf \u001b[38;5;241m=\u001b[39m FlaxGPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m sd_hf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FlaxGPT2LMHeadModel' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.shape(model_hf.params['transformer']['wpe']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c_attn', 'c_proj'])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf.params['transformer']['h']['0']['attn'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.broadcast_to(jnp.arange(10, dtype=\"i4\")[None, :], (32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 65\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        self.c_proj = nn.Dense(self.config.n_embd)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm()\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        #self.attn = nn.MultiHeadAttention(num_heads=self.config.n_head, qkv_features=self.config.n_head)\n",
    "        self.ln_2 = nn.LayerNorm()\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        #x = self.attn(x)\n",
    "        x = self.ln_2(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return inputs\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.wte(x)\n",
    "        \n",
    "        #x += self.wpe(x)\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, 1000, (4,4), dtype='int32')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'transformer': {'h': {'0': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '1': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '2': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '3': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '4': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '5': {'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}}, 'ln_f': {'bias': *, 'scale': *}, 'wte': {'embedding': *}}})\n"
     ]
    }
   ],
   "source": [
    "from jax.tree_util import tree_structure\n",
    "print(tree_structure(y['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'c_fc': {'bias': (1536,), 'kernel': (384, 1536)}, 'c_proj': {'bias': (384,), 'kernel': (1536, 384)}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer']['h']['0']['mlp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer']['h']['0']['mlp'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like pytorch Linear is not equal to flax Dense. The matrices are defined transpose. So, we can't actually use\n",
    "# Dense with the original GPT2 weights\n",
    "\n",
    "x = jax.random.uniform(jax.random.key(0), (32, 64, 384))\n",
    "\n",
    "# This will transpose the kernel and multiply by the input,\n",
    "# This should be the same as the GPT weights\n",
    "lp = nn.Einsum((3*384, 384), '...ij,...kj->ik')\n",
    "\n",
    "y = lp.init(jax.random.key(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': Array([[ 0.00585502,  0.06663669, -0.02862518, ...,  0.03995296,\n",
       "         -0.02848659,  0.03771213],\n",
       "        [-0.01506117, -0.04920985, -0.02150657, ..., -0.00969302,\n",
       "         -0.00604414, -0.06075063],\n",
       "        [-0.01117269, -0.05698648, -0.02741355, ...,  0.01138836,\n",
       "          0.02523999, -0.03665182],\n",
       "        ...,\n",
       "        [ 0.00031354,  0.03191991,  0.01749613, ..., -0.03667144,\n",
       "          0.02852946,  0.02422978],\n",
       "        [ 0.01442624,  0.02207879, -0.02500978, ..., -0.00365288,\n",
       "          0.00623836,  0.01097682],\n",
       "        [-0.03645209, -0.06231447, -0.00414094, ...,  0.03171502,\n",
       "          0.06253771, -0.02757666]], dtype=float32),\n",
       " 'bias': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'bias': (1152,), 'kernel': (1152, 384)}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  38,  62],\n",
       "       [ 38, 126, 214],\n",
       "       [ 62, 214, 366]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[ 0,  1,  2, 3],\n",
    "              [ 4,  5,  6, 7],\n",
    "              [ 8,  9, 10, 11]])\n",
    "\n",
    "B = np.array([[ 0,  1,  2,  3],\n",
    "              [ 4,  5,  6,  7],\n",
    "              [ 8,  9, 10, 11]])\n",
    "\n",
    "np.einsum('ij,kj->ik', A, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
