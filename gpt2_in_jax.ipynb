{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.shape(model_hf.params['transformer']['wpe']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c_attn', 'c_proj'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf.params['transformer']['h']['0']['attn'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        #self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        # Had to use Einsum to match the matrix multiplication of GPT2 and pytorch. They accept \n",
    "        # both shapes and then multiply by the transpose of the matrix (basically means the \n",
    "        # shape of the matrix is transpose, but the operation is the same)\n",
    "        self.c_fc = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "        #self.c_proj = nn.Dense(self.config.n_embd)\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, 4 * self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTAttention(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    # we will need to roll our own attention module because the built in one has a bunch of different\n",
    "    # naming and structure compared to the original GPT, which just references the projection layers\n",
    "    def setup(self):\n",
    "        # The first thing we do is project up to 3x the model size, because we are going to split\n",
    "        # the data into q, v, k\n",
    "        self.c_attn = nn.Einsum((3 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "        # At the end we have to project everything back to the regular model size of n_embd\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        B, T, C = jnp.shape(x)\n",
    "        \n",
    "        # Project to qkv\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
    "\n",
    "        query_length, key_length = q.shape[1], k.shape[1]\n",
    "\n",
    "        # Now reshape with a new head \"batch\" dimension\n",
    "        k = jnp.reshape(k, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        q = jnp.reshape(q, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        v = jnp.reshape(v, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "\n",
    "        # Make the attention mask\n",
    "        # TODO: For round 1 I'm just trusting the linen functions. They seem to be doing the correct thing here, but I may have to \n",
    "        # return to this for a closer look at the math if I'm not getting the GPT2 results\n",
    "        # Just copied this from the huggingface code to be consistent. First part just broadcasts the causal masks to the batch\n",
    "        # dimensions (replicating it as a lower triangular matrix of truths). The attention_bias is a stripped down version of the \n",
    "        # huggingface code, but the bias has to be floats. They bias the attention softmax, so basically set to -inf where you\n",
    "        # want it ignored. This will need to be OR'd with an attention mask in certain situations, like encoder/decoder networks.\n",
    "        c_mask = nn.make_causal_mask(jnp.ones((1, self.config.block_size), dtype=\"bool\"), dtype=\"bool\")\n",
    "        c_mask = c_mask[:, :, :query_length, :key_length]\n",
    "        c_mask = jnp.broadcast_to(c_mask, (B,) + c_mask.shape[1:])\n",
    "\n",
    "        attention_bias = jax.lax.select(\n",
    "                c_mask > 0,\n",
    "                jnp.full(c_mask.shape, 0.0).astype(jnp.float32),\n",
    "                jnp.full(c_mask.shape, jnp.finfo(jnp.float32).min).astype(jnp.float32),\n",
    "            )\n",
    "\n",
    "        # use the built in flax libraries to calculate the attention matrix - attention weights are not returned, but could be \n",
    "        # I think bias gives more control compared to mask - i.e. bias can be a float. They might result in the same output with a \n",
    "        # boolean mask, but I will have to test that.\n",
    "        y = nn.dot_product_attention(q, k, v, bias=attention_bias)\n",
    "\n",
    "        # Merge the heads back together\n",
    "        y = y.reshape((B, T, C))\n",
    "\n",
    "        # Project output with a FC layer\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm(epsilon=1e-05)\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        self.attn = GPTAttention(self.config)\n",
    "        self.ln_2 = nn.LayerNorm(epsilon=1e-05)\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        x = inputs + self.attn(x)\n",
    "        inputs2 = x\n",
    "        x = self.ln_2(x)\n",
    "        x = inputs2 + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm(epsilon=1e-05)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        input_shape = jnp.shape(x)       \n",
    "\n",
    "        x = self.wte(x)\n",
    "\n",
    "        # For the positional encodings we need an index that is simple the position\n",
    "        # of each token. This will be the same shape as the input, but will simply\n",
    "        # be repeating and increasing numbers from 1.\n",
    "        # jnp.atleast_2d is needed so we can initialize with a batch size of 1\n",
    "        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(inputs).shape[-1]), input_shape)\n",
    "        x_wpe = self.wpe(position_ids)\n",
    "        \n",
    "        x += x_wpe\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "        # So the Flax model does not return parameters for lm_head, because lm_head is simply the \n",
    "        # inverse operation of the initial word embedding, so we can just reuse those weights. They\n",
    "        # use the term 'tied' for this. The weights are tied together.\n",
    "        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # This is from the huggingface code - might as well reuse it here.\n",
    "        shared_kernel = self.transformer.variables['params']['wte']['embedding'].T\n",
    "        lm_logits = self.lm_head.apply({'params': {'kernel': shared_kernel}}, x)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, 50257, (1,1024), dtype='i4')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = GPT2.apply({'params': model_hf.params}, np.random.randint(0, 50257, (2,1024), dtype='i4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1024, 50257)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 20\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"A pulsar is\")\n",
    "\n",
    "tokens = jnp.array(tokens, dtype=\"i4\")\n",
    "\n",
    "x = jnp.atleast_2d(tokens).repeat(num_return_sequences, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[   32, 22271,   283,   318,   257,  2099,   286, 18758],\n",
       "       [   32, 22271,   283,   318,   257, 22271,   283,   326]],      dtype=int32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "key = jax.random.key(2)\n",
    "# This needs to use the jax loop functions to be fast\n",
    "# This will be crazy slow without @jit compilation\n",
    "while x.shape[1] < max_length:\n",
    "\n",
    "    #logits = GPT2.apply({'params': model_hf.params}, x)\n",
    "    logits = GPT2.apply({'params': model_hf.params}, x)\n",
    "\n",
    "    logits = logits[:, -1, :]\n",
    "\n",
    "    #probs = nn.softmax(logits)\n",
    "\n",
    "    topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "\n",
    "    #dist = tfd.Multinomial(1, logits=topk_probs)\n",
    "\n",
    "    #key, samp_key = jax.random.split(key)\n",
    "    #ix=dist.sample(seed=samp_key)\n",
    "\n",
    "    ix = jnp.expand_dims(jax.random.categorical(key, topk_probs, axis=-1), axis=-1)\n",
    "\n",
    "    #print(ix)\n",
    "\n",
    "    xcol=jnp.atleast_2d(jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1))\n",
    "\n",
    "    #print(xcol.shape)\n",
    "\n",
    "    x = jnp.append(x, xcol, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (2, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_fn at /tmp/ipykernel_2479/446094793.py:17 for while_loop. This concrete value was not available in Python because it depends on the value of the argument state[2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m initial_input \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray(x, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32)  \u001b[38;5;66;03m# Replace with your initial token(s)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Replace with desired sequence length\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_sequence)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[45], line 48\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model_hf, max_length, key, initial_input)\u001b[0m\n\u001b[1;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m (x_padded, key, initial_length)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run the while loop\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m x, key, _ \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Return the sequence up to max_length\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[:, :max_length]\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[45], line 21\u001b[0m, in \u001b[0;36mgenerate_text.<locals>.body_fn\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     18\u001b[0m x, key, i \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Slice x to the current length using dynamic slicing\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x_slice \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m GPT2\u001b[38;5;241m.\u001b[39mapply({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params}, x_slice)\n\u001b[1;32m     24\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-env/lib/python3.10/site-packages/jax/_src/core.py:1647\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1647\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (2, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function body_fn at /tmp/ipykernel_2479/446094793.py:17 for while_loop. This concrete value was not available in Python because it depends on the value of the argument state[2]."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "pad_width = max_length - x.shape[-1]\n",
    "\n",
    "def generate_text(model_hf, max_length, key, initial_input):\n",
    "    batch_size = initial_input.shape[0]\n",
    "    initial_length = initial_input.shape[1]\n",
    "    \n",
    "    # Pad the initial input to max_length outside of JAX's traced functions\n",
    "    #pad_width = max_length - initial_length\n",
    "    x_padded = jnp.pad(initial_input, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    def body_fn(state):\n",
    "        x, key, i = state\n",
    "        \n",
    "        # Slice x to the current length using dynamic slicing\n",
    "        x_slice = jax.lax.dynamic_slice(x, (0, 0), (batch_size, i + 1))\n",
    "        \n",
    "        logits = GPT2.apply({'params': params}, x_slice)\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "        \n",
    "        dist = tfd.Categorical(logits=topk_probs)\n",
    "        \n",
    "        key, samp_key = jax.random.split(key)\n",
    "        ix = dist.sample(seed=samp_key)\n",
    "        \n",
    "        xcol = topk_indices[jnp.arange(ix.shape[0]), ix][:, None]\n",
    "        \n",
    "        # Update x with new token at position i using dynamic update\n",
    "        x = jax.lax.dynamic_update_slice(x, xcol, (0, i))\n",
    "        \n",
    "        return x, key, i + 1\n",
    "    \n",
    "    def cond_fn(state):\n",
    "        x, key, i = state\n",
    "        return i < max_length\n",
    "\n",
    "    # Initialize state with padded input\n",
    "    state = (x_padded, key, initial_length)\n",
    "    \n",
    "    # Run the while loop\n",
    "    x, key, _ = jax.lax.while_loop(cond_fn, body_fn, state)\n",
    "    \n",
    "    # Return the sequence up to max_length\n",
    "    return x[:, :max_length]\n",
    "\n",
    "# Example usage:\n",
    "key = jax.random.PRNGKey(57)\n",
    "initial_input = jnp.array(x, dtype=jnp.int32)  # Replace with your initial token(s)\n",
    "max_length = 50  # Replace with desired sequence length\n",
    "\n",
    "generated_sequence = jax.jit(generate_text)(model_hf.params, max_length, key, initial_input)\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "num_return_sequences = 20\n",
    "max_length = 50\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"The quick brown fox,\")\n",
    "\n",
    "tokens = jnp.array(tokens, dtype=\"i4\")\n",
    "\n",
    "initial_input = jnp.atleast_2d(tokens).repeat(num_return_sequences, 0)\n",
    "\n",
    "init_length=jnp.shape(initial_input)[-1]\n",
    "\n",
    "initial_input=jnp.pad(initial_input, ((0, 0), (0, max_length-init_length)), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[   32, 22271,   283,   318,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [   32, 22271,   283,   318,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [   32, 22271,   283,   318,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [   32, 22271,   283,   318,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [   32, 22271,   283,   318,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(2)\n",
    "\n",
    "def get_sentence(n, carry):\n",
    "    x, rng = carry\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    # pull out only the sequence up until the currently filled in\n",
    "    # values\n",
    "    logits = GPT2.apply({'params': model_hf.params}, x)\n",
    "\n",
    "    # GPT2 predicted the new token int he last column of the output, \n",
    "    # so we only need this.\n",
    "    logits = logits[:, n, :]\n",
    "\n",
    "    # Sample from the top 50 tokens\n",
    "    topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "\n",
    "    #dist = tfd.Multinomial(1, logits=topk_probs)\n",
    "\n",
    "    #rng, samp_key = jax.random.split(rng)\n",
    "    #ix=dist.sample(seed=samp_key)\n",
    "\n",
    "    #xcol=jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1)\n",
    "\n",
    "    ix = jnp.expand_dims(jax.random.categorical(rng, topk_probs/0.7, axis=-1), axis=-1)\n",
    "\n",
    "    #print(ix)\n",
    "\n",
    "    xcol=jnp.atleast_2d(jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1))\n",
    "\n",
    "    #print(x.shape)\n",
    "\n",
    "    #x = x.at[:, n+1].set(xcol)\n",
    "\n",
    "    x = jax.lax.dynamic_update_slice(x, xcol, (0, n+1))\n",
    "\n",
    "    #x = jnp.append(x, xcol, -1)\n",
    "\n",
    "    return x, rng\n",
    "\n",
    "x, rng = jax.lax.fori_loop(init_length-1, max_length-1, get_sentence, (initial_input, rng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (2, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function scanned_fun at /home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:2007 for scan. This concrete value was not available in Python because it depends on the value of the argument loop_carry[1][2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mkey(\u001b[38;5;241m57\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[95], line 35\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(initial_input, rng)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m#x = jnp.append(x, xcol, -1)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, rng, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 35\u001b[0m x, rng, i \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfori_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m, in \u001b[0;36mgenerate_text.<locals>.get_sentence\u001b[0;34m(n, carry)\u001b[0m\n\u001b[1;32m      7\u001b[0m rng, new_rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(rng)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# pull out only the sequence up until the currently filled in\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# values\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamic_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m logits \u001b[38;5;241m=\u001b[39m GPT2\u001b[38;5;241m.\u001b[39mapply({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: model_hf\u001b[38;5;241m.\u001b[39mparams}, x2)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# GPT2 predicted the new token int he last column of the output, \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# so we only need this.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax-env/lib/python3.10/site-packages/jax/_src/core.py:1647\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1647\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (2, Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/0)>).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function scanned_fun at /home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py:2007 for scan. This concrete value was not available in Python because it depends on the value of the argument loop_carry[1][2]."
     ]
    }
   ],
   "source": [
    "rng = jax.random.key(57)\n",
    "\n",
    "generated_sequence = jax.jit(generate_text)(initial_input=x, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox, however, was not what I was worried about. It was a big bear, but if he'd grown to like it he would have been just as eager to hunt.\n",
      "\n",
      "I could not quite say if there was danger\n",
      "The quick brown fox, by the way, seems to have some sort of a strong instinct for violence. He is, in fact, one of the most cunning-looking foxes amongst the group's members, apparently using both paws to try and keep\n",
      "The quick brown fox, at last seen in the sky behind my back\n",
      "\n",
      "Took my hand for its big nose, but found me the long fox\n",
      "\n",
      "The great bald eagle, at last seen on the horizon\n",
      "\n",
      "He ran with me\n",
      "The quick brown fox, for instance, needs to stay up on a day's sleep, which can mean making sure his coat doesn't become a habit, or maybe picking a litter of flies to feed to.\n",
      "\n",
      "In many cases, these two\n",
      "The quick brown fox, who had a big grin on his face, was smiling. \"So who's in town? Are you looking for your father.\" A small fox with very bright yellow eyes and little blue teeth, who had an arm like a\n",
      "The quick brown fox, with its long yellow head, wore a green hat, of which it was adorned with the usual white.\n",
      "\n",
      "This fox was a blacksmith of one of the many guilds at the city of Veren. That guild\n",
      "The quick brown fox, whose only nickname is \"Ricky\") was a rare dog of great popularity for her own household pets. Her owners were not fond of her and instead put her with a pair of black tuxedo coats. As a child\n",
      "The quick brown fox, which is normally known for chewing on your clothes, is now seen lurking and eating its way through the trees.\n",
      "\n",
      "Image caption The fox is a rare case of fox-fighting\n",
      "\n",
      "The most bizarre part is when the\n",
      "The quick brown fox, or \"caught\" or \"shark\" was introduced, they didn't appear many decades later. The new foxes, they've been replaced by two more giant foxes before, known as the \"Mantok\n",
      "The quick brown fox, which is about two inches long, has been nicknamed 'Tatanka' and 'Zeus,' and is a member of an endangered species of fox in India.\n",
      "\n",
      "While at least three other foxes were reported killed\n",
      "The quick brown fox, now as in his early 30s, is a true monster, the kind that has always been on the hunt. With more than 100 prey species, the fox has been studied at every level of the wild and has a wealth\n",
      "The quick brown fox, who is quite timid and timid for the moment, takes a deep breath, and slowly slowly opens her jaw.\n",
      "\n",
      "\"Hang it!\" She smiles, and then slowly and carefully, so that she can get up with\n",
      "The quick brown fox, who had walked out of the apartment and was heading for his place, was suddenly dragged with ropes and tied up. He looked worried and asked those who was responsible.\n",
      "\n",
      "\"My name is Alex T. and I've\n",
      "The quick brown fox, it was now well past dusk (although it was a late fall morning) and she was standing in the small courtyard of a church with a coterie of strangers and a boy in the group.\n",
      "\n",
      "She was not\n",
      "The quick brown fox, which had been left alone from the beginning, rushed into the forest.\n",
      "\n",
      "\"Come. We've got some good news.\" Yang stopped him for a moment for a moment. \"I did indeed kill him, but I\n",
      "The quick brown fox, which has a nice tan, will eat anything it can see and is very smart though.\"\n",
      "\n",
      "In the following days, Dr. O'Mara will visit, and will occasionally take a break to study the matter and\n",
      "The quick brown fox, then she started to walk up to the fence. After a while, she started to walk toward the fence, and then stopped, the fox turned around to glare at her, but didn't take notice. Then she stopped and\n",
      "The quick brown fox, or feline fox, has been used for many purposes in Britain. Many people who are concerned about their pets or have problems with them have sought out the fur of a fox, but the furry fox is not allowed to roam\n",
      "The quick brown fox, the long red fox, the wolverine and the tiger, are now out by itself and are left alone for just six days.\n",
      "\n",
      "This is an unusual arrangement for a fox. Normally a white fox would leave its\n",
      "The quick brown fox, as is its name, is highly intelligent, with a thick, golden, smooth hair that can walk almost two feet at any given time (although it can fall quite fast). Its natural instincts favor movement of its body, and\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, x.shape[0]):\n",
    "    print(enc.decode(x[i,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = GPT2.apply({'params': model_hf.params}, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 50257)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 8, 50257)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'attn': {'c_attn': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}})\n"
     ]
    }
   ],
   "source": [
    "from jax.tree_util import tree_structure\n",
    "print(tree_structure(y['params']['transformer']['h']['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '1': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '2': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '3': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '4': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '5': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}}, 'ln_f': {'bias': (384,), 'scale': (384,)}, 'wpe': {'embedding': (256, 384)}, 'wte': {'embedding': (65, 384)}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like pytorch Linear is not equal to flax Dense. The matrices are defined transpose. So, we can't actually use\n",
    "# Dense with the original GPT2 weights\n",
    "\n",
    "x = jax.random.uniform(jax.random.key(0), (1, 2, 5))\n",
    "\n",
    "# This will transpose the kernel and multiply by the input,\n",
    "# This should be the same as the GPT weights\n",
    "lp = nn.Einsum((3*5, 5), '...ij,...kj->...ik')\n",
    "\n",
    "y = lp.init(jax.random.key(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.40416887, -0.15771458, -0.1418275 ,  0.21624644,\n",
       "          0.01153318,  0.09767484, -0.16552436, -0.06833879,\n",
       "         -0.07042018, -0.13803177, -0.00222424,  0.2503722 ,\n",
       "         -0.3504156 ,  0.18865553, -0.33016038],\n",
       "        [ 0.62610245, -0.35361993, -0.40769407,  0.22174977,\n",
       "          0.1036396 ,  0.2302788 , -0.25040534, -0.5411675 ,\n",
       "         -0.35929912,  0.05858286,  0.03919724,  0.3880062 ,\n",
       "         -0.13242133,  0.4200734 , -0.47516495]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.apply(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp2 = nn.Dense(3*5)\n",
    "\n",
    "y2 = lp2.init(jax.random.key(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '1': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '2': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '3': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '4': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}, '5': {'attn': {'c_attn': {'bias': (1152,), 'kernel': (1152, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 384)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}}, 'ln_f': {'bias': (384,), 'scale': (384,)}, 'wpe': {'embedding': (256, 384)}, 'wte': {'embedding': (65, 384)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,),\n",
       "      'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}},\n",
       "  'ln_f': {'bias': (768,), 'scale': (768,)},\n",
       "  'wpe': {'embedding': (1024, 768)},\n",
       "  'wte': {'embedding': (50257, 768)}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree.map(lambda x: x.shape, y['params'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,),\n",
       "      'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}},\n",
       "   '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 768)}},\n",
       "    'ln_1': {'bias': (768,), 'scale': (768,)},\n",
       "    'ln_2': {'bias': (768,), 'scale': (768,)},\n",
       "    'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)},\n",
       "     'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}},\n",
       "  'ln_f': {'bias': (768,), 'scale': (768,)},\n",
       "  'wpe': {'embedding': (1024, 768)},\n",
       "  'wte': {'embedding': (50257, 768)}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree.map(lambda x: x.shape, model_hf.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
