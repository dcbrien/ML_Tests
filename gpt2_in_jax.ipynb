{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-18 15:52:09.020073: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-18 15:52:09.020133: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-18 15:52:09.020959: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-18 15:52:09.757771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained weights from huggingface - Do not run this if you are \n",
    "# planning to train your own model. It will just take up memory\n",
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "# Here is the dictionary tree we have to replicate. pytrees are just nested dictionaries.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from flax import linen as nn\n",
    "from typing import Any\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "# TODO:\n",
    "# - Add options for different dtypes of the model\n",
    "# - Add options for difference precisions (and understand that better - I think it has to do with the mantissa of the floats\n",
    "# - Add istraining variable to allow dropout and other operations that are different during training and inference\n",
    "# - Maybe: Flash attention?\n",
    "# - Maybe: Sharding for multiple gpu training?\n",
    "# - Use the Jax configuration utilities instead of the GPTConfig class\n",
    "# - Develop my own training routing with all of the bells and whistles required: timing, checkpointinng, etc...\n",
    "# - Move everything to python files for more professional-like code from the command line\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "    dtype: Any = jnp.bfloat16\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        #self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        # Had to use Einsum to match the matrix multiplication of GPT2 and pytorch. They accept \n",
    "        # both shapes and then multiply by the transpose of the matrix (basically means the \n",
    "        # shape of the matrix is transpose, but the operation is the same). I confirmed that this\n",
    "        # produces the same result as the complicated huggingface conv1d version (conv1d is also\n",
    "        # just a linear matrix operation as well). They do add a lot of variables for \n",
    "        # mixed-precision training, that I do not.\n",
    "        # TODO: Move this into a new module as I am repeating it everywhere\n",
    "        self.c_fc = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        #self.c_proj = nn.Dense(self.config.n_embd)\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, 4 * self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTAttention(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    # we will need to roll our own attention module because the built in one has a bunch of different\n",
    "    # naming and structure compared to the original GPT, which just references the projection layers\n",
    "    def setup(self):\n",
    "        # The first thing we do is project up to 3x the model size, because we are going to split\n",
    "        # the data into q, v, k\n",
    "        self.c_attn = nn.Einsum((3 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "\n",
    "        # At the end we have to project everything back to the regular model size of n_embd\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik', kernel_init=jax.nn.initializers.normal(stddev=(self.config.n_layer*2) ** -0.5), dtype=self.config.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        B, T, C = jnp.shape(x)\n",
    "        \n",
    "        # Project to qkv\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q, k, v = jnp.split(qkv, 3, axis=2)\n",
    "\n",
    "        query_length, key_length = q.shape[1], k.shape[1]\n",
    "\n",
    "        # Now reshape with a new head \"batch\" dimension\n",
    "        k = jnp.reshape(k, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        q = jnp.reshape(q, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "        v = jnp.reshape(v, (B, T, self.config.n_head, C // self.config.n_head)) # Shape is (batch, tokens, num_heads, size of head)\n",
    "\n",
    "        # Make the attention mask\n",
    "        # TODO: For round 1 I'm just trusting the linen functions. They seem to be doing the correct thing here, but I may have to \n",
    "        # return to this for a closer look at the math if I'm not getting the GPT2 results\n",
    "        # Just copied this from the huggingface code to be consistent. First part just broadcasts the causal masks to the batch\n",
    "        # dimensions (replicating it as a lower triangular matrix of truths). The attention_bias is a stripped down version of the \n",
    "        # huggingface code, but the bias has to be floats. They bias the attention softmax, so basically set to -inf where you\n",
    "        # want it ignored. This will need to be OR'd with an attention mask in certain situations, like encoder/decoder networks.\n",
    "        # TODO: Also, this mask does not need to be applied during inference, so we could have an 'istraining' variable passed \n",
    "        # down the network for these cases and then ignore the mask calculations for increased speed. In the Flax examples, they \n",
    "        # also appear to cache the results of previous calculations, which I guess makes sense because we are just adding one \n",
    "        # token at a time to the input sequence and then calculating again. There is no point in recalculating the previous \n",
    "        # token outputs every time. I should probbaly implement some version of that too.\n",
    "        c_mask = nn.make_causal_mask(jnp.ones((1, self.config.block_size), dtype=\"bool\"), dtype=\"bool\")\n",
    "        c_mask = c_mask[:, :, :query_length, :key_length]\n",
    "        c_mask = jnp.broadcast_to(c_mask, (B,) + c_mask.shape[1:])\n",
    "\n",
    "        attention_bias = jax.lax.select(\n",
    "                c_mask > 0,\n",
    "                jnp.full(c_mask.shape, 0.0).astype(jnp.float32),\n",
    "                jnp.full(c_mask.shape, jnp.finfo(jnp.float32).min).astype(jnp.float32),\n",
    "            )\n",
    "\n",
    "        # use the built in flax libraries to calculate the attention matrix - attention weights are not returned, but could be \n",
    "        # I think bias gives more control compared to mask - i.e. bias can be a float. They might result in the same output with a \n",
    "        # boolean mask, but I will have to test that.\n",
    "        # TODO: I don't think Flax has a flash attention module. Is there any way to add that for Flax that will actually be \n",
    "        # optimized for hardware? I don't know.\n",
    "        y = nn.dot_product_attention(q, k, v, bias=attention_bias, dtype=self.config.dtype)\n",
    "\n",
    "        # Merge the heads back together\n",
    "        y = y.reshape((B, T, C))\n",
    "\n",
    "        # Project output with a FC layer\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        self.attn = GPTAttention(self.config)\n",
    "        self.ln_2 = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        x = inputs + self.attn(x)\n",
    "        inputs2 = x\n",
    "        x = self.ln_2(x)\n",
    "        x = inputs2 + self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd, embedding_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd, embedding_init=jax.nn.initializers.normal(stddev=0.02), dtype=self.config.dtype)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm(epsilon=1e-05, dtype=jnp.float32)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        input_shape = jnp.shape(x)       \n",
    "\n",
    "        x = self.wte(x)\n",
    "\n",
    "        # For the positional encodings we need an index that is simple the position\n",
    "        # of each token. This will be the same shape as the input, but will simply\n",
    "        # be repeating and increasing numbers from 1.\n",
    "        # jnp.atleast_2d is needed so we can initialize with a batch size of 1\n",
    "        position_ids = jnp.broadcast_to(jnp.arange(jnp.atleast_2d(inputs).shape[-1]), input_shape)\n",
    "        x_wpe = self.wpe(position_ids)\n",
    "        \n",
    "        x += x_wpe\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "        # So the Flax model does not return parameters for lm_head, because lm_head is simply the \n",
    "        # inverse operation of the initial word embedding, so we can just reuse those weights. They\n",
    "        # use the term 'tied' for this. The weights are tied together.\n",
    "        self.lm_head = nn.Dense(self.config.vocab_size, use_bias=False, dtype=self.config.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # This is from the huggingface code - might as well reuse it here.\n",
    "        shared_kernel = self.transformer.variables['params']['wte']['embedding'].T\n",
    "        lm_logits = self.lm_head.apply({'params': {'kernel': shared_kernel}}, x)\n",
    "        return lm_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, GPTConfig.vocab_size, (1, GPTConfig.block_size), dtype='i4')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "import tiktoken\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "#from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "#tfd = tfp.distributions\n",
    "\n",
    "# Generate a starting sequence and we will generate the rest of the sequence with GPT2 for\n",
    "# however many batches we need\n",
    "num_return_sequences = 1\n",
    "max_length = 50\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"\")\n",
    "\n",
    "tokens = jnp.array(tokens, dtype=\"i4\")\n",
    "\n",
    "initial_input = jnp.atleast_2d(tokens).repeat(num_return_sequences, 0)\n",
    "\n",
    "init_length=jnp.shape(initial_input)[-1]\n",
    "\n",
    "# Has to be a constant length for the fast-autogression, so we need to pad (we will ignore the pads until \n",
    "# those indicies are actually needed by the model as it grows)\n",
    "initial_input=jnp.pad(initial_input, ((0, 0), (0, max_length-init_length)), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax-style fast autoregression using the jax looping tools. This takes a bit of time to get correct\n",
    "# with each model as you have to design it using the XLA tools and every single parameter needs to \n",
    "# be deterministic. This leads to lots of gotchas. For example, dynamic_slice and dynamic_slice_update\n",
    "# need to have a constant for the size, but can be placed anywhere in the matrix (i.e., you can't use\n",
    "# a variable with this). And, take_along_axis needs to be used for the top_k type indexing as it uses \n",
    "# the XLA gather method underneath, which is super complicated to use by hand. Every model produces \n",
    "# new difficulties and I'm sure this can be optimized quite a bit if I understood jax a bit better.\n",
    "rng = jax.random.key(25)\n",
    "\n",
    "temperature = 1.0\n",
    "\n",
    "def get_sentence(n, carry):\n",
    "    x, rng = carry\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    # pull out only the sequence up until the currently filled in\n",
    "    # values. It would be better if we could pass in a variable \n",
    "    # length sequence, but I don't think that's allowed by jax\n",
    "    logits = GPT2.apply({'params': state.params}, x)\n",
    "\n",
    "    # GPT2 predicted the new token int he last column of the output, \n",
    "    # so we only need this.\n",
    "    logits = logits[:, n, :]\n",
    "\n",
    "    # Sample from the top 50 tokens\n",
    "    topk_probs, topk_indices = jax.lax.top_k(logits, 50)\n",
    "\n",
    "    ix = jnp.expand_dims(jax.random.categorical(new_rng, topk_probs/temperature, axis=-1), axis=-1)\n",
    "\n",
    "    #print(ix)\n",
    "\n",
    "    xcol=jnp.atleast_2d(jnp.take_along_axis(topk_indices, ix.astype(jnp.int32), axis=-1))\n",
    "\n",
    "    x = jax.lax.dynamic_update_slice(x, xcol, (0, n+1))\n",
    "\n",
    "    return x, rng\n",
    "\n",
    "x, rng = jax.lax.fori_loop(init_length-1, max_length-1, get_sentence, (initial_input, rng))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The time it will stay.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LUCYTER:\n",
      "He had, I will you have?\n",
      "'TABETH:\n",
      "And, it; I pritinks you.\n",
      "\n",
      "\n",
      "\n",
      "What are thee\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "for i in range(0, x.shape[0]):\n",
    "    print(enc.decode(x[i,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n",
      "initialized parameter shapes:\n",
      " {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}\n"
     ]
    }
   ],
   "source": [
    "# Just comparing the shapes of our model vs the GPT2 trained weights.\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer'])))\n",
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's train our own GPT on the tiny Shakespear dataset:\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open(\"input.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "data = text[:1000]\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B, T = 4, 32\n",
    "# buf = jnp.array(tokens[:(B*T)+1])\n",
    "# x = buf[:-1].reshape((B,T)) # input\n",
    "# y = buf[1:].reshape((B,T)) # targets (one ahead because GPT2 is an autogressive model \n",
    "#                            # - i.e. it's always predicting the next token in parallel)\n",
    "\n",
    "# Initialize\n",
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(2), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x_init = np.random.randint(0, GPTConfig.vocab_size, (1, GPTConfig.block_size), dtype='i4')\n",
    "\n",
    "y_init = GPT2.init(key2, x_init)\n",
    "\n",
    "#logits = GPT2.apply({'params': y_init['params']}, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32, 50257)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.311539\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate one loss\n",
    "import optax\n",
    "\n",
    "# This is a little different than Karpathy because jax can accept the batch dimension. We then mean the losses\n",
    "# of the batches to get the full batch loss. I would assume that the pytorch cross_entropy is doing the same\n",
    "# thing internally\n",
    "loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y))\n",
    "# The loss is close to the expected value of 10.8\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In jax we create a separate apply function to calculate the loss\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, x, labels):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(\n",
    "            {'params': params}, \n",
    "            x)\n",
    "        #one_hot = jax.nn.one_hot(labels, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels))\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (logits)), grads = grad_fn(state.params)\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return grads, loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's simulate a small training algorithm\n",
    "from typing import Any\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Start with AdamW\n",
    "tx = optax.adamw(learning_rate=learning_rate)\n",
    "\n",
    "# Flax handles the training state for us. Generally, you should subclass the TrainState and \n",
    "# add anything you need to it. Here, we don't need any additions, but we may in the future\n",
    "# so I will leave it in for now. Things you might add are the batch_stats or dropout keys\n",
    "class TrainState(train_state.TrainState):\n",
    "  key: jax.Array\n",
    "  batch_stats: Any\n",
    "\n",
    "# Replace this with our subclassed TrainState when we need it.\n",
    "#state = train_state.TrainState.create(apply_fn=GPT2.apply, params=y_init['params'], tx=tx)\n",
    "\n",
    "# Now try our apply function\n",
    "#grads, loss, accuracy = apply_model(state, x, y)\n",
    "#state = state.apply_gradients(grads=grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(11.300755, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss: 11.310973167419434\n",
      "step 2, loss: 8.371686935424805\n",
      "step 3, loss: 7.288360118865967\n",
      "step 4, loss: 7.384831428527832\n",
      "step 5, loss: 6.379255294799805\n",
      "step 6, loss: 5.839833736419678\n",
      "step 7, loss: 5.47161340713501\n",
      "step 8, loss: 5.100825309753418\n",
      "step 9, loss: 4.55207633972168\n",
      "step 10, loss: 4.081092834472656\n",
      "step 11, loss: 3.5328078269958496\n",
      "step 12, loss: 2.8801956176757812\n",
      "step 13, loss: 2.2455806732177734\n",
      "step 14, loss: 1.6977598667144775\n",
      "step 15, loss: 1.2593261003494263\n",
      "step 16, loss: 0.900545597076416\n",
      "step 17, loss: 0.6233440041542053\n",
      "step 18, loss: 0.40818876028060913\n",
      "step 19, loss: 0.2590666115283966\n",
      "step 20, loss: 0.16967424750328064\n",
      "step 21, loss: 0.1210031658411026\n",
      "step 22, loss: 0.0914439707994461\n",
      "step 23, loss: 0.07090624421834946\n",
      "step 24, loss: 0.05630887299776077\n",
      "step 25, loss: 0.04591205716133118\n",
      "step 26, loss: 0.03834603726863861\n",
      "step 27, loss: 0.0326601043343544\n",
      "step 28, loss: 0.02821948006749153\n",
      "step 29, loss: 0.0246470645070076\n",
      "step 30, loss: 0.021716047078371048\n",
      "step 31, loss: 0.019280407577753067\n",
      "step 32, loss: 0.017235297709703445\n",
      "step 33, loss: 0.015508908778429031\n",
      "step 34, loss: 0.014036704786121845\n",
      "step 35, loss: 0.012776904739439487\n",
      "step 36, loss: 0.011688146740198135\n",
      "step 37, loss: 0.010745307430624962\n",
      "step 38, loss: 0.009923090226948261\n",
      "step 39, loss: 0.009203607216477394\n",
      "step 40, loss: 0.008569514378905296\n",
      "step 41, loss: 0.008010569028556347\n",
      "step 42, loss: 0.0075147319585084915\n",
      "step 43, loss: 0.007072921376675367\n",
      "step 44, loss: 0.006676890887320042\n",
      "step 45, loss: 0.006322508677840233\n",
      "step 46, loss: 0.006003261543810368\n",
      "step 47, loss: 0.0057152025401592255\n",
      "step 48, loss: 0.005454070400446653\n",
      "step 49, loss: 0.005216671619564295\n",
      "step 50, loss: 0.0050000473856925964\n"
     ]
    }
   ],
   "source": [
    "# Okay, now let's create a small number of epochs to make sure it's doing something\n",
    "# We get roughly the same results as Karpathy\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    grads, loss, accuracy = apply_model(state, x, y)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    print(f'step {epoch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1., dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do this for the entire tiny Shakespeare dataset\n",
    "# We can follow Karpathy's style, but we will use the tensorflow datasets\n",
    "# tools as our data loader as it is the natural thing to use with jax/flax\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(\"input.txt\", 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        # Could load it into gpu memory here if you want for speed. It would \n",
    "        # depend on how much you have to spare\n",
    "        self.tokens = tokens\n",
    "        print(f'Loaded {len(self.tokens)} tokens')\n",
    "        print(f'One epoch = {len(self.tokens) // (B*T) } batches')\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = np.array(self.tokens[self.current_position:self.current_position+B*T+1])\n",
    "        x = buf[:-1].reshape((B, T))\n",
    "        y = buf[1:].reshape((B,T))\n",
    "        self.current_position+=B*T\n",
    "\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 338025 tokens\n",
      "One epoch = 82 batches\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_loader = DataLoaderLite(16, 256)\n",
    "\n",
    "learning_rate = 0.0003\n",
    "\n",
    "# Start with AdamW\n",
    "tx = optax.adamw(learning_rate=learning_rate)\n",
    "\n",
    "# Flax handles the training state for us. Generally, you should subclass the TrainState and \n",
    "# add anything you need to it. Here, we don't need any additions, but we may in the future\n",
    "# so I will leave it in for now. Things you might add are the batch_stats or dropout keys\n",
    "class TrainState(train_state.TrainState):\n",
    "  key: jax.Array\n",
    "  batch_stats: Any\n",
    "\n",
    "# Replace this with our subclassed TrainState when we need it.\n",
    "state = train_state.TrainState.create(apply_fn=GPT2.apply, params=y_init['params'], tx=tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss: 10.875, dt: 14283.27ms, tok/sec: 286.7690876994621\n",
      "step 2, loss: 10.25, dt: 626.04ms, tok/sec: 6542.705640365526\n",
      "step 3, loss: 10, dt: 407.88ms, tok/sec: 10042.191075475415\n",
      "step 4, loss: 9.75, dt: 364.51ms, tok/sec: 11237.126219790181\n",
      "step 5, loss: 9.5625, dt: 437.70ms, tok/sec: 9358.032545284399\n",
      "step 6, loss: 9.3125, dt: 412.43ms, tok/sec: 9931.398363098851\n",
      "step 7, loss: 9.25, dt: 446.00ms, tok/sec: 9183.923192308966\n",
      "step 8, loss: 9.125, dt: 436.62ms, tok/sec: 9381.052850458436\n",
      "step 9, loss: 9, dt: 458.89ms, tok/sec: 8925.845394659935\n",
      "step 10, loss: 8.875, dt: 376.08ms, tok/sec: 10891.381532184341\n",
      "step 11, loss: 8.8125, dt: 400.12ms, tok/sec: 10237.032110441614\n",
      "step 12, loss: 8.75, dt: 400.87ms, tok/sec: 10217.883564010652\n",
      "step 13, loss: 8.6875, dt: 448.63ms, tok/sec: 9130.069503847626\n",
      "step 14, loss: 8.625, dt: 363.84ms, tok/sec: 11257.59659884317\n",
      "step 15, loss: 8.375, dt: 375.39ms, tok/sec: 10911.199551354479\n",
      "step 16, loss: 8.25, dt: 419.38ms, tok/sec: 9766.788677033488\n",
      "step 17, loss: 8.0625, dt: 429.44ms, tok/sec: 9538.034275003942\n",
      "step 18, loss: 7.9375, dt: 447.18ms, tok/sec: 9159.72440855881\n",
      "step 19, loss: 7.9375, dt: 410.46ms, tok/sec: 9979.134959243811\n",
      "step 20, loss: 7.8125, dt: 427.62ms, tok/sec: 9578.508508936013\n",
      "step 21, loss: 7.875, dt: 392.61ms, tok/sec: 10432.686002301523\n",
      "step 22, loss: 7.65625, dt: 403.03ms, tok/sec: 10163.0713201031\n",
      "step 23, loss: 7.5625, dt: 417.56ms, tok/sec: 9809.271135914254\n",
      "step 24, loss: 7.59375, dt: 396.95ms, tok/sec: 10318.5991968489\n",
      "step 25, loss: 7.53125, dt: 423.03ms, tok/sec: 9682.470696776862\n",
      "step 26, loss: 7.4375, dt: 417.38ms, tok/sec: 9813.563261121028\n",
      "step 27, loss: 7.25, dt: 433.56ms, tok/sec: 9447.347509961583\n",
      "step 28, loss: 7.28125, dt: 431.59ms, tok/sec: 9490.581263306525\n",
      "step 29, loss: 7.21875, dt: 406.28ms, tok/sec: 10081.81069290383\n",
      "step 30, loss: 7.03125, dt: 404.58ms, tok/sec: 10123.96302780693\n",
      "step 31, loss: 7, dt: 382.11ms, tok/sec: 10719.37618995029\n",
      "step 32, loss: 6.9375, dt: 469.79ms, tok/sec: 8718.763034599238\n",
      "step 33, loss: 6.84375, dt: 470.15ms, tok/sec: 8712.08234248278\n",
      "step 34, loss: 6.875, dt: 428.33ms, tok/sec: 9562.705600570873\n",
      "step 35, loss: 6.875, dt: 387.14ms, tok/sec: 10580.163940826997\n",
      "step 36, loss: 6.84375, dt: 417.98ms, tok/sec: 9799.57446478401\n",
      "step 37, loss: 6.6875, dt: 376.09ms, tok/sec: 10890.96726472352\n",
      "step 38, loss: 6.78125, dt: 417.86ms, tok/sec: 9802.364558305399\n",
      "step 39, loss: 6.65625, dt: 364.19ms, tok/sec: 11246.821952549572\n",
      "step 40, loss: 6.53125, dt: 456.33ms, tok/sec: 8975.963870164149\n",
      "step 41, loss: 6.59375, dt: 398.33ms, tok/sec: 10282.913464853476\n",
      "step 42, loss: 6.65625, dt: 395.40ms, tok/sec: 10359.097523988006\n",
      "step 43, loss: 6.53125, dt: 373.67ms, tok/sec: 10961.456713401025\n",
      "step 44, loss: 6.53125, dt: 390.98ms, tok/sec: 10476.181557633463\n",
      "step 45, loss: 6.65625, dt: 408.29ms, tok/sec: 10032.034541293699\n",
      "step 46, loss: 6.59375, dt: 428.12ms, tok/sec: 9567.429273437874\n",
      "step 47, loss: 6.5, dt: 365.11ms, tok/sec: 11218.495367601157\n",
      "step 48, loss: 6.53125, dt: 375.77ms, tok/sec: 10900.42040158013\n",
      "step 49, loss: 6.53125, dt: 373.29ms, tok/sec: 10972.686351228273\n",
      "step 50, loss: 6.4375, dt: 427.61ms, tok/sec: 9578.898375923885\n",
      "step 51, loss: 6.5625, dt: 455.02ms, tok/sec: 9001.784223807648\n",
      "step 52, loss: 6.5, dt: 460.63ms, tok/sec: 8892.170577767747\n",
      "step 53, loss: 6.6875, dt: 437.12ms, tok/sec: 9370.37943998739\n",
      "step 54, loss: 6.625, dt: 395.46ms, tok/sec: 10357.430025923917\n",
      "step 55, loss: 6.5, dt: 442.25ms, tok/sec: 9261.709539148273\n",
      "step 56, loss: 6.5625, dt: 409.14ms, tok/sec: 10011.304576156714\n",
      "step 57, loss: 6.75, dt: 426.23ms, tok/sec: 9609.80923096888\n",
      "step 58, loss: 6.71875, dt: 406.77ms, tok/sec: 10069.667143973305\n",
      "step 59, loss: 6.5, dt: 422.92ms, tok/sec: 9685.123516707528\n",
      "step 60, loss: 6.59375, dt: 424.65ms, tok/sec: 9645.64027439192\n",
      "step 61, loss: 6.53125, dt: 446.33ms, tok/sec: 9177.059944606033\n",
      "step 62, loss: 6.5, dt: 411.77ms, tok/sec: 9947.407565414256\n",
      "step 63, loss: 6.59375, dt: 410.89ms, tok/sec: 9968.71215443614\n",
      "step 64, loss: 6.40625, dt: 438.51ms, tok/sec: 9340.692773684927\n",
      "step 65, loss: 6.3125, dt: 411.39ms, tok/sec: 9956.54507805052\n",
      "step 66, loss: 6.46875, dt: 402.41ms, tok/sec: 10178.582265551635\n",
      "step 67, loss: 6.65625, dt: 425.81ms, tok/sec: 9619.39772705003\n",
      "step 68, loss: 6.59375, dt: 436.21ms, tok/sec: 9389.953975757557\n",
      "step 69, loss: 6.5625, dt: 385.78ms, tok/sec: 10617.467402025615\n",
      "step 70, loss: 6.4375, dt: 407.04ms, tok/sec: 10062.990338167081\n",
      "step 71, loss: 6.34375, dt: 430.35ms, tok/sec: 9517.83287821347\n",
      "step 72, loss: 6.53125, dt: 377.80ms, tok/sec: 10841.57870743165\n",
      "step 73, loss: 6.5625, dt: 413.78ms, tok/sec: 9898.923719511457\n",
      "step 74, loss: 6.34375, dt: 390.31ms, tok/sec: 10494.246863460745\n",
      "step 75, loss: 6.375, dt: 411.44ms, tok/sec: 9955.171938949647\n",
      "step 76, loss: 6.5625, dt: 424.11ms, tok/sec: 9657.981828464392\n",
      "step 77, loss: 6.3125, dt: 428.09ms, tok/sec: 9568.095329698024\n",
      "step 78, loss: 6.15625, dt: 409.86ms, tok/sec: 9993.560086882975\n",
      "step 79, loss: 6.21875, dt: 426.57ms, tok/sec: 9602.192974248825\n",
      "step 80, loss: 6.21875, dt: 427.40ms, tok/sec: 9583.53114197178\n",
      "step 81, loss: 6.59375, dt: 428.26ms, tok/sec: 9564.217520243394\n",
      "step 82, loss: 6.46875, dt: 462.39ms, tok/sec: 8858.324095443755\n",
      "step 83, loss: 6.34375, dt: 422.03ms, tok/sec: 9705.532129905978\n",
      "step 84, loss: 6.28125, dt: 438.68ms, tok/sec: 9337.123928709048\n",
      "step 85, loss: 6.3125, dt: 426.86ms, tok/sec: 9595.76240756049\n",
      "step 86, loss: 6.34375, dt: 451.42ms, tok/sec: 9073.67009194159\n",
      "step 87, loss: 6.125, dt: 415.74ms, tok/sec: 9852.401632368425\n",
      "step 88, loss: 6.03125, dt: 414.87ms, tok/sec: 9872.960344073565\n",
      "step 89, loss: 6.03125, dt: 425.44ms, tok/sec: 9627.69411271549\n",
      "step 90, loss: 5.96875, dt: 432.41ms, tok/sec: 9472.590582475023\n",
      "step 91, loss: 6.09375, dt: 405.66ms, tok/sec: 10097.003191335554\n",
      "step 92, loss: 6, dt: 399.12ms, tok/sec: 10262.458817836981\n",
      "step 93, loss: 6.15625, dt: 392.29ms, tok/sec: 10441.175974658989\n",
      "step 94, loss: 6.1875, dt: 434.39ms, tok/sec: 9429.375298580862\n",
      "step 95, loss: 6.21875, dt: 408.02ms, tok/sec: 10038.646854893703\n",
      "step 96, loss: 6.21875, dt: 384.26ms, tok/sec: 10659.53038392595\n",
      "step 97, loss: 6.0625, dt: 419.79ms, tok/sec: 9757.33093733565\n",
      "step 98, loss: 6.0625, dt: 464.01ms, tok/sec: 8827.409564093236\n",
      "step 99, loss: 5.9375, dt: 431.76ms, tok/sec: 9486.671723318395\n",
      "step 100, loss: 5.90625, dt: 402.04ms, tok/sec: 10188.137539718027\n",
      "step 101, loss: 6.15625, dt: 387.17ms, tok/sec: 10579.336504284725\n",
      "step 102, loss: 6.03125, dt: 448.65ms, tok/sec: 9129.574618101353\n",
      "step 103, loss: 6.25, dt: 474.86ms, tok/sec: 8625.74436975102\n",
      "step 104, loss: 6.0625, dt: 414.63ms, tok/sec: 9878.78510948687\n",
      "step 105, loss: 6.1875, dt: 484.16ms, tok/sec: 8459.980826523768\n",
      "step 106, loss: 6.28125, dt: 467.37ms, tok/sec: 8763.920320155896\n",
      "step 107, loss: 6.3125, dt: 433.54ms, tok/sec: 9447.73196833281\n",
      "step 108, loss: 6.3125, dt: 381.17ms, tok/sec: 10745.75963591738\n",
      "step 109, loss: 6.15625, dt: 413.77ms, tok/sec: 9899.28876929238\n",
      "step 110, loss: 6.21875, dt: 453.38ms, tok/sec: 9034.385714413425\n",
      "step 111, loss: 6.25, dt: 469.88ms, tok/sec: 8717.033295971918\n",
      "step 112, loss: 6.125, dt: 415.87ms, tok/sec: 9849.261118627743\n",
      "step 113, loss: 6.03125, dt: 497.12ms, tok/sec: 8239.480642108863\n",
      "step 114, loss: 5.96875, dt: 432.90ms, tok/sec: 9461.661081600563\n",
      "step 115, loss: 6, dt: 416.02ms, tok/sec: 9845.75582454248\n",
      "step 116, loss: 6.125, dt: 390.97ms, tok/sec: 10476.590425616252\n",
      "step 117, loss: 6.15625, dt: 394.25ms, tok/sec: 10389.37420416062\n",
      "step 118, loss: 6.09375, dt: 408.36ms, tok/sec: 10030.423817241717\n",
      "step 119, loss: 5.96875, dt: 404.35ms, tok/sec: 10129.89668016347\n",
      "step 120, loss: 6.125, dt: 384.31ms, tok/sec: 10657.996184681278\n",
      "step 121, loss: 5.96875, dt: 382.05ms, tok/sec: 10720.981635030794\n",
      "step 122, loss: 5.84375, dt: 419.55ms, tok/sec: 9762.775913605026\n",
      "step 123, loss: 5.96875, dt: 433.14ms, tok/sec: 9456.520691663336\n",
      "step 124, loss: 6.0625, dt: 389.76ms, tok/sec: 10509.075728927399\n",
      "step 125, loss: 5.875, dt: 371.39ms, tok/sec: 11028.714152554341\n",
      "step 126, loss: 5.90625, dt: 408.48ms, tok/sec: 10027.50240562665\n",
      "step 127, loss: 6.09375, dt: 438.91ms, tok/sec: 9332.280148709224\n",
      "step 128, loss: 6, dt: 462.07ms, tok/sec: 8864.462565419768\n",
      "step 129, loss: 5.875, dt: 414.67ms, tok/sec: 9877.76272375344\n",
      "step 130, loss: 5.875, dt: 451.91ms, tok/sec: 9063.751252474349\n",
      "step 131, loss: 5.875, dt: 436.41ms, tok/sec: 9385.608978785522\n",
      "step 132, loss: 5.8125, dt: 409.83ms, tok/sec: 9994.37401117191\n",
      "step 133, loss: 5.96875, dt: 432.63ms, tok/sec: 9467.69396741281\n",
      "step 134, loss: 5.875, dt: 444.64ms, tok/sec: 9211.958255248255\n",
      "step 135, loss: 6.15625, dt: 426.56ms, tok/sec: 9602.311046821964\n",
      "step 136, loss: 6.125, dt: 410.90ms, tok/sec: 9968.289911178263\n",
      "step 137, loss: 5.9375, dt: 415.33ms, tok/sec: 9861.96540708471\n",
      "step 138, loss: 6.0625, dt: 481.59ms, tok/sec: 8505.1341519381\n",
      "step 139, loss: 6.34375, dt: 442.53ms, tok/sec: 9255.90131991882\n",
      "step 140, loss: 6.25, dt: 472.59ms, tok/sec: 8667.12399832912\n",
      "step 141, loss: 5.9375, dt: 398.13ms, tok/sec: 10288.1846569541\n",
      "step 142, loss: 6.125, dt: 423.15ms, tok/sec: 9679.873869945002\n",
      "step 143, loss: 6.03125, dt: 431.10ms, tok/sec: 9501.362269017802\n",
      "step 144, loss: 6, dt: 391.06ms, tok/sec: 10474.112153110982\n",
      "step 145, loss: 6, dt: 418.67ms, tok/sec: 9783.44102072021\n",
      "step 146, loss: 5.875, dt: 416.06ms, tok/sec: 9844.655643013335\n",
      "step 147, loss: 5.71875, dt: 439.34ms, tok/sec: 9323.08826882825\n",
      "step 148, loss: 5.9375, dt: 387.83ms, tok/sec: 10561.360362752908\n",
      "step 149, loss: 6.09375, dt: 365.41ms, tok/sec: 11209.404216281817\n",
      "step 150, loss: 6, dt: 416.88ms, tok/sec: 9825.467476196794\n",
      "step 151, loss: 6, dt: 396.02ms, tok/sec: 10343.019512814786\n",
      "step 152, loss: 5.8125, dt: 424.15ms, tok/sec: 9656.879783297518\n",
      "step 153, loss: 5.71875, dt: 415.78ms, tok/sec: 9851.27172055663\n",
      "step 154, loss: 6.03125, dt: 385.92ms, tok/sec: 10613.617030845828\n",
      "step 155, loss: 5.96875, dt: 423.68ms, tok/sec: 9667.596021962154\n",
      "step 156, loss: 5.75, dt: 383.40ms, tok/sec: 10683.3139320056\n",
      "step 157, loss: 5.8125, dt: 393.76ms, tok/sec: 10402.18192210535\n",
      "step 158, loss: 6, dt: 411.89ms, tok/sec: 9944.350103988132\n",
      "step 159, loss: 5.8125, dt: 399.80ms, tok/sec: 10245.08431029126\n",
      "step 160, loss: 5.59375, dt: 463.63ms, tok/sec: 8834.581832094356\n",
      "step 161, loss: 5.625, dt: 384.11ms, tok/sec: 10663.566378102652\n",
      "step 162, loss: 5.65625, dt: 382.36ms, tok/sec: 10712.324619578638\n",
      "step 163, loss: 6.15625, dt: 381.19ms, tok/sec: 10745.36981366211\n",
      "step 164, loss: 6.03125, dt: 438.64ms, tok/sec: 9337.961321630535\n",
      "step 165, loss: 5.96875, dt: 454.63ms, tok/sec: 9009.479000621963\n",
      "step 166, loss: 5.96875, dt: 409.24ms, tok/sec: 10008.73247235052\n",
      "step 167, loss: 6, dt: 423.26ms, tok/sec: 9677.169414278947\n",
      "step 168, loss: 6, dt: 404.17ms, tok/sec: 10134.348496825176\n",
      "step 169, loss: 5.78125, dt: 359.41ms, tok/sec: 11396.559635041967\n",
      "step 170, loss: 5.65625, dt: 359.99ms, tok/sec: 11378.067438411932\n",
      "step 171, loss: 5.65625, dt: 397.77ms, tok/sec: 10297.335849147077\n",
      "step 172, loss: 5.625, dt: 462.61ms, tok/sec: 8854.17413788394\n",
      "step 173, loss: 5.71875, dt: 416.49ms, tok/sec: 9834.624314490553\n",
      "step 174, loss: 5.625, dt: 367.13ms, tok/sec: 11156.722745535815\n",
      "step 175, loss: 5.8125, dt: 405.85ms, tok/sec: 10092.483324364779\n",
      "step 176, loss: 5.8125, dt: 416.17ms, tok/sec: 9842.202277702316\n",
      "step 177, loss: 5.84375, dt: 396.78ms, tok/sec: 10323.007553638889\n",
      "step 178, loss: 5.84375, dt: 398.22ms, tok/sec: 10285.86244979018\n",
      "step 179, loss: 5.71875, dt: 425.97ms, tok/sec: 9615.70965037282\n",
      "step 180, loss: 5.75, dt: 451.01ms, tok/sec: 9081.747548755295\n",
      "step 181, loss: 5.5625, dt: 418.43ms, tok/sec: 9789.021161084755\n",
      "step 182, loss: 5.5625, dt: 453.18ms, tok/sec: 9038.368737738529\n",
      "step 183, loss: 5.8125, dt: 433.06ms, tok/sec: 9458.264777274886\n",
      "step 184, loss: 5.6875, dt: 436.88ms, tok/sec: 9375.661940962349\n",
      "step 185, loss: 5.9375, dt: 438.84ms, tok/sec: 9333.801213512057\n",
      "step 186, loss: 5.71875, dt: 454.25ms, tok/sec: 9017.068624193746\n",
      "step 187, loss: 5.875, dt: 472.62ms, tok/sec: 8666.551238221497\n",
      "step 188, loss: 5.96875, dt: 445.94ms, tok/sec: 9185.077068088243\n",
      "step 189, loss: 6.03125, dt: 423.42ms, tok/sec: 9673.562191962548\n",
      "step 190, loss: 6, dt: 467.10ms, tok/sec: 8769.015425009546\n",
      "step 191, loss: 5.84375, dt: 388.82ms, tok/sec: 10534.465184185918\n",
      "step 192, loss: 5.9375, dt: 457.48ms, tok/sec: 8953.472433205441\n",
      "step 193, loss: 5.9375, dt: 445.67ms, tok/sec: 9190.644293679165\n",
      "step 194, loss: 5.84375, dt: 414.45ms, tok/sec: 9882.89382538784\n",
      "step 195, loss: 5.75, dt: 399.82ms, tok/sec: 10244.540586698215\n",
      "step 196, loss: 5.6875, dt: 424.83ms, tok/sec: 9641.396354199987\n",
      "step 197, loss: 5.6875, dt: 454.64ms, tok/sec: 9009.403405139909\n",
      "step 198, loss: 5.8125, dt: 490.58ms, tok/sec: 8349.356241264213\n",
      "step 199, loss: 5.84375, dt: 496.21ms, tok/sec: 8254.651293102414\n",
      "step 200, loss: 5.8125, dt: 428.00ms, tok/sec: 9570.099386071779\n",
      "step 201, loss: 5.65625, dt: 423.40ms, tok/sec: 9674.036099407953\n",
      "step 202, loss: 5.84375, dt: 480.44ms, tok/sec: 8525.46502547734\n",
      "step 203, loss: 5.65625, dt: 393.26ms, tok/sec: 10415.381164021677\n",
      "step 204, loss: 5.53125, dt: 456.77ms, tok/sec: 8967.282315476405\n",
      "step 205, loss: 5.65625, dt: 424.84ms, tok/sec: 9641.223212291474\n",
      "step 206, loss: 5.78125, dt: 434.13ms, tok/sec: 9434.993980340134\n",
      "step 207, loss: 5.5625, dt: 380.14ms, tok/sec: 10774.908186170256\n",
      "step 208, loss: 5.625, dt: 388.22ms, tok/sec: 10550.638959667143\n",
      "step 209, loss: 5.84375, dt: 395.31ms, tok/sec: 10361.590404056853\n",
      "step 210, loss: 5.75, dt: 434.56ms, tok/sec: 9425.598751729594\n",
      "step 211, loss: 5.625, dt: 426.27ms, tok/sec: 9609.040612209055\n",
      "step 212, loss: 5.5625, dt: 417.31ms, tok/sec: 9815.245273877708\n",
      "step 213, loss: 5.59375, dt: 423.43ms, tok/sec: 9673.491382193659\n",
      "step 214, loss: 5.5625, dt: 412.10ms, tok/sec: 9939.22400797921\n",
      "step 215, loss: 5.6875, dt: 390.38ms, tok/sec: 10492.253618877894\n",
      "step 216, loss: 5.59375, dt: 413.03ms, tok/sec: 9917.071857756473\n",
      "step 217, loss: 5.875, dt: 405.08ms, tok/sec: 10111.664219562512\n",
      "step 218, loss: 5.8125, dt: 435.22ms, tok/sec: 9411.414623083121\n",
      "step 219, loss: 5.65625, dt: 443.03ms, tok/sec: 9245.47083322346\n",
      "step 220, loss: 5.8125, dt: 476.04ms, tok/sec: 8604.407204918673\n",
      "step 221, loss: 6.125, dt: 446.85ms, tok/sec: 9166.410124745227\n",
      "step 222, loss: 6, dt: 431.84ms, tok/sec: 9484.92761973432\n",
      "step 223, loss: 5.65625, dt: 404.05ms, tok/sec: 10137.248762780082\n",
      "step 224, loss: 5.8125, dt: 423.13ms, tok/sec: 9680.272032365487\n",
      "step 225, loss: 5.78125, dt: 397.04ms, tok/sec: 10316.393332604735\n",
      "step 226, loss: 5.6875, dt: 391.18ms, tok/sec: 10470.958522986306\n",
      "step 227, loss: 5.6875, dt: 409.31ms, tok/sec: 10006.977668193354\n",
      "step 228, loss: 5.5625, dt: 461.92ms, tok/sec: 8867.404547702148\n",
      "step 229, loss: 5.40625, dt: 428.74ms, tok/sec: 9553.66536224961\n",
      "step 230, loss: 5.625, dt: 416.67ms, tok/sec: 9830.330625489589\n",
      "step 231, loss: 5.75, dt: 506.41ms, tok/sec: 8088.2640579384\n",
      "step 232, loss: 5.6875, dt: 412.00ms, tok/sec: 9941.628184904119\n",
      "step 233, loss: 5.65625, dt: 403.88ms, tok/sec: 10141.545401470712\n",
      "step 234, loss: 5.46875, dt: 426.82ms, tok/sec: 9596.58786977588\n",
      "step 235, loss: 5.40625, dt: 416.54ms, tok/sec: 9833.419677291955\n",
      "step 236, loss: 5.78125, dt: 446.49ms, tok/sec: 9173.776673973349\n",
      "step 237, loss: 5.65625, dt: 432.59ms, tok/sec: 9468.52363428734\n",
      "step 238, loss: 5.4375, dt: 416.32ms, tok/sec: 9838.544241716203\n",
      "step 239, loss: 5.46875, dt: 431.34ms, tok/sec: 9496.073683017763\n",
      "step 240, loss: 5.65625, dt: 439.03ms, tok/sec: 9329.553613844771\n",
      "step 241, loss: 5.46875, dt: 450.94ms, tok/sec: 9083.16882556588\n",
      "step 242, loss: 5.25, dt: 460.88ms, tok/sec: 8887.38653183395\n",
      "step 243, loss: 5.25, dt: 450.58ms, tok/sec: 9090.474287201912\n",
      "step 244, loss: 5.28125, dt: 437.28ms, tok/sec: 9366.890018352266\n",
      "step 245, loss: 5.875, dt: 426.92ms, tok/sec: 9594.213709107704\n",
      "step 246, loss: 5.71875, dt: 410.22ms, tok/sec: 9984.853584781526\n",
      "step 247, loss: 5.71875, dt: 432.94ms, tok/sec: 9460.9003475448\n",
      "step 248, loss: 5.6875, dt: 415.41ms, tok/sec: 9860.131525284429\n",
      "step 249, loss: 5.71875, dt: 447.99ms, tok/sec: 9143.145161573591\n",
      "step 250, loss: 5.71875, dt: 419.30ms, tok/sec: 9768.74908111649\n",
      "step 251, loss: 5.5, dt: 497.77ms, tok/sec: 8228.761889525875\n",
      "step 252, loss: 5.3125, dt: 452.64ms, tok/sec: 9049.185268994084\n",
      "step 253, loss: 5.34375, dt: 414.10ms, tok/sec: 9891.2411091862\n",
      "step 254, loss: 5.3125, dt: 467.06ms, tok/sec: 8769.68238704609\n",
      "step 255, loss: 5.4375, dt: 439.23ms, tok/sec: 9325.512355583734\n",
      "step 256, loss: 5.34375, dt: 461.23ms, tok/sec: 8880.60110651685\n",
      "step 257, loss: 5.53125, dt: 460.30ms, tok/sec: 8898.46667788911\n",
      "step 258, loss: 5.53125, dt: 446.03ms, tok/sec: 9183.181918722341\n",
      "step 259, loss: 5.5625, dt: 454.95ms, tok/sec: 9003.251353511321\n",
      "step 260, loss: 5.53125, dt: 445.29ms, tok/sec: 9198.43399698559\n",
      "step 261, loss: 5.4375, dt: 440.69ms, tok/sec: 9294.564847666441\n",
      "step 262, loss: 5.4375, dt: 481.64ms, tok/sec: 8504.292118223431\n",
      "step 263, loss: 5.21875, dt: 413.86ms, tok/sec: 9897.161592268403\n",
      "step 264, loss: 5.25, dt: 422.07ms, tok/sec: 9704.589144221574\n",
      "step 265, loss: 5.53125, dt: 455.43ms, tok/sec: 8993.688238071756\n",
      "step 266, loss: 5.34375, dt: 444.26ms, tok/sec: 9219.863313143933\n",
      "step 267, loss: 5.625, dt: 433.66ms, tok/sec: 9445.28547685585\n",
      "step 268, loss: 5.34375, dt: 393.92ms, tok/sec: 10398.032943494916\n",
      "step 269, loss: 5.59375, dt: 435.05ms, tok/sec: 9414.926983890378\n",
      "step 270, loss: 5.71875, dt: 459.10ms, tok/sec: 8921.812639469299\n",
      "step 271, loss: 5.75, dt: 403.99ms, tok/sec: 10138.911931843331\n",
      "step 272, loss: 5.75, dt: 458.92ms, tok/sec: 8925.242567393108\n",
      "step 273, loss: 5.59375, dt: 494.66ms, tok/sec: 8280.460655462735\n",
      "step 274, loss: 5.625, dt: 447.17ms, tok/sec: 9159.76347796415\n",
      "step 275, loss: 5.65625, dt: 407.62ms, tok/sec: 10048.62870502137\n",
      "step 276, loss: 5.5625, dt: 417.68ms, tok/sec: 9806.488305188459\n",
      "step 277, loss: 5.46875, dt: 412.68ms, tok/sec: 9925.431126342803\n",
      "step 278, loss: 5.375, dt: 493.21ms, tok/sec: 8304.701320447897\n",
      "step 279, loss: 5.4375, dt: 468.96ms, tok/sec: 8734.144044315697\n",
      "step 280, loss: 5.5625, dt: 471.72ms, tok/sec: 8683.183297431113\n",
      "step 281, loss: 5.59375, dt: 407.83ms, tok/sec: 10043.51199618367\n",
      "step 282, loss: 5.53125, dt: 469.01ms, tok/sec: 8733.207223098543\n",
      "step 283, loss: 5.40625, dt: 488.50ms, tok/sec: 8384.898664946073\n",
      "step 284, loss: 5.59375, dt: 444.27ms, tok/sec: 9219.695084437595\n",
      "step 285, loss: 5.40625, dt: 455.22ms, tok/sec: 8997.904061081465\n",
      "step 286, loss: 5.25, dt: 421.21ms, tok/sec: 9724.380863540653\n",
      "step 287, loss: 5.375, dt: 407.39ms, tok/sec: 10054.186028437527\n",
      "step 288, loss: 5.53125, dt: 429.00ms, tok/sec: 9547.724080899516\n",
      "step 289, loss: 5.25, dt: 428.95ms, tok/sec: 9548.801351738168\n",
      "step 290, loss: 5.3125, dt: 485.80ms, tok/sec: 8431.436237882172\n",
      "step 291, loss: 5.59375, dt: 414.63ms, tok/sec: 9878.705583006196\n",
      "step 292, loss: 5.4375, dt: 399.67ms, tok/sec: 10248.408999340827\n",
      "step 293, loss: 5.34375, dt: 451.94ms, tok/sec: 9063.134435583466\n",
      "step 294, loss: 5.25, dt: 442.70ms, tok/sec: 9252.237509653529\n",
      "step 295, loss: 5.34375, dt: 447.56ms, tok/sec: 9151.800214360444\n",
      "step 296, loss: 5.28125, dt: 419.35ms, tok/sec: 9767.438355510174\n",
      "step 297, loss: 5.40625, dt: 444.92ms, tok/sec: 9206.079050705626\n",
      "step 298, loss: 5.28125, dt: 443.44ms, tok/sec: 9236.905959193857\n",
      "step 299, loss: 5.65625, dt: 487.13ms, tok/sec: 8408.380833052726\n",
      "step 300, loss: 5.59375, dt: 466.52ms, tok/sec: 8779.959250048807\n",
      "step 301, loss: 5.40625, dt: 424.35ms, tok/sec: 9652.430730921556\n",
      "step 302, loss: 5.5625, dt: 450.31ms, tok/sec: 9096.028522990679\n",
      "step 303, loss: 5.90625, dt: 424.78ms, tok/sec: 9642.662643434296\n",
      "step 304, loss: 5.75, dt: 460.64ms, tok/sec: 8891.908241847254\n",
      "step 305, loss: 5.40625, dt: 396.19ms, tok/sec: 10338.575386420738\n",
      "step 306, loss: 5.5625, dt: 421.26ms, tok/sec: 9723.291129325284\n",
      "step 307, loss: 5.53125, dt: 422.59ms, tok/sec: 9692.532903804546\n",
      "step 308, loss: 5.4375, dt: 413.74ms, tok/sec: 9900.007539737193\n",
      "step 309, loss: 5.40625, dt: 399.00ms, tok/sec: 10265.757989186812\n",
      "step 310, loss: 5.28125, dt: 413.41ms, tok/sec: 9907.852340733521\n",
      "step 311, loss: 5.125, dt: 447.21ms, tok/sec: 9159.011450450864\n",
      "step 312, loss: 5.3125, dt: 428.95ms, tok/sec: 9548.87565523961\n",
      "step 313, loss: 5.4375, dt: 429.83ms, tok/sec: 9529.278450631806\n",
      "step 314, loss: 5.375, dt: 447.69ms, tok/sec: 9149.119634969715\n",
      "step 315, loss: 5.34375, dt: 440.76ms, tok/sec: 9293.106812039803\n",
      "step 316, loss: 5.125, dt: 404.40ms, tok/sec: 10128.546969283385\n",
      "step 317, loss: 5.09375, dt: 435.47ms, tok/sec: 9405.963023005364\n",
      "step 318, loss: 5.5625, dt: 407.08ms, tok/sec: 10061.782146115993\n",
      "step 319, loss: 5.375, dt: 451.60ms, tok/sec: 9069.962349338753\n",
      "step 320, loss: 5.125, dt: 412.76ms, tok/sec: 9923.38441046388\n",
      "step 321, loss: 5.15625, dt: 433.50ms, tok/sec: 9448.667267983403\n",
      "step 322, loss: 5.34375, dt: 421.77ms, tok/sec: 9711.534249812184\n",
      "step 323, loss: 5.1875, dt: 430.76ms, tok/sec: 9508.693039561556\n",
      "step 324, loss: 4.96875, dt: 393.58ms, tok/sec: 10407.008721842405\n",
      "step 325, loss: 4.9375, dt: 435.49ms, tok/sec: 9405.551060541442\n",
      "step 326, loss: 4.96875, dt: 419.93ms, tok/sec: 9753.957215116137\n",
      "step 327, loss: 5.625, dt: 466.94ms, tok/sec: 8772.055624855437\n",
      "step 328, loss: 5.5, dt: 444.99ms, tok/sec: 9204.64371060337\n",
      "step 329, loss: 5.5, dt: 431.05ms, tok/sec: 9502.308217844564\n",
      "step 330, loss: 5.4375, dt: 455.29ms, tok/sec: 8996.405695951638\n",
      "step 331, loss: 5.5, dt: 401.64ms, tok/sec: 10198.26164260935\n",
      "step 332, loss: 5.5, dt: 461.05ms, tok/sec: 8884.008602764407\n",
      "step 333, loss: 5.25, dt: 428.81ms, tok/sec: 9552.06117813729\n",
      "step 334, loss: 5.0625, dt: 428.37ms, tok/sec: 9561.720980401105\n",
      "step 335, loss: 5.09375, dt: 438.41ms, tok/sec: 9342.85164928691\n",
      "step 336, loss: 5.0625, dt: 397.74ms, tok/sec: 10298.169144220423\n",
      "step 337, loss: 5.1875, dt: 403.11ms, tok/sec: 10161.045628055583\n",
      "step 338, loss: 5.125, dt: 415.96ms, tok/sec: 9847.04250252053\n",
      "step 339, loss: 5.3125, dt: 452.98ms, tok/sec: 9042.302922612456\n",
      "step 340, loss: 5.3125, dt: 418.11ms, tok/sec: 9796.478705896978\n",
      "step 341, loss: 5.3125, dt: 457.20ms, tok/sec: 8958.972590967118\n",
      "step 342, loss: 5.28125, dt: 430.51ms, tok/sec: 9514.195910858283\n",
      "step 343, loss: 5.1875, dt: 408.64ms, tok/sec: 10023.4187959787\n",
      "step 344, loss: 5.21875, dt: 487.46ms, tok/sec: 8402.738386231535\n",
      "step 345, loss: 4.96875, dt: 377.85ms, tok/sec: 10840.258416565657\n",
      "step 346, loss: 5, dt: 421.55ms, tok/sec: 9716.543522019338\n",
      "step 347, loss: 5.28125, dt: 441.64ms, tok/sec: 9274.47926965349\n",
      "step 348, loss: 5.09375, dt: 495.80ms, tok/sec: 8261.419255312498\n",
      "step 349, loss: 5.34375, dt: 452.44ms, tok/sec: 9053.133648561843\n",
      "step 350, loss: 5.0625, dt: 403.62ms, tok/sec: 10148.135064554563\n",
      "step 351, loss: 5.40625, dt: 424.42ms, tok/sec: 9650.804052919166\n",
      "step 352, loss: 5.46875, dt: 473.00ms, tok/sec: 8659.609812551791\n",
      "step 353, loss: 5.53125, dt: 478.18ms, tok/sec: 8565.770553269227\n",
      "step 354, loss: 5.5, dt: 391.01ms, tok/sec: 10475.561927932707\n",
      "step 355, loss: 5.375, dt: 497.16ms, tok/sec: 8238.836571505302\n",
      "step 356, loss: 5.40625, dt: 406.00ms, tok/sec: 10088.696160570045\n",
      "step 357, loss: 5.4375, dt: 392.66ms, tok/sec: 10431.355742905804\n",
      "step 358, loss: 5.34375, dt: 399.51ms, tok/sec: 10252.476119545498\n",
      "step 359, loss: 5.21875, dt: 393.64ms, tok/sec: 10405.338369280882\n",
      "step 360, loss: 5.125, dt: 415.62ms, tok/sec: 9855.19927903907\n",
      "step 361, loss: 5.21875, dt: 402.47ms, tok/sec: 10177.19543503695\n",
      "step 362, loss: 5.34375, dt: 483.30ms, tok/sec: 8475.138836460148\n",
      "step 363, loss: 5.375, dt: 463.86ms, tok/sec: 8830.22714827236\n",
      "step 364, loss: 5.3125, dt: 399.79ms, tok/sec: 10245.481448261713\n",
      "step 365, loss: 5.15625, dt: 406.84ms, tok/sec: 10067.831909292714\n",
      "step 366, loss: 5.375, dt: 415.59ms, tok/sec: 9855.9625493519\n",
      "step 367, loss: 5.1875, dt: 452.96ms, tok/sec: 9042.716996068651\n",
      "step 368, loss: 5, dt: 439.87ms, tok/sec: 9311.743718048227\n",
      "step 369, loss: 5.1875, dt: 420.04ms, tok/sec: 9751.388325603491\n",
      "step 370, loss: 5.34375, dt: 429.80ms, tok/sec: 9529.939205403372\n",
      "step 371, loss: 5.0625, dt: 385.21ms, tok/sec: 10633.133780448254\n",
      "step 372, loss: 5.09375, dt: 441.62ms, tok/sec: 9275.005012212003\n",
      "step 373, loss: 5.375, dt: 462.23ms, tok/sec: 8861.408264899323\n",
      "step 374, loss: 5.25, dt: 427.74ms, tok/sec: 9575.833702509244\n",
      "step 375, loss: 5.15625, dt: 396.28ms, tok/sec: 10336.012725786142\n",
      "step 376, loss: 5.03125, dt: 450.55ms, tok/sec: 9091.152560544204\n",
      "step 377, loss: 5.125, dt: 428.44ms, tok/sec: 9560.348172946791\n",
      "step 378, loss: 5.0625, dt: 368.39ms, tok/sec: 11118.641718781651\n",
      "step 379, loss: 5.1875, dt: 456.84ms, tok/sec: 8965.878354336506\n",
      "step 380, loss: 5.0625, dt: 394.39ms, tok/sec: 10385.549337477913\n",
      "step 381, loss: 5.46875, dt: 429.05ms, tok/sec: 9546.641748149004\n",
      "step 382, loss: 5.40625, dt: 411.38ms, tok/sec: 9956.643173909919\n",
      "step 383, loss: 5.25, dt: 397.56ms, tok/sec: 10302.943128280414\n",
      "step 384, loss: 5.40625, dt: 392.95ms, tok/sec: 10423.767088312672\n",
      "step 385, loss: 5.71875, dt: 394.44ms, tok/sec: 10384.35660563141\n",
      "step 386, loss: 5.5625, dt: 371.54ms, tok/sec: 11024.29803563224\n",
      "step 387, loss: 5.21875, dt: 381.27ms, tok/sec: 10743.098653412992\n",
      "step 388, loss: 5.34375, dt: 405.81ms, tok/sec: 10093.437972178795\n",
      "step 389, loss: 5.34375, dt: 387.05ms, tok/sec: 10582.699178017387\n",
      "step 390, loss: 5.25, dt: 486.25ms, tok/sec: 8423.622795340201\n",
      "step 391, loss: 5.1875, dt: 519.54ms, tok/sec: 7883.910680761138\n",
      "step 392, loss: 5.09375, dt: 385.69ms, tok/sec: 10619.941771615115\n",
      "step 393, loss: 4.9375, dt: 457.14ms, tok/sec: 8960.00053405765\n",
      "step 394, loss: 5.125, dt: 406.05ms, tok/sec: 10087.428481172828\n",
      "step 395, loss: 5.21875, dt: 485.99ms, tok/sec: 8428.214021358179\n",
      "step 396, loss: 5.15625, dt: 372.98ms, tok/sec: 10981.755434827772\n",
      "step 397, loss: 5.125, dt: 447.27ms, tok/sec: 9157.868995633187\n",
      "step 398, loss: 4.90625, dt: 510.34ms, tok/sec: 8026.014756206778\n",
      "step 399, loss: 4.875, dt: 409.99ms, tok/sec: 9990.509051132838\n",
      "step 400, loss: 5.40625, dt: 421.51ms, tok/sec: 9717.538301420196\n",
      "step 401, loss: 5.15625, dt: 389.23ms, tok/sec: 10523.437296864873\n",
      "step 402, loss: 4.9375, dt: 446.72ms, tok/sec: 9169.144890137544\n",
      "step 403, loss: 4.9375, dt: 466.01ms, tok/sec: 8789.473212878775\n",
      "step 404, loss: 5.125, dt: 451.34ms, tok/sec: 9075.151161580652\n",
      "step 405, loss: 5, dt: 534.16ms, tok/sec: 7668.167215523559\n",
      "step 406, loss: 4.75, dt: 386.30ms, tok/sec: 10603.090339264443\n",
      "step 407, loss: 4.75, dt: 410.99ms, tok/sec: 9966.242827789207\n",
      "step 408, loss: 4.75, dt: 380.21ms, tok/sec: 10772.854197275163\n",
      "step 409, loss: 5.4375, dt: 470.93ms, tok/sec: 8697.593403144117\n",
      "step 410, loss: 5.28125, dt: 382.51ms, tok/sec: 10708.11147740029\n",
      "step 411, loss: 5.3125, dt: 378.55ms, tok/sec: 10820.104123196452\n",
      "step 412, loss: 5.25, dt: 384.45ms, tok/sec: 10654.096546198387\n",
      "step 413, loss: 5.28125, dt: 437.10ms, tok/sec: 9370.885443053165\n",
      "step 414, loss: 5.28125, dt: 436.72ms, tok/sec: 9379.101577862968\n",
      "step 415, loss: 5.03125, dt: 534.19ms, tok/sec: 7667.725718274543\n",
      "step 416, loss: 4.8125, dt: 440.49ms, tok/sec: 9298.715210876224\n",
      "step 417, loss: 4.90625, dt: 434.22ms, tok/sec: 9432.968411943832\n",
      "step 418, loss: 4.875, dt: 410.92ms, tok/sec: 9967.954455732777\n",
      "step 419, loss: 5, dt: 431.81ms, tok/sec: 9485.571763479498\n",
      "step 420, loss: 4.9375, dt: 402.23ms, tok/sec: 10183.119237647243\n",
      "step 421, loss: 5.15625, dt: 465.13ms, tok/sec: 8806.138501417554\n",
      "step 422, loss: 5.125, dt: 430.99ms, tok/sec: 9503.748523809893\n",
      "step 423, loss: 5.09375, dt: 451.44ms, tok/sec: 9073.095049580248\n",
      "step 424, loss: 5.0625, dt: 501.05ms, tok/sec: 8174.860155057056\n",
      "step 425, loss: 5.03125, dt: 546.71ms, tok/sec: 7492.0659742921625\n",
      "step 426, loss: 5.03125, dt: 534.78ms, tok/sec: 7659.251306496325\n",
      "step 427, loss: 4.78125, dt: 358.95ms, tok/sec: 11411.199655668746\n",
      "step 428, loss: 4.8125, dt: 375.83ms, tok/sec: 10898.62940381252\n",
      "step 429, loss: 5.125, dt: 386.23ms, tok/sec: 10604.942622379069\n",
      "step 430, loss: 4.875, dt: 463.27ms, tok/sec: 8841.588298959125\n",
      "step 431, loss: 5.09375, dt: 429.70ms, tok/sec: 9532.265791333895\n",
      "step 432, loss: 4.78125, dt: 430.91ms, tok/sec: 9505.383852786044\n",
      "step 433, loss: 5.21875, dt: 491.47ms, tok/sec: 8334.211804108632\n",
      "step 434, loss: 5.28125, dt: 458.42ms, tok/sec: 8934.967288825277\n",
      "step 435, loss: 5.34375, dt: 372.92ms, tok/sec: 10983.475551031994\n",
      "step 436, loss: 5.3125, dt: 413.84ms, tok/sec: 9897.492299715403\n",
      "step 437, loss: 5.1875, dt: 422.70ms, tok/sec: 9690.050917713468\n",
      "step 438, loss: 5.21875, dt: 463.67ms, tok/sec: 8833.84591146136\n",
      "step 439, loss: 5.25, dt: 463.53ms, tok/sec: 8836.603973621663\n",
      "step 440, loss: 5.15625, dt: 395.73ms, tok/sec: 10350.528425611186\n",
      "step 441, loss: 5, dt: 382.92ms, tok/sec: 10696.757191724955\n",
      "step 442, loss: 4.90625, dt: 413.95ms, tok/sec: 9895.023930819549\n",
      "step 443, loss: 5.03125, dt: 423.04ms, tok/sec: 9682.317903688947\n",
      "step 444, loss: 5.15625, dt: 399.79ms, tok/sec: 10245.389798305025\n",
      "step 445, loss: 5.21875, dt: 441.27ms, tok/sec: 9282.281396826495\n",
      "step 446, loss: 5.15625, dt: 428.48ms, tok/sec: 9559.32148554545\n",
      "step 447, loss: 5, dt: 437.18ms, tok/sec: 9369.224526613327\n",
      "step 448, loss: 5.21875, dt: 360.89ms, tok/sec: 11349.75152178997\n",
      "step 449, loss: 5.03125, dt: 460.83ms, tok/sec: 8888.273951312789\n",
      "step 450, loss: 4.84375, dt: 427.16ms, tok/sec: 9589.013956084438\n",
      "step 451, loss: 5, dt: 408.77ms, tok/sec: 10020.402104650424\n",
      "step 452, loss: 5.1875, dt: 426.52ms, tok/sec: 9603.271836150612\n",
      "step 453, loss: 4.90625, dt: 408.31ms, tok/sec: 10031.565913670653\n",
      "step 454, loss: 4.96875, dt: 429.86ms, tok/sec: 9528.617787480254\n",
      "step 455, loss: 5.25, dt: 437.52ms, tok/sec: 9361.80614701526\n",
      "step 456, loss: 5.09375, dt: 398.98ms, tok/sec: 10266.1260579927\n",
      "step 457, loss: 5, dt: 408.93ms, tok/sec: 10016.470271612827\n",
      "step 458, loss: 4.84375, dt: 418.07ms, tok/sec: 9797.30554025309\n",
      "step 459, loss: 4.96875, dt: 455.01ms, tok/sec: 9001.935160422203\n",
      "step 460, loss: 4.90625, dt: 429.91ms, tok/sec: 9527.534498200679\n",
      "step 461, loss: 5.03125, dt: 495.63ms, tok/sec: 8264.165326896407\n",
      "step 462, loss: 4.875, dt: 367.29ms, tok/sec: 11151.86329970439\n",
      "step 463, loss: 5.28125, dt: 418.93ms, tok/sec: 9777.177222076783\n",
      "step 464, loss: 5.25, dt: 426.72ms, tok/sec: 9598.796947581677\n",
      "step 465, loss: 5.09375, dt: 408.85ms, tok/sec: 10018.391985691902\n",
      "step 466, loss: 5.25, dt: 421.50ms, tok/sec: 9717.697704897633\n",
      "step 467, loss: 5.5625, dt: 479.49ms, tok/sec: 8542.4896047353\n",
      "step 468, loss: 5.40625, dt: 464.31ms, tok/sec: 8821.779786570869\n",
      "step 469, loss: 5.0625, dt: 359.85ms, tok/sec: 11382.462383259304\n",
      "step 470, loss: 5.15625, dt: 454.51ms, tok/sec: 9011.936544572705\n",
      "step 471, loss: 5.1875, dt: 433.53ms, tok/sec: 9448.048910248901\n",
      "step 472, loss: 5.09375, dt: 395.07ms, tok/sec: 10367.718362829455\n",
      "step 473, loss: 5.03125, dt: 385.51ms, tok/sec: 10624.834833891378\n",
      "step 474, loss: 4.90625, dt: 379.18ms, tok/sec: 10802.163708729197\n",
      "step 475, loss: 4.78125, dt: 470.95ms, tok/sec: 8697.333616159141\n",
      "step 476, loss: 4.96875, dt: 395.87ms, tok/sec: 10346.769491131401\n",
      "step 477, loss: 5.03125, dt: 413.38ms, tok/sec: 9908.4694896984\n",
      "step 478, loss: 4.96875, dt: 465.52ms, tok/sec: 8798.832063434018\n",
      "step 479, loss: 4.9375, dt: 392.47ms, tok/sec: 10436.418874146035\n",
      "step 480, loss: 4.75, dt: 427.31ms, tok/sec: 9585.584456728122\n",
      "step 481, loss: 4.6875, dt: 404.19ms, tok/sec: 10133.810485683376\n",
      "step 482, loss: 5.28125, dt: 377.61ms, tok/sec: 10847.198670039974\n",
      "step 483, loss: 5, dt: 403.64ms, tok/sec: 10147.763419456092\n",
      "step 484, loss: 4.75, dt: 443.41ms, tok/sec: 9237.526788762823\n",
      "step 485, loss: 4.78125, dt: 457.48ms, tok/sec: 8953.351113705297\n",
      "step 486, loss: 4.96875, dt: 410.97ms, tok/sec: 9966.75741087429\n",
      "step 487, loss: 4.84375, dt: 436.12ms, tok/sec: 9391.925170168439\n",
      "step 488, loss: 4.5625, dt: 378.23ms, tok/sec: 10829.332197022226\n",
      "step 489, loss: 4.59375, dt: 388.45ms, tok/sec: 10544.519437539511\n",
      "step 490, loss: 4.625, dt: 488.25ms, tok/sec: 8389.062680643336\n",
      "step 491, loss: 5.28125, dt: 443.33ms, tok/sec: 9239.260586778184\n",
      "step 492, loss: 5.125, dt: 444.86ms, tok/sec: 9207.322387362185\n",
      "step 493, loss: 5.1875, dt: 427.29ms, tok/sec: 9586.001643805348\n",
      "step 494, loss: 5.09375, dt: 432.54ms, tok/sec: 9469.744917965918\n",
      "step 495, loss: 5.125, dt: 382.36ms, tok/sec: 10712.531689818092\n",
      "step 496, loss: 5.09375, dt: 422.20ms, tok/sec: 9701.646233948115\n",
      "step 497, loss: 4.84375, dt: 421.71ms, tok/sec: 9712.840995963883\n",
      "step 498, loss: 4.625, dt: 399.27ms, tok/sec: 10258.800322694873\n",
      "step 499, loss: 4.75, dt: 422.80ms, tok/sec: 9687.761401815087\n",
      "step 500, loss: 4.71875, dt: 444.55ms, tok/sec: 9213.860360147766\n",
      "step 501, loss: 4.84375, dt: 450.25ms, tok/sec: 9097.208588719905\n",
      "step 502, loss: 4.78125, dt: 507.14ms, tok/sec: 8076.708243382503\n",
      "step 503, loss: 5.03125, dt: 468.92ms, tok/sec: 8735.027770786652\n",
      "step 504, loss: 5, dt: 413.58ms, tok/sec: 9903.745687398972\n",
      "step 505, loss: 4.9375, dt: 421.96ms, tok/sec: 9707.078584428213\n",
      "step 506, loss: 4.9375, dt: 414.77ms, tok/sec: 9875.389333003768\n",
      "step 507, loss: 4.90625, dt: 421.35ms, tok/sec: 9721.112393437577\n",
      "step 508, loss: 4.90625, dt: 409.68ms, tok/sec: 9998.143045534382\n",
      "step 509, loss: 4.625, dt: 413.93ms, tok/sec: 9895.348795845535\n",
      "step 510, loss: 4.6875, dt: 483.18ms, tok/sec: 8477.234004526817\n",
      "step 511, loss: 5, dt: 413.23ms, tok/sec: 9912.04818284464\n",
      "step 512, loss: 4.6875, dt: 428.90ms, tok/sec: 9550.027507401606\n",
      "step 513, loss: 4.9375, dt: 361.58ms, tok/sec: 11328.197921873083\n",
      "step 514, loss: 4.59375, dt: 405.09ms, tok/sec: 10111.247632696422\n",
      "step 515, loss: 5.0625, dt: 413.37ms, tok/sec: 9908.84667413777\n",
      "step 516, loss: 5.125, dt: 405.30ms, tok/sec: 10106.025357022476\n",
      "step 517, loss: 5.21875, dt: 459.59ms, tok/sec: 8912.380045869537\n",
      "step 518, loss: 5.1875, dt: 418.64ms, tok/sec: 9783.97033122238\n",
      "step 519, loss: 5.03125, dt: 462.14ms, tok/sec: 8863.118048035576\n",
      "step 520, loss: 5.0625, dt: 407.19ms, tok/sec: 10059.195822649992\n",
      "step 521, loss: 5.09375, dt: 427.38ms, tok/sec: 9583.878646420244\n",
      "step 522, loss: 5.03125, dt: 378.58ms, tok/sec: 10819.313682505515\n",
      "step 523, loss: 4.875, dt: 444.41ms, tok/sec: 9216.707573852249\n",
      "step 524, loss: 4.75, dt: 449.76ms, tok/sec: 9107.142897885251\n",
      "step 525, loss: 4.9375, dt: 386.45ms, tok/sec: 10599.126141666606\n",
      "step 526, loss: 5.0625, dt: 405.23ms, tok/sec: 10107.71992342079\n",
      "step 527, loss: 5.125, dt: 410.82ms, tok/sec: 9970.268400740522\n",
      "step 528, loss: 5.0625, dt: 409.02ms, tok/sec: 10014.251579967147\n",
      "step 529, loss: 4.90625, dt: 428.22ms, tok/sec: 9565.277213643596\n",
      "step 530, loss: 5.09375, dt: 405.44ms, tok/sec: 10102.63198736397\n",
      "step 531, loss: 4.9375, dt: 363.64ms, tok/sec: 11263.840883595383\n",
      "step 532, loss: 4.71875, dt: 427.04ms, tok/sec: 9591.572965180874\n",
      "step 533, loss: 4.90625, dt: 448.55ms, tok/sec: 9131.641849814709\n",
      "step 534, loss: 5.0625, dt: 383.13ms, tok/sec: 10690.886157997316\n",
      "step 535, loss: 4.8125, dt: 415.09ms, tok/sec: 9867.709188699262\n",
      "step 536, loss: 4.84375, dt: 470.23ms, tok/sec: 8710.637901399696\n",
      "step 537, loss: 5.15625, dt: 415.86ms, tok/sec: 9849.554750869725\n",
      "step 538, loss: 4.96875, dt: 468.61ms, tok/sec: 8740.831944269903\n",
      "step 539, loss: 4.875, dt: 379.96ms, tok/sec: 10780.100701213869\n",
      "step 540, loss: 4.75, dt: 405.10ms, tok/sec: 10111.116712484853\n",
      "step 541, loss: 4.84375, dt: 399.67ms, tok/sec: 10248.4273400058\n",
      "step 542, loss: 4.78125, dt: 399.77ms, tok/sec: 10245.95805446979\n",
      "step 543, loss: 4.9375, dt: 411.91ms, tok/sec: 9943.866609943363\n",
      "step 544, loss: 4.75, dt: 478.64ms, tok/sec: 8557.497398112455\n",
      "step 545, loss: 5.15625, dt: 428.80ms, tok/sec: 9552.178021038304\n",
      "step 546, loss: 5.15625, dt: 416.59ms, tok/sec: 9832.265979127738\n",
      "step 547, loss: 5, dt: 429.69ms, tok/sec: 9532.551405146507\n",
      "step 548, loss: 5.15625, dt: 443.52ms, tok/sec: 9235.20779590788\n",
      "step 549, loss: 5.46875, dt: 438.08ms, tok/sec: 9349.899062771608\n",
      "step 550, loss: 5.3125, dt: 426.10ms, tok/sec: 9612.745088538086\n",
      "step 551, loss: 4.96875, dt: 429.74ms, tok/sec: 9531.303293040432\n",
      "step 552, loss: 5.0625, dt: 417.90ms, tok/sec: 9801.39707142523\n",
      "step 553, loss: 5.09375, dt: 394.32ms, tok/sec: 10387.451997876542\n",
      "step 554, loss: 5, dt: 468.69ms, tok/sec: 8739.19123917385\n",
      "step 555, loss: 4.9375, dt: 375.45ms, tok/sec: 10909.695976590332\n",
      "step 556, loss: 4.8125, dt: 382.32ms, tok/sec: 10713.660697403508\n",
      "step 557, loss: 4.65625, dt: 400.49ms, tok/sec: 10227.512804729218\n",
      "step 558, loss: 4.84375, dt: 415.30ms, tok/sec: 9862.752374139873\n",
      "step 559, loss: 4.90625, dt: 426.79ms, tok/sec: 9597.140043896916\n",
      "step 560, loss: 4.84375, dt: 386.11ms, tok/sec: 10608.374000517455\n",
      "step 561, loss: 4.78125, dt: 407.17ms, tok/sec: 10059.796626604793\n",
      "step 562, loss: 4.625, dt: 423.61ms, tok/sec: 9669.228366062438\n",
      "step 563, loss: 4.59375, dt: 441.06ms, tok/sec: 9286.656726228572\n",
      "step 564, loss: 5.15625, dt: 470.97ms, tok/sec: 8696.880126920745\n",
      "step 565, loss: 4.875, dt: 496.21ms, tok/sec: 8254.579901626568\n",
      "step 566, loss: 4.65625, dt: 459.03ms, tok/sec: 8923.114774567579\n",
      "step 567, loss: 4.65625, dt: 478.24ms, tok/sec: 8564.809722678523\n",
      "step 568, loss: 4.875, dt: 357.69ms, tok/sec: 11451.375731297221\n",
      "step 569, loss: 4.71875, dt: 432.97ms, tok/sec: 9460.24913133547\n",
      "step 570, loss: 4.46875, dt: 589.05ms, tok/sec: 6953.574291492978\n",
      "step 571, loss: 4.46875, dt: 381.69ms, tok/sec: 10731.200850507237\n",
      "step 572, loss: 4.5, dt: 442.96ms, tok/sec: 9246.923912522789\n",
      "step 573, loss: 5.15625, dt: 418.52ms, tok/sec: 9786.76827057715\n",
      "step 574, loss: 5, dt: 452.64ms, tok/sec: 9049.113772139684\n",
      "step 575, loss: 5.0625, dt: 487.46ms, tok/sec: 8402.684959030485\n",
      "step 576, loss: 4.96875, dt: 484.83ms, tok/sec: 8448.261471257743\n",
      "step 577, loss: 5.03125, dt: 430.84ms, tok/sec: 9506.930308357807\n",
      "step 578, loss: 5, dt: 428.22ms, tok/sec: 9565.234608373597\n",
      "step 579, loss: 4.75, dt: 404.33ms, tok/sec: 10130.31480517275\n",
      "step 580, loss: 4.5, dt: 393.50ms, tok/sec: 10409.158908108482\n",
      "step 581, loss: 4.625, dt: 411.64ms, tok/sec: 9950.328591542851\n",
      "step 582, loss: 4.59375, dt: 445.26ms, tok/sec: 9199.123551381352\n",
      "step 583, loss: 4.71875, dt: 388.34ms, tok/sec: 10547.536219213607\n",
      "step 584, loss: 4.71875, dt: 409.84ms, tok/sec: 9994.147261798042\n",
      "step 585, loss: 4.96875, dt: 427.97ms, tok/sec: 9570.675174493455\n",
      "step 586, loss: 4.875, dt: 411.77ms, tok/sec: 9947.361488016175\n",
      "step 587, loss: 4.8125, dt: 451.39ms, tok/sec: 9074.278757839038\n",
      "step 588, loss: 4.78125, dt: 461.99ms, tok/sec: 8866.02710919385\n",
      "step 589, loss: 4.78125, dt: 439.05ms, tok/sec: 9329.183779260491\n",
      "step 590, loss: 4.78125, dt: 454.31ms, tok/sec: 9015.918719666817\n",
      "step 591, loss: 4.5, dt: 469.53ms, tok/sec: 8723.52667957095\n",
      "step 592, loss: 4.5625, dt: 444.67ms, tok/sec: 9211.311224588277\n",
      "step 593, loss: 4.875, dt: 563.05ms, tok/sec: 7274.69824762279\n",
      "step 594, loss: 4.5625, dt: 468.38ms, tok/sec: 8744.965337503258\n",
      "step 595, loss: 4.8125, dt: 472.53ms, tok/sec: 8668.22163570285\n",
      "step 596, loss: 4.46875, dt: 396.60ms, tok/sec: 10327.711471120962\n",
      "step 597, loss: 4.96875, dt: 490.27ms, tok/sec: 8354.53715317222\n",
      "step 598, loss: 5, dt: 423.49ms, tok/sec: 9671.977390595479\n",
      "step 599, loss: 5.09375, dt: 451.73ms, tok/sec: 9067.267276891012\n",
      "step 600, loss: 5.0625, dt: 450.39ms, tok/sec: 9094.372130989601\n",
      "step 601, loss: 4.9375, dt: 484.11ms, tok/sec: 8460.889111274837\n",
      "step 602, loss: 4.96875, dt: 460.34ms, tok/sec: 8897.82606741061\n",
      "step 603, loss: 5, dt: 423.09ms, tok/sec: 9681.18847580417\n",
      "step 604, loss: 4.90625, dt: 413.51ms, tok/sec: 9905.401646915292\n",
      "step 605, loss: 4.75, dt: 426.36ms, tok/sec: 9606.971865002564\n",
      "step 606, loss: 4.625, dt: 410.25ms, tok/sec: 9984.058616982327\n",
      "step 607, loss: 4.84375, dt: 363.59ms, tok/sec: 11265.443666451149\n",
      "step 608, loss: 4.96875, dt: 437.19ms, tok/sec: 9369.009928063126\n",
      "step 609, loss: 5.03125, dt: 394.80ms, tok/sec: 10374.943646355456\n",
      "step 610, loss: 4.96875, dt: 399.28ms, tok/sec: 10258.592044693885\n",
      "step 611, loss: 4.8125, dt: 389.74ms, tok/sec: 10509.487168601476\n",
      "step 612, loss: 5, dt: 427.16ms, tok/sec: 9589.013956084438\n",
      "step 613, loss: 4.84375, dt: 410.10ms, tok/sec: 9987.78503425406\n",
      "step 614, loss: 4.65625, dt: 438.85ms, tok/sec: 9333.471607395773\n",
      "step 615, loss: 4.78125, dt: 426.27ms, tok/sec: 9608.895502186067\n",
      "step 616, loss: 5, dt: 394.54ms, tok/sec: 10381.77120166593\n",
      "step 617, loss: 4.6875, dt: 444.62ms, tok/sec: 9212.259575353426\n",
      "step 618, loss: 4.75, dt: 414.18ms, tok/sec: 9889.470330211814\n",
      "step 619, loss: 5.0625, dt: 413.52ms, tok/sec: 9905.13322966881\n",
      "step 620, loss: 4.90625, dt: 411.17ms, tok/sec: 9961.700613246496\n",
      "step 621, loss: 4.78125, dt: 429.90ms, tok/sec: 9527.904374162212\n",
      "step 622, loss: 4.625, dt: 399.18ms, tok/sec: 10261.030652524096\n",
      "step 623, loss: 4.75, dt: 434.28ms, tok/sec: 9431.663391362112\n",
      "step 624, loss: 4.6875, dt: 469.31ms, tok/sec: 8727.767866024187\n",
      "step 625, loss: 4.8125, dt: 402.08ms, tok/sec: 10187.086366752133\n",
      "step 626, loss: 4.65625, dt: 426.46ms, tok/sec: 9604.646260085514\n",
      "step 627, loss: 5.0625, dt: 418.69ms, tok/sec: 9782.917338274561\n",
      "step 628, loss: 5.03125, dt: 470.73ms, tok/sec: 8701.430370046592\n",
      "step 629, loss: 4.90625, dt: 428.41ms, tok/sec: 9560.954713675608\n",
      "step 630, loss: 5.0625, dt: 425.90ms, tok/sec: 9617.276064388097\n",
      "step 631, loss: 5.375, dt: 478.49ms, tok/sec: 8560.294572044297\n",
      "step 632, loss: 5.1875, dt: 434.31ms, tok/sec: 9430.990307634878\n",
      "step 633, loss: 4.875, dt: 445.09ms, tok/sec: 9202.632028510165\n",
      "step 634, loss: 4.96875, dt: 401.25ms, tok/sec: 10208.029813853555\n",
      "step 635, loss: 5.03125, dt: 387.86ms, tok/sec: 10560.633239569604\n",
      "step 636, loss: 4.90625, dt: 528.60ms, tok/sec: 7748.707757381512\n",
      "step 637, loss: 4.8125, dt: 339.55ms, tok/sec: 12062.95617981946\n",
      "step 638, loss: 4.6875, dt: 421.81ms, tok/sec: 9710.551676580775\n",
      "step 639, loss: 4.5625, dt: 417.71ms, tok/sec: 9805.917376240732\n",
      "step 640, loss: 4.71875, dt: 446.29ms, tok/sec: 9177.859065902658\n",
      "step 641, loss: 4.78125, dt: 433.87ms, tok/sec: 9440.697179364171\n",
      "step 642, loss: 4.6875, dt: 428.31ms, tok/sec: 9563.184678303262\n",
      "step 643, loss: 4.6875, dt: 445.77ms, tok/sec: 9188.692785644564\n",
      "step 644, loss: 4.46875, dt: 479.99ms, tok/sec: 8533.5830761566\n",
      "step 645, loss: 4.4375, dt: 448.06ms, tok/sec: 9141.56398771041\n",
      "step 646, loss: 5.0625, dt: 388.52ms, tok/sec: 10542.617033758723\n",
      "step 647, loss: 4.78125, dt: 467.50ms, tok/sec: 8761.421902905571\n",
      "step 648, loss: 4.5625, dt: 439.71ms, tok/sec: 9315.323489813523\n",
      "step 649, loss: 4.5625, dt: 478.23ms, tok/sec: 8565.001871551913\n",
      "step 650, loss: 4.75, dt: 428.48ms, tok/sec: 9559.284252322512\n",
      "step 651, loss: 4.625, dt: 408.63ms, tok/sec: 10023.61763399981\n",
      "step 652, loss: 4.34375, dt: 415.55ms, tok/sec: 9856.827731977159\n",
      "step 653, loss: 4.34375, dt: 356.71ms, tok/sec: 11482.710758472907\n",
      "step 654, loss: 4.375, dt: 392.39ms, tok/sec: 10438.473409639682\n",
      "step 655, loss: 5.0625, dt: 502.52ms, tok/sec: 8150.9063962329055\n",
      "step 656, loss: 4.90625, dt: 340.52ms, tok/sec: 12028.496961346082\n",
      "step 657, loss: 4.9375, dt: 439.48ms, tok/sec: 9320.109229473903\n",
      "step 658, loss: 4.875, dt: 469.31ms, tok/sec: 8727.76343212004\n",
      "step 659, loss: 4.9375, dt: 486.26ms, tok/sec: 8423.453457858877\n",
      "step 660, loss: 4.875, dt: 443.70ms, tok/sec: 9231.525622783449\n",
      "step 661, loss: 4.625, dt: 470.36ms, tok/sec: 8708.315431982368\n",
      "step 662, loss: 4.40625, dt: 411.55ms, tok/sec: 9952.553639632551\n",
      "step 663, loss: 4.53125, dt: 433.10ms, tok/sec: 9457.390050948086\n",
      "step 664, loss: 4.5, dt: 393.78ms, tok/sec: 10401.778841240279\n",
      "step 665, loss: 4.59375, dt: 385.45ms, tok/sec: 10626.405509938362\n",
      "step 666, loss: 4.59375, dt: 419.54ms, tok/sec: 9763.020026277385\n",
      "step 667, loss: 4.84375, dt: 451.09ms, tok/sec: 9080.201935080819\n",
      "step 668, loss: 4.78125, dt: 378.72ms, tok/sec: 10815.431297880525\n",
      "step 669, loss: 4.71875, dt: 490.26ms, tok/sec: 8354.75248687083\n",
      "step 670, loss: 4.71875, dt: 416.16ms, tok/sec: 9842.472933852692\n",
      "step 671, loss: 4.6875, dt: 499.43ms, tok/sec: 8201.362249959184\n",
      "step 672, loss: 4.6875, dt: 491.64ms, tok/sec: 8331.31798477559\n",
      "step 673, loss: 4.375, dt: 483.64ms, tok/sec: 8469.122558700143\n",
      "step 674, loss: 4.46875, dt: 428.20ms, tok/sec: 9565.554157150224\n",
      "step 675, loss: 4.8125, dt: 421.07ms, tok/sec: 9727.706612852448\n",
      "step 676, loss: 4.46875, dt: 439.42ms, tok/sec: 9321.343099657744\n",
      "step 677, loss: 4.6875, dt: 455.73ms, tok/sec: 8987.693976629726\n",
      "step 678, loss: 4.34375, dt: 412.67ms, tok/sec: 9925.654768308148\n",
      "step 679, loss: 4.875, dt: 417.59ms, tok/sec: 9808.660681701398\n",
      "step 680, loss: 4.9375, dt: 397.39ms, tok/sec: 10307.20825158899\n",
      "step 681, loss: 5, dt: 409.49ms, tok/sec: 10002.788449311443\n",
      "step 682, loss: 4.9375, dt: 444.69ms, tok/sec: 9210.856875705027\n",
      "step 683, loss: 4.8125, dt: 411.69ms, tok/sec: 9949.187631640974\n",
      "step 684, loss: 4.875, dt: 370.77ms, tok/sec: 11047.166993540122\n",
      "step 685, loss: 4.875, dt: 397.37ms, tok/sec: 10307.684433658025\n",
      "step 686, loss: 4.8125, dt: 365.28ms, tok/sec: 11213.223339346834\n",
      "step 687, loss: 4.65625, dt: 388.06ms, tok/sec: 10554.943064925681\n",
      "step 688, loss: 4.5, dt: 427.43ms, tok/sec: 9582.78275643761\n",
      "step 689, loss: 4.71875, dt: 399.07ms, tok/sec: 10263.87511672409\n",
      "step 690, loss: 4.875, dt: 438.21ms, tok/sec: 9347.045257889009\n",
      "step 691, loss: 4.9375, dt: 428.41ms, tok/sec: 9560.890863658877\n",
      "step 692, loss: 4.875, dt: 422.42ms, tok/sec: 9696.559266577828\n",
      "step 693, loss: 4.71875, dt: 393.73ms, tok/sec: 10403.09526776669\n",
      "step 694, loss: 4.90625, dt: 431.30ms, tok/sec: 9496.819085324429\n",
      "step 695, loss: 4.71875, dt: 390.64ms, tok/sec: 10485.28642138966\n",
      "step 696, loss: 4.5625, dt: 414.81ms, tok/sec: 9874.51521336415\n",
      "step 697, loss: 4.71875, dt: 420.64ms, tok/sec: 9737.498828997335\n",
      "step 698, loss: 4.90625, dt: 482.25ms, tok/sec: 8493.54978899449\n",
      "step 699, loss: 4.625, dt: 434.12ms, tok/sec: 9435.19088503094\n",
      "step 700, loss: 4.65625, dt: 435.28ms, tok/sec: 9409.940315985792\n",
      "step 701, loss: 4.96875, dt: 443.99ms, tok/sec: 9225.383629061576\n",
      "step 702, loss: 4.78125, dt: 438.68ms, tok/sec: 9337.134078027078\n",
      "step 703, loss: 4.6875, dt: 366.99ms, tok/sec: 11161.144100043073\n",
      "step 704, loss: 4.5, dt: 497.27ms, tok/sec: 8237.039238427662\n",
      "step 705, loss: 4.6875, dt: 481.71ms, tok/sec: 8502.999920809641\n",
      "step 706, loss: 4.59375, dt: 448.06ms, tok/sec: 9141.607766700989\n",
      "step 707, loss: 4.75, dt: 427.05ms, tok/sec: 9591.41767257286\n",
      "step 708, loss: 4.5625, dt: 367.66ms, tok/sec: 11140.589200290255\n",
      "step 709, loss: 5, dt: 417.73ms, tok/sec: 9805.492021419284\n",
      "step 710, loss: 4.9375, dt: 456.51ms, tok/sec: 8972.335532390482\n",
      "step 711, loss: 4.8125, dt: 402.38ms, tok/sec: 10179.348199165974\n",
      "step 712, loss: 5, dt: 385.60ms, tok/sec: 10622.430430588875\n",
      "step 713, loss: 5.28125, dt: 495.90ms, tok/sec: 8259.719275100748\n",
      "step 714, loss: 5.09375, dt: 412.97ms, tok/sec: 9918.417327225974\n",
      "step 715, loss: 4.78125, dt: 441.01ms, tok/sec: 9287.721076475613\n",
      "step 716, loss: 4.84375, dt: 402.59ms, tok/sec: 10174.019208789283\n",
      "step 717, loss: 4.9375, dt: 366.59ms, tok/sec: 11173.382682165571\n",
      "step 718, loss: 4.8125, dt: 395.80ms, tok/sec: 10348.776409911046\n",
      "step 719, loss: 4.71875, dt: 446.45ms, tok/sec: 9174.560525058008\n",
      "step 720, loss: 4.5625, dt: 447.72ms, tok/sec: 9148.651912977446\n",
      "step 721, loss: 4.46875, dt: 490.62ms, tok/sec: 8348.589395461308\n",
      "step 722, loss: 4.625, dt: 490.46ms, tok/sec: 8351.389673073174\n",
      "step 723, loss: 4.65625, dt: 398.99ms, tok/sec: 10265.862272668875\n",
      "step 724, loss: 4.59375, dt: 388.29ms, tok/sec: 10548.779687242879\n",
      "step 725, loss: 4.5625, dt: 346.32ms, tok/sec: 11827.279907308668\n",
      "step 726, loss: 4.375, dt: 352.18ms, tok/sec: 11630.479117105871\n",
      "step 727, loss: 4.3125, dt: 411.82ms, tok/sec: 9946.16362547951\n",
      "step 728, loss: 4.96875, dt: 415.76ms, tok/sec: 9851.774499307561\n",
      "step 729, loss: 4.6875, dt: 434.09ms, tok/sec: 9435.82310827607\n",
      "step 730, loss: 4.4375, dt: 455.09ms, tok/sec: 9000.492036232758\n",
      "step 731, loss: 4.4375, dt: 408.64ms, tok/sec: 10023.46558068572\n",
      "step 732, loss: 4.625, dt: 410.14ms, tok/sec: 9986.873488369389\n",
      "step 733, loss: 4.53125, dt: 420.21ms, tok/sec: 9747.620486726568\n",
      "step 734, loss: 4.25, dt: 458.65ms, tok/sec: 8930.601309554257\n",
      "step 735, loss: 4.25, dt: 380.64ms, tok/sec: 10760.83651045588\n",
      "step 736, loss: 4.28125, dt: 433.59ms, tok/sec: 9446.776075609976\n",
      "step 737, loss: 4.96875, dt: 464.44ms, tok/sec: 8819.27091281689\n",
      "step 738, loss: 4.8125, dt: 378.15ms, tok/sec: 10831.708260406842\n",
      "step 739, loss: 4.84375, dt: 399.21ms, tok/sec: 10260.338166120799\n",
      "step 740, loss: 4.75, dt: 475.45ms, tok/sec: 8615.030174899483\n",
      "step 741, loss: 4.8125, dt: 417.04ms, tok/sec: 9821.608494441145\n",
      "step 742, loss: 4.75, dt: 430.79ms, tok/sec: 9508.029965465332\n",
      "step 743, loss: 4.5625, dt: 436.02ms, tok/sec: 9394.010197882668\n",
      "step 744, loss: 4.28125, dt: 365.78ms, tok/sec: 11198.12301864065\n",
      "step 745, loss: 4.4375, dt: 436.07ms, tok/sec: 9393.049737615896\n",
      "step 746, loss: 4.40625, dt: 436.09ms, tok/sec: 9392.531068028577\n",
      "step 747, loss: 4.5, dt: 398.82ms, tok/sec: 10270.323908635268\n",
      "step 748, loss: 4.53125, dt: 396.49ms, tok/sec: 10330.779392884697\n",
      "step 749, loss: 4.75, dt: 454.97ms, tok/sec: 9002.8078619531\n",
      "step 750, loss: 4.65625, dt: 417.68ms, tok/sec: 9806.617053254675\n",
      "step 751, loss: 4.5625, dt: 396.46ms, tok/sec: 10331.524912214445\n",
      "step 752, loss: 4.59375, dt: 373.37ms, tok/sec: 10970.346113862588\n",
      "step 753, loss: 4.59375, dt: 415.14ms, tok/sec: 9866.558763902593\n",
      "step 754, loss: 4.59375, dt: 368.96ms, tok/sec: 11101.549048800662\n",
      "step 755, loss: 4.28125, dt: 410.76ms, tok/sec: 9971.761465291273\n",
      "step 756, loss: 4.375, dt: 396.30ms, tok/sec: 10335.658282968174\n",
      "step 757, loss: 4.71875, dt: 458.93ms, tok/sec: 8925.057098461584\n",
      "step 758, loss: 4.375, dt: 442.58ms, tok/sec: 9254.9140487756\n",
      "step 759, loss: 4.625, dt: 427.65ms, tok/sec: 9577.921098068513\n",
      "step 760, loss: 4.25, dt: 413.89ms, tok/sec: 9896.249410425587\n",
      "step 761, loss: 4.78125, dt: 470.62ms, tok/sec: 8703.444920655185\n",
      "step 762, loss: 4.84375, dt: 353.46ms, tok/sec: 11588.178877516468\n",
      "step 763, loss: 4.90625, dt: 388.49ms, tok/sec: 10543.399912730914\n",
      "step 764, loss: 4.84375, dt: 391.93ms, tok/sec: 10450.728626941429\n",
      "step 765, loss: 4.71875, dt: 451.72ms, tok/sec: 9067.56877734674\n",
      "step 766, loss: 4.8125, dt: 434.18ms, tok/sec: 9433.931873094529\n",
      "step 767, loss: 4.78125, dt: 391.67ms, tok/sec: 10457.739080466767\n",
      "step 768, loss: 4.71875, dt: 399.90ms, tok/sec: 10242.463971649979\n",
      "step 769, loss: 4.59375, dt: 380.73ms, tok/sec: 10758.296058288115\n",
      "step 770, loss: 4.4375, dt: 459.42ms, tok/sec: 8915.677795952937\n",
      "step 771, loss: 4.625, dt: 445.31ms, tok/sec: 9198.06463536731\n",
      "step 772, loss: 4.78125, dt: 386.37ms, tok/sec: 10601.140577871462\n",
      "step 773, loss: 4.84375, dt: 431.00ms, tok/sec: 9503.517204373642\n",
      "step 774, loss: 4.78125, dt: 481.26ms, tok/sec: 8511.058358661108\n",
      "step 775, loss: 4.625, dt: 407.07ms, tok/sec: 10062.235920815809\n",
      "step 776, loss: 4.8125, dt: 466.08ms, tok/sec: 8788.115381979009\n",
      "step 777, loss: 4.65625, dt: 382.81ms, tok/sec: 10699.921701885953\n",
      "step 778, loss: 4.4375, dt: 399.24ms, tok/sec: 10259.535489126183\n",
      "step 779, loss: 4.625, dt: 432.15ms, tok/sec: 9478.198148813257\n",
      "step 780, loss: 4.84375, dt: 402.00ms, tok/sec: 10189.049940365729\n",
      "step 781, loss: 4.5625, dt: 413.09ms, tok/sec: 9915.457776161742\n",
      "step 782, loss: 4.59375, dt: 365.24ms, tok/sec: 11214.496958746264\n",
      "step 783, loss: 4.90625, dt: 350.08ms, tok/sec: 11700.222213292998\n",
      "step 784, loss: 4.75, dt: 381.17ms, tok/sec: 10745.732750715866\n",
      "step 785, loss: 4.625, dt: 406.38ms, tok/sec: 10079.2495442889\n",
      "step 786, loss: 4.4375, dt: 455.49ms, tok/sec: 8992.53958160818\n",
      "step 787, loss: 4.59375, dt: 365.99ms, tok/sec: 11191.608694604209\n",
      "step 788, loss: 4.53125, dt: 395.49ms, tok/sec: 10356.730710894302\n",
      "step 789, loss: 4.6875, dt: 522.38ms, tok/sec: 7841.029977791186\n",
      "step 790, loss: 4.5, dt: 467.75ms, tok/sec: 8756.84888866688\n",
      "step 791, loss: 4.9375, dt: 481.62ms, tok/sec: 8504.649962179266\n",
      "step 792, loss: 4.90625, dt: 468.97ms, tok/sec: 8734.108521362125\n",
      "step 793, loss: 4.75, dt: 432.81ms, tok/sec: 9463.818897043553\n",
      "step 794, loss: 4.9375, dt: 417.59ms, tok/sec: 9808.699882899904\n",
      "step 795, loss: 5.21875, dt: 360.57ms, tok/sec: 11359.635155892012\n",
      "step 796, loss: 5, dt: 465.56ms, tok/sec: 8798.093075110579\n",
      "step 797, loss: 4.71875, dt: 384.22ms, tok/sec: 10660.588709742475\n",
      "step 798, loss: 4.75, dt: 396.40ms, tok/sec: 10332.99762904617\n",
      "step 799, loss: 4.875, dt: 465.23ms, tok/sec: 8804.337826757192\n",
      "step 800, loss: 4.75, dt: 388.91ms, tok/sec: 10531.9142629443\n",
      "step 801, loss: 4.59375, dt: 412.83ms, tok/sec: 9921.774003599125\n",
      "step 802, loss: 4.46875, dt: 398.25ms, tok/sec: 10285.123505637404\n",
      "step 803, loss: 4.40625, dt: 429.04ms, tok/sec: 9546.795594214753\n",
      "step 804, loss: 4.5625, dt: 362.57ms, tok/sec: 11297.253644345543\n",
      "step 805, loss: 4.5625, dt: 429.58ms, tok/sec: 9534.789305636725\n",
      "step 806, loss: 4.5, dt: 445.91ms, tok/sec: 9185.725329750267\n",
      "step 807, loss: 4.46875, dt: 488.89ms, tok/sec: 8378.0903875067\n",
      "step 808, loss: 4.28125, dt: 479.53ms, tok/sec: 8541.776058581716\n",
      "step 809, loss: 4.25, dt: 439.88ms, tok/sec: 9311.642777080393\n",
      "step 810, loss: 4.875, dt: 376.09ms, tok/sec: 10891.02940283359\n",
      "step 811, loss: 4.59375, dt: 411.10ms, tok/sec: 9963.41645948054\n",
      "step 812, loss: 4.34375, dt: 414.93ms, tok/sec: 9871.451340786585\n",
      "step 813, loss: 4.34375, dt: 404.91ms, tok/sec: 10115.718806783589\n",
      "step 814, loss: 4.53125, dt: 382.63ms, tok/sec: 10704.74868510664\n",
      "step 815, loss: 4.4375, dt: 487.05ms, tok/sec: 8409.7720454699\n",
      "step 816, loss: 4.15625, dt: 446.22ms, tok/sec: 9179.349823517909\n",
      "step 817, loss: 4.15625, dt: 460.13ms, tok/sec: 8901.906351024842\n",
      "step 818, loss: 4.21875, dt: 395.63ms, tok/sec: 10353.173159646181\n",
      "step 819, loss: 4.90625, dt: 449.82ms, tok/sec: 9105.940947682313\n",
      "step 820, loss: 4.75, dt: 431.38ms, tok/sec: 9495.118479310377\n",
      "step 821, loss: 4.75, dt: 418.03ms, tok/sec: 9798.38957507289\n",
      "step 822, loss: 4.6875, dt: 399.82ms, tok/sec: 10244.650548611773\n",
      "step 823, loss: 4.71875, dt: 423.02ms, tok/sec: 9682.847243185603\n",
      "step 824, loss: 4.65625, dt: 419.83ms, tok/sec: 9756.316913482162\n",
      "step 825, loss: 4.46875, dt: 366.11ms, tok/sec: 11188.037424856257\n",
      "step 826, loss: 4.1875, dt: 399.73ms, tok/sec: 10247.021412739805\n",
      "step 827, loss: 4.34375, dt: 483.19ms, tok/sec: 8476.966300595119\n",
      "step 828, loss: 4.3125, dt: 436.61ms, tok/sec: 9381.426809023713\n",
      "step 829, loss: 4.40625, dt: 334.79ms, tok/sec: 12234.683515240771\n",
      "step 830, loss: 4.4375, dt: 350.06ms, tok/sec: 11700.72424084333\n",
      "step 831, loss: 4.6875, dt: 382.55ms, tok/sec: 10706.990311907348\n",
      "step 832, loss: 4.625, dt: 465.74ms, tok/sec: 8794.629595843251\n",
      "step 833, loss: 4.46875, dt: 421.37ms, tok/sec: 9720.721864717982\n",
      "step 834, loss: 4.53125, dt: 427.50ms, tok/sec: 9581.313053662454\n",
      "step 835, loss: 4.53125, dt: 417.73ms, tok/sec: 9805.30733936838\n",
      "step 836, loss: 4.5625, dt: 410.20ms, tok/sec: 9985.375895305036\n",
      "step 837, loss: 4.21875, dt: 433.66ms, tok/sec: 9445.212776754264\n",
      "step 838, loss: 4.3125, dt: 430.21ms, tok/sec: 9520.881638136132\n",
      "step 839, loss: 4.65625, dt: 475.50ms, tok/sec: 8614.036666593127\n",
      "step 840, loss: 4.28125, dt: 371.53ms, tok/sec: 11024.672985045989\n",
      "step 841, loss: 4.53125, dt: 385.75ms, tok/sec: 10618.208935556622\n",
      "step 842, loss: 4.15625, dt: 428.75ms, tok/sec: 9553.261609553334\n",
      "step 843, loss: 4.71875, dt: 392.91ms, tok/sec: 10424.760137064188\n",
      "step 844, loss: 4.75, dt: 437.74ms, tok/sec: 9357.232320932637\n",
      "step 845, loss: 4.8125, dt: 367.55ms, tok/sec: 11144.173611719245\n",
      "step 846, loss: 4.75, dt: 362.02ms, tok/sec: 11314.4480173551\n",
      "step 847, loss: 4.65625, dt: 414.76ms, tok/sec: 9875.485836215143\n",
      "step 848, loss: 4.71875, dt: 468.80ms, tok/sec: 8737.280083325197\n",
      "step 849, loss: 4.6875, dt: 423.63ms, tok/sec: 9668.76037106261\n",
      "step 850, loss: 4.65625, dt: 385.00ms, tok/sec: 10638.89541910941\n",
      "step 851, loss: 4.5, dt: 403.61ms, tok/sec: 10148.344876228237\n",
      "step 852, loss: 4.34375, dt: 367.25ms, tok/sec: 11153.180942587298\n",
      "step 853, loss: 4.5625, dt: 368.93ms, tok/sec: 11102.467367027877\n",
      "step 854, loss: 4.71875, dt: 398.84ms, tok/sec: 10269.869589596627\n",
      "step 855, loss: 4.78125, dt: 472.23ms, tok/sec: 8673.670204150872\n",
      "step 856, loss: 4.6875, dt: 408.10ms, tok/sec: 10036.711499600106\n",
      "step 857, loss: 4.5625, dt: 378.37ms, tok/sec: 10825.469905846681\n",
      "step 858, loss: 4.75, dt: 421.71ms, tok/sec: 9712.80255723417\n",
      "step 859, loss: 4.59375, dt: 438.44ms, tok/sec: 9342.13529994573\n",
      "step 860, loss: 4.375, dt: 385.97ms, tok/sec: 10612.155015238874\n",
      "step 861, loss: 4.5625, dt: 354.87ms, tok/sec: 11542.120450132688\n",
      "step 862, loss: 4.75, dt: 409.12ms, tok/sec: 10011.648790084779\n",
      "step 863, loss: 4.5, dt: 389.76ms, tok/sec: 10508.97930241378\n",
      "step 864, loss: 4.53125, dt: 391.96ms, tok/sec: 10449.978518378843\n",
      "step 865, loss: 4.84375, dt: 445.37ms, tok/sec: 9196.81887428581\n",
      "step 866, loss: 4.65625, dt: 405.43ms, tok/sec: 10102.822098415645\n",
      "step 867, loss: 4.5625, dt: 453.50ms, tok/sec: 9032.048868067468\n",
      "step 868, loss: 4.34375, dt: 450.20ms, tok/sec: 9098.152861782753\n",
      "step 869, loss: 4.5, dt: 395.85ms, tok/sec: 10347.299192205859\n",
      "step 870, loss: 4.4375, dt: 372.83ms, tok/sec: 10986.362344244106\n",
      "step 871, loss: 4.59375, dt: 415.10ms, tok/sec: 9867.431475393265\n",
      "step 872, loss: 4.4375, dt: 412.21ms, tok/sec: 9936.66006762533\n",
      "step 873, loss: 4.90625, dt: 489.78ms, tok/sec: 8362.923045918542\n",
      "step 874, loss: 4.84375, dt: 433.30ms, tok/sec: 9453.107269924034\n",
      "step 875, loss: 4.71875, dt: 410.14ms, tok/sec: 9986.821239189096\n",
      "step 876, loss: 4.9375, dt: 497.30ms, tok/sec: 8236.545602907263\n",
      "step 877, loss: 5.15625, dt: 474.75ms, tok/sec: 8627.771684723355\n",
      "step 878, loss: 4.96875, dt: 506.43ms, tok/sec: 8087.97466439122\n",
      "step 879, loss: 4.65625, dt: 395.64ms, tok/sec: 10352.973509925792\n",
      "step 880, loss: 4.6875, dt: 412.27ms, tok/sec: 9935.280916134718\n",
      "step 881, loss: 4.84375, dt: 453.20ms, tok/sec: 9037.85996997177\n",
      "step 882, loss: 4.71875, dt: 463.40ms, tok/sec: 8839.06814726473\n",
      "step 883, loss: 4.5625, dt: 429.61ms, tok/sec: 9534.21253575357\n",
      "step 884, loss: 4.375, dt: 462.95ms, tok/sec: 8847.653465983913\n",
      "step 885, loss: 4.34375, dt: 410.26ms, tok/sec: 9984.02380376769\n",
      "step 886, loss: 4.5, dt: 405.28ms, tok/sec: 10106.524749114496\n",
      "step 887, loss: 4.46875, dt: 456.37ms, tok/sec: 8975.227652270494\n",
      "step 888, loss: 4.4375, dt: 458.84ms, tok/sec: 8926.907496165246\n",
      "step 889, loss: 4.4375, dt: 417.94ms, tok/sec: 9800.480092551921\n",
      "step 890, loss: 4.21875, dt: 406.67ms, tok/sec: 10072.087607896156\n",
      "step 891, loss: 4.1875, dt: 412.65ms, tok/sec: 9926.061937249247\n",
      "step 892, loss: 4.84375, dt: 411.03ms, tok/sec: 9965.150237674703\n",
      "step 893, loss: 4.53125, dt: 415.45ms, tok/sec: 9859.220498152674\n",
      "step 894, loss: 4.25, dt: 397.45ms, tok/sec: 10305.76142280578\n",
      "step 895, loss: 4.28125, dt: 531.86ms, tok/sec: 7701.2178972646125\n",
      "step 896, loss: 4.46875, dt: 452.29ms, tok/sec: 9056.15448082748\n",
      "step 897, loss: 4.375, dt: 395.81ms, tok/sec: 10348.32135697764\n",
      "step 898, loss: 4.09375, dt: 402.15ms, tok/sec: 10185.377163593936\n",
      "step 899, loss: 4.0625, dt: 393.21ms, tok/sec: 10416.909467393552\n",
      "step 900, loss: 4.125, dt: 438.10ms, tok/sec: 9349.563230576243\n",
      "step 901, loss: 4.8125, dt: 401.04ms, tok/sec: 10213.321640041282\n",
      "step 902, loss: 4.65625, dt: 419.42ms, tok/sec: 9765.961433822316\n",
      "step 903, loss: 4.6875, dt: 397.79ms, tok/sec: 10296.805079636026\n",
      "step 904, loss: 4.59375, dt: 426.03ms, tok/sec: 9614.24059176717\n",
      "step 905, loss: 4.65625, dt: 500.97ms, tok/sec: 8176.144030679334\n",
      "step 906, loss: 4.59375, dt: 456.58ms, tok/sec: 8971.089261451281\n",
      "step 907, loss: 4.40625, dt: 418.38ms, tok/sec: 9790.14799534081\n",
      "step 908, loss: 4.15625, dt: 445.37ms, tok/sec: 9196.902570976752\n",
      "step 909, loss: 4.28125, dt: 461.65ms, tok/sec: 8872.597813130718\n",
      "step 910, loss: 4.25, dt: 474.92ms, tok/sec: 8624.65746294582\n",
      "step 911, loss: 4.3125, dt: 478.75ms, tok/sec: 8555.596704625983\n",
      "step 912, loss: 4.40625, dt: 414.17ms, tok/sec: 9889.572801740762\n",
      "step 913, loss: 4.625, dt: 419.05ms, tok/sec: 9774.42369092559\n",
      "step 914, loss: 4.53125, dt: 492.73ms, tok/sec: 8312.826519957225\n",
      "step 915, loss: 4.375, dt: 468.47ms, tok/sec: 8743.29193249598\n",
      "step 916, loss: 4.40625, dt: 478.79ms, tok/sec: 8554.915047254066\n",
      "step 917, loss: 4.40625, dt: 433.08ms, tok/sec: 9457.733674137306\n",
      "step 918, loss: 4.46875, dt: 428.80ms, tok/sec: 9552.262999489023\n",
      "step 919, loss: 4.15625, dt: 449.24ms, tok/sec: 9117.621494823667\n",
      "step 920, loss: 4.25, dt: 494.83ms, tok/sec: 8277.667850025633\n",
      "step 921, loss: 4.59375, dt: 474.73ms, tok/sec: 8628.06633115052\n",
      "step 922, loss: 4.21875, dt: 485.27ms, tok/sec: 8440.711257675553\n",
      "step 923, loss: 4.46875, dt: 455.23ms, tok/sec: 8997.663723636591\n",
      "step 924, loss: 4.0625, dt: 375.72ms, tok/sec: 10901.831486099742\n",
      "step 925, loss: 4.625, dt: 523.79ms, tok/sec: 7819.876193757797\n",
      "step 926, loss: 4.6875, dt: 426.39ms, tok/sec: 9606.160729402793\n",
      "step 927, loss: 4.75, dt: 417.85ms, tok/sec: 9802.660994241605\n",
      "step 928, loss: 4.71875, dt: 524.85ms, tok/sec: 7804.1077647375205\n",
      "step 929, loss: 4.5625, dt: 365.88ms, tok/sec: 11195.072816748687\n",
      "step 930, loss: 4.65625, dt: 467.86ms, tok/sec: 8754.809542713145\n",
      "step 931, loss: 4.625, dt: 456.36ms, tok/sec: 8975.391766817755\n",
      "step 932, loss: 4.59375, dt: 454.18ms, tok/sec: 9018.512335193043\n",
      "step 933, loss: 4.4375, dt: 468.27ms, tok/sec: 8747.155973467097\n",
      "step 934, loss: 4.28125, dt: 454.23ms, tok/sec: 9017.461457144687\n",
      "step 935, loss: 4.5, dt: 409.46ms, tok/sec: 10003.446606051608\n",
      "step 936, loss: 4.65625, dt: 434.76ms, tok/sec: 9421.251711110855\n",
      "step 937, loss: 4.6875, dt: 431.61ms, tok/sec: 9490.0779783218\n",
      "step 938, loss: 4.59375, dt: 418.29ms, tok/sec: 9792.285230123305\n",
      "step 939, loss: 4.46875, dt: 418.56ms, tok/sec: 9785.959936499003\n",
      "step 940, loss: 4.6875, dt: 422.69ms, tok/sec: 9690.39525832602\n",
      "step 941, loss: 4.53125, dt: 500.87ms, tok/sec: 8177.825350213991\n",
      "step 942, loss: 4.3125, dt: 448.56ms, tok/sec: 9131.423436045798\n",
      "step 943, loss: 4.46875, dt: 464.14ms, tok/sec: 8824.970018502785\n",
      "step 944, loss: 4.65625, dt: 456.48ms, tok/sec: 8973.024409632651\n",
      "step 945, loss: 4.375, dt: 458.82ms, tok/sec: 8927.292512397235\n",
      "step 946, loss: 4.4375, dt: 469.71ms, tok/sec: 8720.205745634026\n",
      "step 947, loss: 4.78125, dt: 426.25ms, tok/sec: 9609.454468570188\n",
      "step 948, loss: 4.59375, dt: 422.86ms, tok/sec: 9686.51600993693\n",
      "step 949, loss: 4.5, dt: 407.89ms, tok/sec: 10042.044327955324\n",
      "step 950, loss: 4.25, dt: 405.63ms, tok/sec: 10097.863729525661\n",
      "step 951, loss: 4.4375, dt: 458.20ms, tok/sec: 8939.407311741568\n",
      "step 952, loss: 4.375, dt: 463.99ms, tok/sec: 8827.736148364647\n",
      "step 953, loss: 4.5, dt: 486.66ms, tok/sec: 8416.603068409182\n",
      "step 954, loss: 4.34375, dt: 407.31ms, tok/sec: 10056.298834736524\n",
      "step 955, loss: 4.8125, dt: 451.57ms, tok/sec: 9070.618408638167\n",
      "step 956, loss: 4.75, dt: 470.87ms, tok/sec: 8698.839715822236\n",
      "step 957, loss: 4.625, dt: 495.70ms, tok/sec: 8263.05634600826\n",
      "step 958, loss: 4.84375, dt: 459.79ms, tok/sec: 8908.43335554409\n",
      "step 959, loss: 5.0625, dt: 438.09ms, tok/sec: 9349.659907068679\n",
      "step 960, loss: 4.84375, dt: 436.69ms, tok/sec: 9379.700701189833\n",
      "step 961, loss: 4.59375, dt: 426.73ms, tok/sec: 9598.684324357546\n",
      "step 962, loss: 4.65625, dt: 478.11ms, tok/sec: 8567.022091416506\n",
      "step 963, loss: 4.78125, dt: 420.33ms, tok/sec: 9744.618260464626\n",
      "step 964, loss: 4.625, dt: 408.87ms, tok/sec: 10017.901265192426\n",
      "step 965, loss: 4.5, dt: 485.48ms, tok/sec: 8436.97642785127\n",
      "step 966, loss: 4.3125, dt: 484.57ms, tok/sec: 8452.929510493914\n",
      "step 967, loss: 4.28125, dt: 398.77ms, tok/sec: 10271.64411770509\n",
      "step 968, loss: 4.4375, dt: 468.07ms, tok/sec: 8750.89416380189\n",
      "step 969, loss: 4.40625, dt: 533.10ms, tok/sec: 7683.304644007156\n",
      "step 970, loss: 4.34375, dt: 469.28ms, tok/sec: 8728.322139522123\n",
      "step 971, loss: 4.34375, dt: 474.22ms, tok/sec: 8637.405774134644\n",
      "step 972, loss: 4.1875, dt: 482.25ms, tok/sec: 8493.566585520683\n",
      "step 973, loss: 4.15625, dt: 542.84ms, tok/sec: 7545.534522181318\n",
      "step 974, loss: 4.75, dt: 420.90ms, tok/sec: 9731.580342511186\n",
      "step 975, loss: 4.46875, dt: 468.95ms, tok/sec: 8734.459323191182\n",
      "step 976, loss: 4.1875, dt: 436.82ms, tok/sec: 9376.8184420172\n",
      "step 977, loss: 4.21875, dt: 384.81ms, tok/sec: 10644.089532468128\n",
      "step 978, loss: 4.375, dt: 509.64ms, tok/sec: 8036.993460419845\n",
      "step 979, loss: 4.3125, dt: 469.97ms, tok/sec: 8715.38824813782\n",
      "step 980, loss: 4.03125, dt: 416.23ms, tok/sec: 9840.770303330593\n",
      "step 981, loss: 4, dt: 457.06ms, tok/sec: 8961.617685506377\n",
      "step 982, loss: 4.0625, dt: 470.14ms, tok/sec: 8712.214884004445\n",
      "step 983, loss: 4.75, dt: 421.00ms, tok/sec: 9729.238103112588\n",
      "step 984, loss: 4.59375, dt: 420.25ms, tok/sec: 9746.475773469301\n",
      "step 985, loss: 4.59375, dt: 406.47ms, tok/sec: 10077.121173319005\n",
      "step 986, loss: 4.5, dt: 409.81ms, tok/sec: 9994.804281857954\n",
      "step 987, loss: 4.59375, dt: 412.87ms, tok/sec: 9920.817176657145\n",
      "step 988, loss: 4.53125, dt: 427.09ms, tok/sec: 9590.566330045274\n",
      "step 989, loss: 4.34375, dt: 399.69ms, tok/sec: 10248.036086723621\n",
      "step 990, loss: 4.0625, dt: 333.43ms, tok/sec: 12284.46195649065\n",
      "step 991, loss: 4.21875, dt: 413.86ms, tok/sec: 9896.990545364048\n",
      "step 992, loss: 4.15625, dt: 400.01ms, tok/sec: 10239.698495205841\n",
      "step 993, loss: 4.28125, dt: 458.28ms, tok/sec: 8937.672620651696\n",
      "step 994, loss: 4.3125, dt: 484.49ms, tok/sec: 8454.252296887256\n",
      "step 995, loss: 4.5625, dt: 460.62ms, tok/sec: 8892.419120813594\n",
      "step 996, loss: 4.4375, dt: 422.59ms, tok/sec: 9692.538372137633\n",
      "step 997, loss: 4.28125, dt: 384.25ms, tok/sec: 10659.5965231333\n",
      "step 998, loss: 4.34375, dt: 409.50ms, tok/sec: 10002.456491602938\n",
      "step 999, loss: 4.375, dt: 513.72ms, tok/sec: 7973.158923717451\n",
      "step 1000, loss: 4.40625, dt: 430.39ms, tok/sec: 9516.973459739793\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    grads, loss, accuracy = apply_model(state, x, y)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    dt = (t1 - t0)*1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    print(f'step {epoch}, loss: {loss}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
