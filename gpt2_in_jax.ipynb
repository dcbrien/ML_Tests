{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import flax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/miniconda3/envs/jax-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import FlaxGPT2LMHeadModel\n",
    "\n",
    "model_hf = FlaxGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '1': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '10': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '11': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '2': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '3': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '4': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '5': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '6': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '7': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '8': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}, '9': {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}}, 'ln_f': {'bias': (768,), 'scale': (768,)}, 'wpe': {'embedding': (1024, 768)}, 'wte': {'embedding': (50257, 768)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.shape(model_hf.params['transformer']['wpe']['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c_attn', 'c_proj'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hf.params['transformer']['h']['0']['attn'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "       [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.broadcast_to(jnp.arange(10, dtype=\"i4\")[None, :], (32, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "\n",
    "# define a full GTP2 class in Flax and see if we can replicate the original paper along with\n",
    "# Karpathy\n",
    "\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 65\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "\n",
    "class GPTMLP(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # Simple MLP that upscales, runs through a gelu activation,\n",
    "        # and then resamples back to the n_embd size (the model size)\n",
    "        #self.c_fc = nn.Dense(4 * self.config.n_embd)\n",
    "        self.c_fc = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "        #self.c_proj = nn.Dense(self.config.n_embd)\n",
    "        self.c_proj = nn.Einsum((self.config.n_embd, 4 * self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = self.c_fc(x)\n",
    "        x = nn.gelu(x, approximate=True)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class GPTAttention(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    # we will need to roll our own attention module because the built in one has a bunch of different\n",
    "    # naming and structure compared to the original GPT, which just references the projection layers\n",
    "    def setup(self):\n",
    "        self.c_proj = nn.Einsum((4 * self.config.n_embd, self.config.n_embd), '...ij,...kj->...ik')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_1 = nn.LayerNorm()\n",
    "        # I might have to write this manually to get the proper number of parameters, as the old GPT2 code \n",
    "        # migh have subtle differences from the Flax implementation\n",
    "        self.attn = nn.MultiHeadAttention(num_heads=self.config.n_head, qkv_features=self.config.n_head)\n",
    "        self.ln_2 = nn.LayerNorm()\n",
    "        self.mlp = GPTMLP(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.ln_1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.ln_2(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "class GPTLayers(nn.Module):\n",
    "    config: GPTConfig\n",
    "    \n",
    "    def setup(self):\n",
    "        self.blocks = [ GPTBlock(self.config, name=str(i)) for i in range(self.config.n_layer) ]\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return inputs\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        # This is a little confusing. vocab size is the number of embeddings we need.\n",
    "        # n_embd is the dimension of each embedding (i.e. capacity for learning properties\n",
    "        # about this embedding token)\n",
    "        # input size = (1 x self.block_size) - 1 int for each token\n",
    "        # output size = then (self.block_size x self.n_embd)\n",
    "        self.wte = nn.Embed(self.config.vocab_size, self.config.n_embd)\n",
    "        # embed is just a randomzied parameter matrix, so can be used for positional \n",
    "        # encoding as well. I think block size is the token length.\n",
    "        # This has to match the size of the previous output, as we are just adding.\n",
    "        self.wpe = nn.Embed(self.config.block_size, self.config.n_embd)\n",
    "        # The attention layers\n",
    "        self.h = GPTLayers(self.config)\n",
    "        self.ln_f = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.wte(x)\n",
    "        \n",
    "        #x += self.wpe(x)\n",
    "\n",
    "        x = self.h(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    config: GPTConfig\n",
    "\n",
    "    def setup(self):\n",
    "        self.transformer = GPTModel(self.config)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2 = GPT(GPTConfig)\n",
    "\n",
    "key1, key2 = jax.random.split(jax.random.key(0), 2)\n",
    "\n",
    "# We never want to use int64 as it signficantly slows down training\n",
    "x = np.random.randint(0, 1000, (4,4), dtype='int32')\n",
    "\n",
    "y = GPT2.init(key2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef({'transformer': {'h': {'0': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '1': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '2': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '3': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '4': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}, '5': {'attn': {'key': {'bias': *, 'kernel': *}, 'out': {'bias': *, 'kernel': *}, 'query': {'bias': *, 'kernel': *}, 'value': {'bias': *, 'kernel': *}}, 'ln_1': {'bias': *, 'scale': *}, 'ln_2': {'bias': *, 'scale': *}, 'mlp': {'c_fc': {'bias': *, 'kernel': *}, 'c_proj': {'bias': *, 'kernel': *}}}}, 'ln_f': {'bias': *, 'scale': *}, 'wte': {'embedding': *}}})\n"
     ]
    }
   ],
   "source": [
    "from jax.tree_util import tree_structure\n",
    "print(tree_structure(y['params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'attn': {'key': {'bias': (6, 1), 'kernel': (384, 6, 1)}, 'out': {'bias': (384,), 'kernel': (6, 1, 384)}, 'query': {'bias': (6, 1), 'kernel': (384, 6, 1)}, 'value': {'bias': (6, 1), 'kernel': (384, 6, 1)}}, 'ln_1': {'bias': (384,), 'scale': (384,)}, 'ln_2': {'bias': (384,), 'scale': (384,)}, 'mlp': {'c_fc': {'bias': (1536,), 'kernel': (1536, 384)}, 'c_proj': {'bias': (384,), 'kernel': (384, 1536)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params']['transformer']['h']['0']['attn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'attn': {'c_attn': {'bias': (2304,), 'kernel': (2304, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 768)}}, 'ln_1': {'bias': (768,), 'scale': (768,)}, 'ln_2': {'bias': (768,), 'scale': (768,)}, 'mlp': {'c_fc': {'bias': (3072,), 'kernel': (3072, 768)}, 'c_proj': {'bias': (768,), 'kernel': (768, 3072)}}}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(model_hf.params['transformer']['h']['0']['attn'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seems like pytorch Linear is not equal to flax Dense. The matrices are defined transpose. So, we can't actually use\n",
    "# Dense with the original GPT2 weights\n",
    "\n",
    "x = jax.random.uniform(jax.random.key(0), (1, 2, 5))\n",
    "\n",
    "# This will transpose the kernel and multiply by the input,\n",
    "# This should be the same as the GPT weights\n",
    "lp = nn.Einsum((3*5, 5), '...ij,...kj->...ik')\n",
    "\n",
    "y = lp.init(jax.random.key(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.40416887, -0.15771458, -0.1418275 ,  0.21624644,\n",
       "          0.01153318,  0.09767484, -0.16552436, -0.06833879,\n",
       "         -0.07042018, -0.13803177, -0.00222424,  0.2503722 ,\n",
       "         -0.3504156 ,  0.18865553, -0.33016038],\n",
       "        [ 0.62610245, -0.35361993, -0.40769407,  0.22174977,\n",
       "          0.1036396 ,  0.2302788 , -0.25040534, -0.5411675 ,\n",
       "         -0.35929912,  0.05858286,  0.03919724,  0.3880062 ,\n",
       "         -0.13242133,  0.4200734 , -0.47516495]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.apply(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp2 = nn.Dense(3*5)\n",
    "\n",
    "y2 = lp2.init(jax.random.key(0), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': Array([[ 0.65126026, -0.20171352,  0.7524083 ,  0.21318944,  0.66664237,\n",
       "         -0.5474334 ,  0.5442594 , -0.66585183, -0.21363808, -0.22357357,\n",
       "         -0.49834606,  0.24824531, -0.08310969, -0.48232296, -0.21830165],\n",
       "        [ 0.23788127,  0.42619142,  0.03609344,  0.46733794, -0.27462825,\n",
       "         -0.70285726,  0.40865424, -0.2941622 ,  0.544966  ,  0.0685642 ,\n",
       "          0.06401048, -0.16370532,  0.50371426,  0.77258414, -0.4492223 ],\n",
       "        [-0.6562224 ,  0.04016771, -0.08710401, -0.15943906, -0.01229531,\n",
       "          0.27645266,  0.26468387,  0.20921318, -0.64844024, -0.9589507 ,\n",
       "          0.580644  ,  0.07664067, -0.12084891, -0.6119603 , -0.550388  ],\n",
       "        [-0.4711891 , -0.17671804, -0.25029108,  0.4280646 ,  0.13103884,\n",
       "          0.60195965, -0.41966525, -0.01266516, -0.66290927,  0.5892318 ,\n",
       "         -0.0306597 ,  0.27763686,  0.15033169,  0.42397204,  0.34783563],\n",
       "        [-0.63989514, -0.8911751 ,  0.31193644,  0.50128835, -0.2739739 ,\n",
       "          0.32711524, -0.09540533,  0.11771223,  0.38339144,  0.3927799 ,\n",
       "         -0.34034586, -0.31676415, -0.14573576, -0.46937844, -0.27016112]],      dtype=float32),\n",
       " 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': Array([[ 0.3760053 , -0.11645935,  0.43440315,  0.12308498,  0.38488615],\n",
       "        [-0.3160608 ,  0.31422833, -0.38442972, -0.123344  , -0.12908025],\n",
       "        [-0.28772023,  0.1433245 , -0.0479834 , -0.2784693 , -0.12603652],\n",
       "        [ 0.13734081,  0.24606173,  0.02083855,  0.26981768, -0.15855668],\n",
       "        [-0.40579483,  0.23593663, -0.16983464,  0.31463626,  0.03958556],\n",
       "        [ 0.03695647, -0.09451531,  0.2908196 ,  0.44605166, -0.2593586 ],\n",
       "        [-0.37887016,  0.02319084, -0.05028952, -0.09205218, -0.0070987 ],\n",
       "        [ 0.15961002,  0.1528153 ,  0.12078928, -0.37437713, -0.55365044],\n",
       "        [ 0.33523497,  0.04424851, -0.06977215, -0.3533154 , -0.31776664],\n",
       "        [-0.27204117, -0.10202821, -0.14450562,  0.24714321,  0.07565531],\n",
       "        [ 0.34754157, -0.24229383, -0.00731223, -0.38273084,  0.34019315],\n",
       "        [-0.01770138,  0.16029371,  0.08679405,  0.24478038,  0.200823  ],\n",
       "        [-0.36944363, -0.51452017,  0.18009658,  0.28941897, -0.15817891],\n",
       "        [ 0.18886006, -0.05508229,  0.06796119,  0.22135115,  0.22677156],\n",
       "        [-0.19649878, -0.18288386, -0.08414058, -0.27099577, -0.15597759]],      dtype=float32),\n",
       " 'bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[-0.22546573, -0.13162534,  0.29677063,  0.5539263 ,\n",
       "          0.00525441, -0.25402257,  0.42499188, -0.2886829 ,\n",
       "         -0.05022871, -0.18250518, -0.00925286, -0.01849659,\n",
       "          0.20977122, -0.02310264, -0.5931318 ],\n",
       "        [-0.6551649 , -0.74319994,  0.34639242,  0.85376245,\n",
       "          0.12546577,  0.35563067,  0.02477333, -0.2340745 ,\n",
       "         -0.37190086,  0.38618025, -0.30876082,  0.07604566,\n",
       "          0.06765305, -0.19773354, -0.28735214]]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp2.apply(y2, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': Array([[ 0.00585502,  0.06663669, -0.02862518, ...,  0.03995296,\n",
       "         -0.02848659,  0.03771213],\n",
       "        [-0.01506117, -0.04920985, -0.02150657, ..., -0.00969302,\n",
       "         -0.00604414, -0.06075063],\n",
       "        [-0.01117269, -0.05698648, -0.02741355, ...,  0.01138836,\n",
       "          0.02523999, -0.03665182],\n",
       "        ...,\n",
       "        [ 0.00031354,  0.03191991,  0.01749613, ..., -0.03667144,\n",
       "          0.02852946,  0.02422978],\n",
       "        [ 0.01442624,  0.02207879, -0.02500978, ..., -0.00365288,\n",
       "          0.00623836,  0.01097682],\n",
       "        [-0.03645209, -0.06231447, -0.00414094, ...,  0.03171502,\n",
       "          0.06253771, -0.02757666]], dtype=float32),\n",
       " 'bias': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized parameter shapes:\n",
      " {'bias': (15,), 'kernel': (15, 5)}\n"
     ]
    }
   ],
   "source": [
    "print('initialized parameter shapes:\\n', jax.tree_util.tree_map(jnp.shape, flax.core.unfreeze(y['params'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  38,  62],\n",
       "       [ 38, 126, 214],\n",
       "       [ 62, 214, 366]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[ 0,  1,  2, 3],\n",
    "              [ 4,  5,  6, 7],\n",
    "              [ 8,  9, 10, 11]])\n",
    "\n",
    "B = np.array([[ 0,  1,  2,  3],\n",
    "              [ 4,  5,  6,  7],\n",
    "              [ 8,  9, 10, 11]])\n",
    "\n",
    "np.einsum('ij,kj->ik', A, B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
