{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing imgage in python\n",
    "# Let's generate a set of data of simple shapes and then create a deep network to classify them\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "h=128\n",
    "w=128\n",
    "\n",
    "NUM_SAMP = 5000\n",
    "\n",
    "x_data=np.empty([NUM_SAMP*3, h, w, 3])\n",
    "\n",
    "for i in range(NUM_SAMP):\n",
    "    # Circles\n",
    "    img = np.zeros((h,w,3), np.uint8)\n",
    "\n",
    "    cv2.circle(img, (random.randint(0, h-1), random.randint(0, w-1)), random.randint(10, 50), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), -1)\n",
    "    x_data[i,:,:,:] = img/255\n",
    "    \n",
    "    # Rectangles\n",
    "    img = np.zeros((h,w,3), np.uint8)\n",
    "    cv2.rectangle(img, (random.randint(0, h-1), random.randint(0, w-1)), (random.randint(0, h-1), random.randint(0, w-1)), \n",
    "                 (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), -1)\n",
    "    x_data[i+NUM_SAMP,:,:,:] = img/255\n",
    "    \n",
    "    # Triangles\n",
    "    img = np.zeros((h,w,3), np.uint8)\n",
    "    \n",
    "    pts = np.array([[random.randint(0, h-1), random.randint(0, w-1)], [random.randint(0, h-1), random.randint(0, w-1)], [random.randint(0, h-1), random.randint(0, w-1)]])\n",
    "    cv2.fillPoly(img, [pts], (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)))\n",
    "    x_data[i+NUM_SAMP*2,:,:,:] = img/255\n",
    "\n",
    "y_data=np.concatenate((np.ones((1, NUM_SAMP))*0, np.ones((1, NUM_SAMP))*1, np.ones((1, NUM_SAMP))*2), axis=None)\n",
    "\n",
    "y_data\n",
    "#nplt.imshow(x_data[290, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have some made up data in x_data, y_data of basic shapes.  Split into test and validation sets and train a simple network\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, Y_train, Y_test = train_test_split(x_data, y_data, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 0., 1., ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (10050, 128, 128, 3)\n",
      "10050 train samples\n",
      "4950 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 126, 126, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 124, 124, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_9 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 14,755\n",
      "Trainable params: 14,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10050 samples, validate on 4950 samples\n",
      "Epoch 1/15\n",
      "10050/10050 [==============================] - 7s 739us/step - loss: 1.0746 - acc: 0.4045 - val_loss: 1.0333 - val_acc: 0.4469\n",
      "Epoch 2/15\n",
      "10050/10050 [==============================] - 7s 652us/step - loss: 0.9212 - acc: 0.5427 - val_loss: 0.7876 - val_acc: 0.6135\n",
      "Epoch 3/15\n",
      "10050/10050 [==============================] - 7s 652us/step - loss: 0.7612 - acc: 0.6344 - val_loss: 0.7084 - val_acc: 0.6610\n",
      "Epoch 4/15\n",
      "10050/10050 [==============================] - 7s 652us/step - loss: 0.7175 - acc: 0.6607 - val_loss: 0.6665 - val_acc: 0.7036\n",
      "Epoch 5/15\n",
      "10050/10050 [==============================] - 7s 653us/step - loss: 0.6499 - acc: 0.7005 - val_loss: 0.5713 - val_acc: 0.7844\n",
      "Epoch 6/15\n",
      "10050/10050 [==============================] - 7s 654us/step - loss: 0.5132 - acc: 0.8172 - val_loss: 0.3540 - val_acc: 0.9497\n",
      "Epoch 7/15\n",
      "10050/10050 [==============================] - 7s 655us/step - loss: 0.2980 - acc: 0.9228 - val_loss: 0.2029 - val_acc: 0.9451\n",
      "Epoch 8/15\n",
      "10050/10050 [==============================] - 7s 656us/step - loss: 0.2009 - acc: 0.9441 - val_loss: 0.1396 - val_acc: 0.9711\n",
      "Epoch 9/15\n",
      "10050/10050 [==============================] - 7s 654us/step - loss: 0.1580 - acc: 0.9529 - val_loss: 0.1180 - val_acc: 0.9770\n",
      "Epoch 10/15\n",
      "10050/10050 [==============================] - 7s 657us/step - loss: 0.1397 - acc: 0.9588 - val_loss: 0.0944 - val_acc: 0.9790\n",
      "Epoch 11/15\n",
      "10050/10050 [==============================] - 7s 655us/step - loss: 0.1184 - acc: 0.9655 - val_loss: 0.0770 - val_acc: 0.9851\n",
      "Epoch 12/15\n",
      "10050/10050 [==============================] - 7s 657us/step - loss: 0.1075 - acc: 0.9694 - val_loss: 0.1048 - val_acc: 0.9584\n",
      "Epoch 13/15\n",
      "10050/10050 [==============================] - 7s 659us/step - loss: 0.1027 - acc: 0.9703 - val_loss: 0.0607 - val_acc: 0.9889\n",
      "Epoch 14/15\n",
      "10050/10050 [==============================] - 7s 660us/step - loss: 0.0886 - acc: 0.9747 - val_loss: 0.0587 - val_acc: 0.9913\n",
      "Epoch 15/15\n",
      "10050/10050 [==============================] - 7s 659us/step - loss: 0.0948 - acc: 0.9699 - val_loss: 0.0513 - val_acc: 0.9927\n",
      "Test loss: 0.05130400600005882\n",
      "Test accuracy: 0.9927272727272727\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "#from keras.utils import to_categorical\n",
    "\n",
    "import pdb\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "batch_size = 64\n",
    "num_classes = 3\n",
    "epochs = 15\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = w, h\n",
    "\n",
    "input_shape = (img_rows, img_cols, 3)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))    \n",
    " \n",
    "# let's define the loss function by hand\n",
    "def get_cat_crossentropy_loss():\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        # scale preds so that the class probas of each sample sum to 1\n",
    "        # Don notes: Haven't figured out why this is required.  y_pred is the output of softmax,\n",
    "        # and so should already sum to 1???\n",
    "        y_pred = K.print_tensor(y_pred, message=\"y_pred is: \")\n",
    "        y_pred /= tf.reduce_sum(y_pred, -1, True)\n",
    "        # manual computation of crossentropy\n",
    "        # Don notes: Looks like any math has to be as a tensor in the same type\n",
    "        _epsilon = tf.convert_to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "        # Don notes: We clip the values of y_pred to be away from the asymtopes of the log\n",
    "        # function.  If we didn't, we will get inf or NaN values.\n",
    "        y_pred = tf.clip_by_value(y_pred, _epsilon, 1. - _epsilon)\n",
    "        return - tf.reduce_sum(y_true * tf.log(y_pred), -1)\n",
    "    return weighted_loss\n",
    "    \n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss=get_cat_crossentropy_loss(),\n",
    "              optimizer=keras.optimizers.adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21072103131565256\n",
      "0.21072103131565206\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "x=np.array(([0.1, 0.9, 0.4], [0.9, 0.2, 0.3]))\n",
    "\n",
    "y=np.array(([0, 1, 0], [1, 0, 0]))\n",
    "\n",
    "print(np.sum(-np.log(x[y.astype('bool')])))\n",
    "\n",
    "print(np.sum(-np.log(x+sys.float_info.epsilon)*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(x_test)\n",
    "y_prob=model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "0 is not a valid Shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c88b9c74d448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-18/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simple value lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# otherwise, functional API: we're creating a new Enum type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqualname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqualname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-18/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# still not found -- try _missing_ hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_next_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-18/lib/python3.6/enum.py\u001b[0m in \u001b[0;36m_missing_\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_missing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%r is not a valid %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not a valid Shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from enum import Enum\n",
    "class Shape(Enum):\n",
    "    CIRCLE = 0\n",
    "    RECTANGLE = 1\n",
    "    TRIANGLE = 2\n",
    "\n",
    "img_num = random.randint(0, x_test.shape[0])\n",
    "\n",
    "plt.imshow(x_test[img_num, :, :, :])\n",
    "\n",
    "print(Shape(y_pred[img_num]))\n",
    "print((str(y_prob[img_num][y_pred[img_num]]*100) + '%'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-18",
   "language": "python",
   "name": "venv-18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
